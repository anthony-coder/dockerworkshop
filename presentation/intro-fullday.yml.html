<!DOCTYPE html>
<html>
  <head>
    <title>Introduction to Containers </title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="workshop.css">
  </head>
  <body>
    <!--
    <div style="position: absolute; left: 20%; right: 20%; top: 30%;">
      <h1 style="font-size: 3em;">Loading ...</h1>
      The slides should show up here. If they don't, it might be
      because you are accessing this file directly from your filesystem.
      It needs to be served from a web server. You can try this:
      <pre>
        docker-compose up -d
        open http://localhost:8888/workshop.html # on MacOS
        xdg-open http://localhost:8888/workshop.html # on Linux
      </pre>
      Once the slides are loaded, this notice disappears when you
      go full screen (e.g. by hitting "f").
    </div>
    -->
    <textarea id="source">class: title, self-paced

Introduction<br/>to Containers<br/>

.nav[*Self-paced version*]

.debug[
```
 M slides/intro-fullday.yml
?? slides/mytestfile/

```

These slides have been built from commit: 018282f


[shared/title.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/title.md)]
---

class: title, in-person

Introduction<br/>to Containers<br/><br/></br>

.footnote[
**Be kind to the WiFi!**<br/>
<!-- *Use the 5G network.* -->
*Don't use your hotspot.*<br/>
*Don't stream videos or download big files during the workshop.*<br/>
*Thank you!*

**Slides: http://container.training/**
]

.debug[[shared/title.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/title.md)]
---
## Intros

- This slide should be customized by the tutorial instructor(s).

- Hello! We are:

   - .emoji[üë©üèª‚Äçüè´] Ann O'Nymous ([@...](https://twitter.com/...), Megacorp Inc)

   - .emoji[üë®üèæ‚Äçüéì] Stu Dent ([@...](https://twitter.com/...), University of Wakanda)

 <!-- .dummy[

   - .emoji[üë∑üèª‚Äç‚ôÄÔ∏è] AJ ([@s0ulshake](https://twitter.com/s0ulshake), Travis CI)

   - .emoji[üê≥] J√©r√¥me ([@jpetazzo](https://twitter.com/jpetazzo), Enix SAS)

   - .emoji[‚õµ] J√©r√©my ([@jeremygarrouste](twitter.com/jeremygarrouste), Inpiwee)

] -->

- The workshop will run from ...

- There will be a lunch break at ...

  (And coffee breaks!)

- Feel free to interrupt for questions at any time

- *Especially when you see full screen container pictures!*

- Live feedback, questions, help: [Slack](https://dockercommunity.slack.com/messages/C7GKACWDV)

.debug[[logistics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/logistics.md)]
---
## A brief introduction

- This was initially written to support in-person, instructor-led workshops and tutorials

- These materials are maintained by [J√©r√¥me Petazzoni](https://twitter.com/jpetazzo) and [multiple contributors](https://github.com/jpetazzo/container.training/graphs/contributors)

- You can also follow along on your own, at your own pace

- We included as much information as possible in these slides

- We recommend having a mentor to help you ...

- ... Or be comfortable spending some time reading the Docker
 [documentation](https://docs.docker.com/) ...

- ... And looking for answers in the [Docker forums](forums.docker.com),
  [StackOverflow](http://stackoverflow.com/questions/tagged/docker),
  and other outlets

.debug[[containers/intro.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/intro.md)]
---

class: self-paced

## Hands on, you shall practice

- Nobody ever became a Jedi by spending their lives reading Wookiepedia

- Likewise, it will take more than merely *reading* these slides
  to make you an expert

- These slides include *tons* of exercises and examples

- They assume that you have acccess to a machine running Docker

- If you are attending a workshop or tutorial:
  <br/>you will be given specific instructions to access a cloud VM

- If you are doing this on your own:
  <br/>we will tell you how to install Docker or access a Docker environment

.debug[[containers/intro.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/intro.md)]
---
## About these slides

- All the content is available in a public GitHub repository:

  https://github.com/jpetazzo/container.training

- You can get updated "builds" of the slides there:

  http://container.training/

<!--
.exercise[
```open https://github.com/jpetazzo/container.training```
```open http://container.training/```
]
-->

--

- Typos? Mistakes? Questions? Feel free to hover over the bottom of the slide ...

.footnote[.emoji[üëá] Try it! The source file will be shown and you can view it on GitHub and fork and edit it.]

<!--
.exercise[
```open https://github.com/jpetazzo/container.training/tree/master/slides/common/about-slides.md```
]
-->

.debug[[shared/about-slides.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/about-slides.md)]
---

class: extra-details

## Extra details

- This slide has a little magnifying glass in the top left corner

- This magnifying glass indicates slides that provide extra details

- Feel free to skip them if:

  - you are in a hurry

  - you are new to this and want to avoid cognitive overload

  - you want only the most essential information

- You can review these slides another time if you want, they'll be waiting for you ‚ò∫

.debug[[shared/about-slides.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/about-slides.md)]
---

name: toc-chapter-1

## Chapter 1

- [Docker 30,000ft overview](#toc-docker-ft-overview)

- [Our training environment](#toc-our-training-environment)

- [Installing Docker](#toc-installing-docker)

- [Our first containers](#toc-our-first-containers)

- [Background containers](#toc-background-containers)

- [Restarting and attaching to containers](#toc-restarting-and-attaching-to-containers)

.debug[(auto-generated TOC)]
---
name: toc-chapter-2

## Chapter 2

- [Understanding Docker images](#toc-understanding-docker-images)

- [Building images interactively](#toc-building-images-interactively)

- [Building Docker images with a Dockerfile](#toc-building-docker-images-with-a-dockerfile)

- [`CMD` and `ENTRYPOINT`](#toc-cmd-and-entrypoint)

- [Copying files during the build](#toc-copying-files-during-the-build)

- [Publishing images to the Docker Hub](#toc-publishing-images-to-the-docker-hub)

- [Tips for efficient Dockerfiles](#toc-tips-for-efficient-dockerfiles)

- [Dockerfile examples](#toc-dockerfile-examples)

.debug[(auto-generated TOC)]
---
name: toc-chapter-3

## Chapter 3

- [Naming and inspecting containers](#toc-naming-and-inspecting-containers)

- [Labels](#toc-labels)

- [Getting inside a container](#toc-getting-inside-a-container)

.debug[(auto-generated TOC)]
---
name: toc-chapter-4

## Chapter 4

- [Container networking basics](#toc-container-networking-basics)

- [The Container Network Model](#toc-the-container-network-model)

- [Service discovery with containers](#toc-service-discovery-with-containers)

- [Links and resources](#toc-links-and-resources)

.debug[(auto-generated TOC)]
---
name: toc-chapter-5

## Chapter 5

- [Pre-requirements](#toc-pre-requirements)

- [Our sample application](#toc-our-sample-application)

- [Kubernetes concepts](#toc-kubernetes-concepts)

- [Declarative vs imperative](#toc-declarative-vs-imperative)

- [Kubernetes network model](#toc-kubernetes-network-model)

- [First contact with `kubectl`](#toc-first-contact-with-kubectl)

- [Setting up Kubernetes](#toc-setting-up-kubernetes)

.debug[(auto-generated TOC)]
---
name: toc-chapter-6

## Chapter 6

- [Running our first containers on Kubernetes](#toc-running-our-first-containers-on-kubernetes)

- [Exposing containers](#toc-exposing-containers)

- [Deploying a self-hosted registry](#toc-deploying-a-self-hosted-registry)

- [Exposing services internally](#toc-exposing-services-internally)

- [Exposing services for external access](#toc-exposing-services-for-external-access)

.debug[(auto-generated TOC)]
---
name: toc-chapter-7

## Chapter 7

- [The Kubernetes dashboard](#toc-the-kubernetes-dashboard)

- [Security implications of `kubectl apply`](#toc-security-implications-of-kubectl-apply)

- [Scaling a deployment](#toc-scaling-a-deployment)

- [Daemon sets](#toc-daemon-sets)

- [Updating a service through labels and selectors](#toc-updating-a-service-through-labels-and-selectors)

- [Rolling updates](#toc-rolling-updates)

.debug[(auto-generated TOC)]
---
name: toc-chapter-8

## Chapter 8

- [Accessing logs from the CLI](#toc-accessing-logs-from-the-cli)

- [Managing stacks with Helm](#toc-managing-stacks-with-helm)

- [Namespaces](#toc-namespaces)

- [Next steps](#toc-next-steps)

- [Links and resources](#toc-links-and-resources)

.debug[(auto-generated TOC)]



.debug[[shared/toc.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/toc.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-docker-ft-overview
class: title

Docker 30,000ft overview

.nav[
[Previous section](#toc-)
|
[Back to table of contents](#toc-chapter-1)
|
[Next section](#toc-our-training-environment)
]

.debug[(automatically generated title slide)]

---
# Docker 30,000ft overview

In this lesson, we will learn about:

* Why containers (non-technical elevator pitch)

* Why containers (technical elevator pitch)

* How Docker helps us to build, ship, and run

* The history of containers

We won't actually run Docker or containers in this chapter (yet!).

Don't worry, we will get to that fast enough!

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

## Elevator pitch

### (for your manager, your boss...)

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

## OK... Why the buzz around containers?

* The software industry has changed

* Before:
  * monolithic applications
  * long development cycles
  * single environment
  * slowly scaling up

* Now:
  * decoupled services
  * fast, iterative improvements
  * multiple environments
  * quickly scaling out

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

## Deployment becomes very complex

* Many different stacks:
  * languages
  * frameworks
  * databases

* Many different targets:
  * individual development environments
  * pre-production, QA, staging...
  * production: on prem, cloud, hybrid

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: pic

## The deployment problem

![problem](images/shipping-software-problem.png)

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: pic

## The matrix from hell

![matrix](images/shipping-matrix-from-hell.png)

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: pic

## The parallel with the shipping industry

![history](images/shipping-industry-problem.png)

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: pic

## Intermodal shipping containers

![shipping](images/shipping-industry-solution.png)

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: pic

## A new shipping ecosystem

![shipeco](images/shipping-indsutry-results.png)

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: pic

## A shipping container system for applications

![shipapp](images/shipping-software-solution.png)

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: pic

## Eliminate the matrix from hell

![elimatrix](images/shipping-matrix-solved.png)

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

## Results

* [Dev-to-prod reduced from 9 months to 15 minutes (ING)](
  https://www.docker.com/sites/default/files/CS_ING_01.25.2015_1.pdf)

* [Continuous integration job time reduced by more than 60% (BBC)](
  https://www.docker.com/sites/default/files/CS_BBCNews_01.25.2015_1.pdf)

* [Deploy 100 times a day instead of once a week (GILT)](
  https://www.docker.com/sites/default/files/CS_Gilt%20Groupe_03.18.2015_0.pdf)

* [70% infrastructure consolidation (MetLife)](
  https://www.docker.com/customers/metlife-transforms-customer-experience-legacy-and-microservices-mashup)

* [60% infrastructure consolidation (Intesa Sanpaolo)](
  https://blog.docker.com/2017/11/intesa-sanpaolo-builds-resilient-foundation-banking-docker-enterprise-edition/)

* [14x application density; 60% of legacy datacenter migrated in 4 months (GE Appliances)](
  https://www.docker.com/customers/ge-uses-docker-enable-self-service-their-developers)

* etc.

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

## Elevator pitch

### (for your fellow devs and ops)

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

## Escape dependency hell

1. Write installation instructions into an `INSTALL.txt` file

2. Using this file, write an `install.sh` script that works *for you*

3. Turn this file into a `Dockerfile`, test it on your machine

4. If the Dockerfile builds on your machine, it will build *anywhere*

5. Rejoice as you escape dependency hell and "works on my machine"

Never again "worked in dev - ops problem now!"

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

## On-board developers and contributors rapidly

1. Write Dockerfiles for your application components

2. Use pre-made images from the Docker Hub (mysql, redis...)

3. Describe your stack with a Compose file

4. On-board somebody with two commands:

```bash
git clone ...
docker-compose up
```

With this, you can create development, integration, QA environments in minutes!

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: extra-details

## Implement reliable CI easily

1. Build test environment with a Dockerfile or Compose file

2. For each test run, stage up a new container or stack

3. Each run is now in a clean environment

4. No pollution from previous tests

Way faster and cheaper than creating VMs each time!

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: extra-details

## Use container images as build artefacts

1. Build your app from Dockerfiles

2. Store the resulting images in a registry

3. Keep them forever (or as long as necessary)

4. Test those images in QA, CI, integration...

5. Run the same images in production

6. Something goes wrong? Rollback to previous image

7. Investigating old regression? Old image has your back!

Images contain all the libraries, dependencies, etc. needed to run the app.

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: extra-details

## Decouple "plumbing" from application logic

1. Write your code to connect to named services ("db", "api"...)

2. Use Compose to start your stack

3. Docker will setup per-container DNS resolver for those names

4. You can now scale, add load balancers, replication ... without changing your code

Note: this is not covered in this intro level workshop!

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: extra-details

## What did Docker bring to the table?

### Docker before/after

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: extra-details

## Formats and APIs, before Docker

* No standardized exchange format.
  <br/>(No, a rootfs tarball is *not* a format!)

* Containers are hard to use for developers.
  <br/>(Where's the equivalent of `docker run debian`?)

* As a result, they are *hidden* from the end users.

* No re-usable components, APIs, tools.
  <br/>(At best: VM abstractions, e.g. libvirt.)

Analogy: 

* Shipping containers are not just steel boxes.
* They are steel boxes that are a standard size, with the same hooks and holes.

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: extra-details

## Formats and APIs, after Docker

* Standardize the container format, because containers were not portable.

* Make containers easy to use for developers.

* Emphasis on re-usable components, APIs, ecosystem of standard tools.

* Improvement over ad-hoc, in-house, specific tools.

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: extra-details

## Shipping, before Docker

* Ship packages: deb, rpm, gem, jar, homebrew...

* Dependency hell.

* "Works on my machine."

* Base deployment often done from scratch (debootstrap...) and unreliable.

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: extra-details

## Shipping, after Docker

* Ship container images with all their dependencies.

* Images are bigger, but they are broken down into layers.

* Only ship layers that have changed.

* Save disk, network, memory usage.

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: extra-details

## Example

Layers:

* CentOS
* JRE
* Tomcat
* Dependencies
* Application JAR
* Configuration

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: extra-details

## Devs vs Ops, before Docker

* Drop a tarball (or a commit hash) with instructions.

* Dev environment very different from production.

* Ops don't always have a dev environment themselves ...

* ... and when they do, it can differ from the devs'.

* Ops have to sort out differences and make it work ...

* ... or bounce it back to devs.

* Shipping code causes frictions and delays.

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: extra-details

## Devs vs Ops, after Docker

* Drop a container image or a Compose file.

* Ops can always run that container image.

* Ops can always run that Compose file.

* Ops still have to adapt to prod environment,
  but at least they have a reference point.

* Ops have tools allowing to use the same image
  in dev and prod.

* Devs can be empowered to make releases themselves
  more easily.

.debug[[containers/Docker_Overview.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Docker_Overview.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-our-training-environment
class: title

Our training environment

.nav[
[Previous section](#toc-docker-ft-overview)
|
[Back to table of contents](#toc-chapter-1)
|
[Next section](#toc-installing-docker)
]

.debug[(automatically generated title slide)]

---
class: title

# Our training environment

![SSH terminal](images/title-our-training-environment.jpg)

.debug[[containers/Training_Environment.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Training_Environment.md)]
---

## Our training environment

- If you are attending a tutorial or workshop:

  - a VM has been provisioned for each student

- If you are doing or re-doing this course on your own, you can:

  - install Docker locally (as explained in the chapter "Installing Docker")

  - install Docker on e.g. a cloud VM

  - use http://www.play-with-docker.com/ to instantly get a training environment

.debug[[containers/Training_Environment.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Training_Environment.md)]
---

## Our Docker VM

*This section assumes that you are following this course as part of
a tutorial, training or workshop, where each student is given an
individual Docker VM.*

- The VM is created just before the training.

- It will stay up during the whole training.

- It will be destroyed shortly after the training.

- It comes pre-loaded with Docker and some other useful tools.

.debug[[containers/Training_Environment.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Training_Environment.md)]
---

## What *is* Docker?

- "Installing Docker" really means "Installing the Docker Engine and CLI".

- The Docker Engine is a daemon (a service running in the background).

- This daemon manages containers, the same way that an hypervisor manages VMs.

- We interact with the Docker Engine by using the Docker CLI.

- The Docker CLI and the Docker Engine communicate through an API.

- There are many other programs, and many client libraries, to use that API.

.debug[[containers/Training_Environment.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Training_Environment.md)]
---

## Why don't we run Docker locally?

- We are going to download container images and distribution packages.

- This could put a bit of stress on the local WiFi and slow us down.

- Instead, we use a remote VM that has a good connectivity

- In some rare cases, installing Docker locally is challenging:

  - no administrator/root access (computer managed by strict corp IT)

  - 32-bit CPU or OS

  - old OS version (e.g. CentOS 6, OSX pre-Yosemite, Windows 7)

- It's better to spend time learning containers than fiddling with the installer!

.debug[[containers/Training_Environment.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Training_Environment.md)]
---

## Connecting to your Virtual Machine

You need an SSH client.

* On OS X, Linux, and other UNIX systems, just use `ssh`:

```bash
$ ssh <login>@<ip-address>
```

* On Windows, if you don't have an SSH client, you can download:

  * Putty (www.putty.org)

  * Git BASH (https://git-for-windows.github.io/)

  * MobaXterm (http://moabaxterm.mobatek.net)

.debug[[containers/Training_Environment.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Training_Environment.md)]
---

## Checking your Virtual Machine

Once logged in, make sure that you can run a basic Docker command:

.small[
```bash
$ docker version
Client:
 Version:       18.03.0-ce
 API version:   1.37
 Go version:    go1.9.4
 Git commit:    0520e24
 Built:         Wed Mar 21 23:10:06 2018
 OS/Arch:       linux/amd64
 Experimental:  false
 Orchestrator:  swarm

Server:
 Engine:
  Version:      18.03.0-ce
  API version:  1.37 (minimum version 1.12)
  Go version:   go1.9.4
  Git commit:   0520e24
  Built:        Wed Mar 21 23:08:35 2018
  OS/Arch:      linux/amd64
  Experimental: false
```
]

If this doesn't work, raise your hand so that an instructor can assist you!

.debug[[containers/Training_Environment.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Training_Environment.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-installing-docker
class: title

Installing Docker

.nav[
[Previous section](#toc-our-training-environment)
|
[Back to table of contents](#toc-chapter-1)
|
[Next section](#toc-our-first-containers)
]

.debug[(automatically generated title slide)]

---
class: title

# Installing Docker

![install](images/title-installing-docker.jpg)

.debug[[containers/Installing_Docker.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Installing_Docker.md)]
---

## Objectives

At the end of this lesson, you will know:

* How to install Docker.

* When to use `sudo` when running Docker commands.

*Note:* if you were provided with a training VM for a hands-on
tutorial, you can skip this chapter, since that VM already
has Docker installed, and Docker has already been setup to run
without `sudo`.

.debug[[containers/Installing_Docker.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Installing_Docker.md)]
---

## Installing Docker

There are many ways to install Docker.

We can arbitrarily distinguish:

* Installing Docker on an existing Linux machine (physical or VM)

* Installing Docker on macOS or Windows

* Installing Docker on a fleet of cloud VMs

.debug[[containers/Installing_Docker.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Installing_Docker.md)]
---

## Installing Docker on Linux

* The recommended method is to install the packages supplied by Docker Inc.:

  https://store.docker.com

* The general method is:

  - add Docker Inc.'s package repositories to your system configuration

  - install the Docker Engine

* Detailed installation instructions (distro by distro) are available on:

  https://docs.docker.com/engine/installation/

* You can also install from binaries (if your distro is not supported):

  https://docs.docker.com/engine/installation/linux/docker-ce/binaries/

.debug[[containers/Installing_Docker.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Installing_Docker.md)]
---

class: extra-details

## Docker Inc. packages vs distribution packages

* Docker Inc. releases new versions monthly (edge) and quarterly (stable)

* Releases are immediately available on Docker Inc.'s package repositories

* Linux distros don't always update to the latest Docker version

  (Sometimes, updating would break their guidelines for major/minor upgrades)

* Sometimes, some distros have carried packages with custom patches

* Sometimes, these patches added critical security bugs ‚òπ

* Installing through Docker Inc.'s repositories is a bit of extra work ‚Ä¶

  ‚Ä¶ but it is generally worth it!

.debug[[containers/Installing_Docker.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Installing_Docker.md)]
---

## Installing Docker on macOS and Windows

* On macOS, the recommended method is to use Docker for Mac:

  https://docs.docker.com/docker-for-mac/install/

* On Windows 10 Pro, Enterprise, and Education, you can use Docker for Windows:

  https://docs.docker.com/docker-for-windows/install/

* On older versions of Windows, you can use the Docker Toolbox:

  https://docs.docker.com/toolbox/toolbox_install_windows/

* On Windows Server 2016, you can also install the native engine:

  https://docs.docker.com/install/windows/docker-ee/

.debug[[containers/Installing_Docker.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Installing_Docker.md)]
---

## Docker for Mac and Docker for Windows

* Special Docker Editions that integrate well with their respective host OS

* Provide user-friendly GUI to edit Docker configuration and settings

* Leverage the host OS virtualization subsystem (e.g. the [Hypervisor API](https://developer.apple.com/documentation/hypervisor) on macOS)

* Installed like normal user applications on the host

* Under the hood, they both run a tiny VM (transparent to our daily use)

* Access network resources like normal applications
  <br/>(and therefore, play better with enterprise VPNs and firewalls)

* Support filesystem sharing through volumes (we'll talk about this later)

* They only support running one Docker VM at a time ...
  <br/>
  ... but we can use `docker-machine`, the Docker Toolbox, VirtualBox, etc. to get a cluster.

.debug[[containers/Installing_Docker.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Installing_Docker.md)]
---

## Running Docker on macOS and Windows

When you execute `docker version` from the terminal:

* the CLI connects to the Docker Engine over a standard socket,
* the Docker Engine is, in fact, running in a VM,
* ... but the CLI doesn't know or care about that,
* the CLI sends a request using the REST API,
* the Docker Engine in the VM processes the request,
* the CLI gets the response and displays it to you.

All communication with the Docker Engine happens over the API.

This will also allow to use remote Engines exactly as if they were local.

.debug[[containers/Installing_Docker.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Installing_Docker.md)]
---

## Important PSA about security

* If you have access to the Docker control socket, you can take over the machine

  (Because you can run containers that will access the machine's resources)

* Therefore, on Linux machines, the `docker` user is equivalent to `root`

* You should restrict access to it like you would protect `root`

* By default, the Docker control socket belongs to the `docker` group

* You can add trusted users to the `docker` group

* Otherwise, you will have to prefix every `docker` command with `sudo`, e.g.:

  ```bash
  sudo docker version
  ```

.debug[[containers/Installing_Docker.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Installing_Docker.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-our-first-containers
class: title

Our first containers

.nav[
[Previous section](#toc-installing-docker)
|
[Back to table of contents](#toc-chapter-1)
|
[Next section](#toc-background-containers)
]

.debug[(automatically generated title slide)]

---

class: title

# Our first containers

![Colorful plastic tubs](images/title-our-first-containers.jpg)

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

## Objectives

At the end of this lesson, you will have:

* Seen Docker in action.

* Started your first containers.

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

## Hello World

In your Docker environment, just run the following command:

```bash
$ docker run busybox echo hello world
hello world
```

(If your Docker install is brand new, you will also see a few extra lines,
corresponding to the download of the `busybox` image.)

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

## That was our first container!

* We used one of the smallest, simplest images available: `busybox`.

* `busybox` is typically used in embedded systems (phones, routers...)

* We ran a single process and echo'ed `hello world`.

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

## A more useful container

Let's run a more exciting container:

```bash
$ docker run -it ubuntu
root@04c0bb0a6c07:/#
```

* This is a brand new container.

* It runs a bare-bones, no-frills `ubuntu` system.

* `-it` is shorthand for `-i -t`.

  * `-i` tells Docker to connect us to the container's stdin.

  * `-t` tells Docker that we want a pseudo-terminal.

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

## Do something in our container

Try to run `figlet` in our container.

```bash
root@04c0bb0a6c07:/# figlet hello
bash: figlet: command not found
```

Alright, we need to install it.

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

## Install a package in our container

We want `figlet`, so let's install it:

```bash
root@04c0bb0a6c07:/# apt-get update
...
Fetched 1514 kB in 14s (103 kB/s)
Reading package lists... Done
root@04c0bb0a6c07:/# apt-get install figlet
Reading package lists... Done
...
```

One minute later, `figlet` is installed!

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

## Try to run our freshly installed program

The `figlet` program takes a message as parameter.

```bash
root@04c0bb0a6c07:/# figlet hello
 _          _ _       
| |__   ___| | | ___  
| '_ \ / _ \ | |/ _ \ 
| | | |  __/ | | (_) |
|_| |_|\___|_|_|\___/ 
```

Beautiful! .emoji[üòç]

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

class: in-person

## Counting packages in the container

Let's check how many packages are installed there.

```bash
root@04c0bb0a6c07:/# dpkg -l | wc -l
190
```

* `dpkg -l` lists the packages installed in our container

* `wc -l` counts them

How many packages do we have on our host?

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

class: in-person

## Counting packages on the host

Exit the container by logging out of the shell, like you would usually do.

(E.g. with `^D` or `exit`)

```bash
root@04c0bb0a6c07:/# exit
```

Now, try to:

* run `dpkg -l | wc -l`. How many packages are installed?

* run `figlet`. Does that work?

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

class: self-paced

## Comparing the container and the host

Exit the container by logging out of the shell, with `^D` or `exit`.

Now try to run `figlet`. Does that work?

(It shouldn't; except if, by coincidence, you are running on a machine where figlet was installed before.)

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

## Host and containers are independent things

* We ran an `ubuntu` container on an Linux/Windows/macOS host.

* They have different, independent packages.

* Installing something on the host doesn't expose it to the container.

* And vice-versa.

* Even if both the host and the container have the same Linux distro!

* We can run *any container* on *any host*.

  (One exception: Windows containers cannot run on Linux machines; at least not yet.)

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

## Where's our container?

* Our container is now in a *stopped* state.

* It still exists on disk, but all compute resources have been freed up.

* We will see later how to get back to that container.

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

## Starting another container

What if we start a new container, and try to run `figlet` again?
 
```bash
$ docker run -it ubuntu
root@b13c164401fb:/# figlet
bash: figlet: command not found
```

* We started a *brand new container*.

* The basic Ubuntu image was used, and `figlet` is not here.

* We will see in the next chapters how to bake a custom image with `figlet`.

.debug[[containers/First_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/First_Containers.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/chinook-helicopter-container.jpg)]

---

name: toc-background-containers
class: title

Background containers

.nav[
[Previous section](#toc-our-first-containers)
|
[Back to table of contents](#toc-chapter-1)
|
[Next section](#toc-restarting-and-attaching-to-containers)
]

.debug[(automatically generated title slide)]

---

class: title

# Background containers

![Background containers](images/title-background-containers.jpg)

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## Objectives

Our first containers were *interactive*.

We will now see how to:

* Run a non-interactive container.
* Run a container in the background.
* List running containers.
* Check the logs of a container.
* Stop a container.
* List stopped containers.

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## A non-interactive container

We will run a small custom container.

This container just displays the time every second.

```bash
$ docker run jpetazzo/clock
Fri Feb 20 00:28:53 UTC 2015
Fri Feb 20 00:28:54 UTC 2015
Fri Feb 20 00:28:55 UTC 2015
...
```

* This container will run forever.
* To stop it, press `^C`.
* Docker has automatically downloaded the image `jpetazzo/clock`.
* This image is a user image, created by `jpetazzo`.
* We will hear more about user images (and other types of images) later.

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## Run a container in the background

Containers can be started in the background, with the `-d` flag (daemon mode):

```bash
$ docker run -d jpetazzo/clock
47d677dcfba4277c6cc68fcaa51f932b544cab1a187c853b7d0caf4e8debe5ad
```

* We don't see the output of the container.
* But don't worry: Docker collects that output and logs it!
* Docker gives us the ID of the container.

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## List running containers

How can we check that our container is still running?

With `docker ps`, just like the UNIX `ps` command, lists running processes.

```bash
$ docker ps
CONTAINER ID  IMAGE           ...  CREATED        STATUS        ...
47d677dcfba4  jpetazzo/clock  ...  2 minutes ago  Up 2 minutes  ...
```

Docker tells us:

* The (truncated) ID of our container.
* The image used to start the container.
* That our container has been running (`Up`) for a couple of minutes.
* Other information (COMMAND, PORTS, NAMES) that we will explain later.

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## Starting more containers

Let's start two more containers.

```bash
$ docker run -d jpetazzo/clock
57ad9bdfc06bb4407c47220cf59ce21585dce9a1298d7a67488359aeaea8ae2a
```

```bash
$ docker run -d jpetazzo/clock
068cc994ffd0190bbe025ba74e4c0771a5d8f14734af772ddee8dc1aaf20567d
```

Check that `docker ps` correctly reports all 3 containers.

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## Viewing only the last container started

When many containers are already running, it can be useful to
see only the last container that was started.

This can be achieved with the `-l` ("Last") flag:

```bash
$ docker ps -l
CONTAINER ID  IMAGE           ...  CREATED        STATUS        ...
068cc994ffd0  jpetazzo/clock  ...  2 minutes ago  Up 2 minutes  ...
```

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## View only the IDs of the containers

Many Docker commands will work on container IDs: `docker stop`, `docker rm`...

If we want to list only the IDs of our containers (without the other columns
or the header line),
we can use the `-q` ("Quiet", "Quick") flag:

```bash
$ docker ps -q
068cc994ffd0
57ad9bdfc06b
47d677dcfba4
```

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## Combining flags

We can combine `-l` and `-q` to see only the ID of the last container started:

```bash
$ docker ps -lq
068cc994ffd0
```

At a first glance, it looks like this would be particularly useful in scripts.

However, if we want to start a container and get its ID in a reliable way,
it is better to use `docker run -d`, which we will cover in a bit.

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## View the logs of a container

We told you that Docker was logging the container output.

Let's see that now.

```bash
$ docker logs 068
Fri Feb 20 00:39:52 UTC 2015
Fri Feb 20 00:39:53 UTC 2015
...
```

* We specified a *prefix* of the full container ID.
* You can, of course, specify the full ID.
* The `logs` command will output the *entire* logs of the container.
  <br/>(Sometimes, that will be too much. Let's see how to address that.)

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## View only the tail of the logs

To avoid being spammed with eleventy pages of output,
we can use the `--tail` option:

```bash
$ docker logs --tail 3 068
Fri Feb 20 00:55:35 UTC 2015
Fri Feb 20 00:55:36 UTC 2015
Fri Feb 20 00:55:37 UTC 2015
```

* The parameter is the number of lines that we want to see.

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## Follow the logs in real time

Just like with the standard UNIX command `tail -f`, we can
follow the logs of our container:

```bash
$ docker logs --tail 1 --follow 068
Fri Feb 20 00:57:12 UTC 2015
Fri Feb 20 00:57:13 UTC 2015
^C
```

* This will display the last line in the log file.
* Then, it will continue to display the logs in real time.
* Use `^C` to exit.

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## Stop our container

There are two ways we can terminate our detached container.

* Killing it using the `docker kill` command.
* Stopping it using the `docker stop` command.

The first one stops the container immediately, by using the
`KILL` signal.

The second one is more graceful. It sends a `TERM` signal,
and after 10 seconds, if the container has not stopped, it
sends `KILL.`

Reminder: the `KILL` signal cannot be intercepted, and will
forcibly terminate the container.

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## Stopping our containers

Let's stop one of those containers:

```bash
$ docker stop 47d6
47d6
```

This will take 10 seconds:

* Docker sends the TERM signal;
* the container doesn't react to this signal
  (it's a simple Shell script with no special
  signal handling);
* 10 seconds later, since the container is still
  running, Docker sends the KILL signal;
* this terminates the container.

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## Killing the remaining containers

Let's be less patient with the two other containers:

```bash
$ docker kill 068 57ad
068
57ad
```

The `stop` and `kill` commands can take multiple container IDs.

Those containers will be terminated immediately (without
the 10 seconds delay).

Let's check that our containers don't show up anymore:

```bash
$ docker ps
```

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

## List stopped containers

We can also see stopped containers, with the `-a` (`--all`) option.

```bash
$ docker ps -a
CONTAINER ID  IMAGE           ...  CREATED      STATUS
068cc994ffd0  jpetazzo/clock  ...  21 min. ago  Exited (137) 3 min. ago
57ad9bdfc06b  jpetazzo/clock  ...  21 min. ago  Exited (137) 3 min. ago
47d677dcfba4  jpetazzo/clock  ...  23 min. ago  Exited (137) 3 min. ago
5c1dfd4d81f1  jpetazzo/clock  ...  40 min. ago  Exited (0) 40 min. ago
b13c164401fb  ubuntu          ...  55 min. ago  Exited (130) 53 min. ago
```

.debug[[containers/Background_Containers.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Background_Containers.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-cranes.jpg)]

---

name: toc-restarting-and-attaching-to-containers
class: title

Restarting and attaching to containers

.nav[
[Previous section](#toc-background-containers)
|
[Back to table of contents](#toc-chapter-1)
|
[Next section](#toc-understanding-docker-images)
]

.debug[(automatically generated title slide)]

---
# Restarting and attaching to containers

We have started containers in the foreground, and in the background.

In this chapter, we will see how to:

* Put a container in the background.
* Attach to a background container to bring it to the foreground.
* Restart a stopped container.

.debug[[containers/Start_And_Attach.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Start_And_Attach.md)]
---

## Background and foreground

The distinction between foreground and background containers is arbitrary.

From Docker's point of view, all containers are the same.

All containers run the same way, whether there is a client attached to them or not.

It is always possible to detach from a container, and to reattach to a container.

Analogy: attaching to a container is like plugging a keyboard and screen to a physical server.

.debug[[containers/Start_And_Attach.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Start_And_Attach.md)]
---

## Detaching from a container

* If you have started an *interactive* container (with option `-it`), you can detach from it.

* The "detach" sequence is `^P^Q`.

* Otherwise you can detach by killing the Docker client.
  
  (But not by hitting `^C`, as this would deliver `SIGINT` to the container.)

What does `-it` stand for?

* `-t` means "allocate a terminal."
* `-i` means "connect stdin to the terminal."

.debug[[containers/Start_And_Attach.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Start_And_Attach.md)]
---

class: extra-details

## Specifying a custom detach sequence

* You don't like `^P^Q`? No problem!
* You can change the sequence with `docker run --detach-keys`.
* This can also be passed as a global option to the engine.

Start a container with a custom detach command:

```bash
$ docker run -ti --detach-keys ctrl-x,x jpetazzo/clock
```

Detach by hitting `^X x`. (This is ctrl-x then x, not ctrl-x twice!)

Check that our container is still running:

```bash
$ docker ps -l
```

.debug[[containers/Start_And_Attach.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Start_And_Attach.md)]
---

class: extra-details

## Attaching to a container

You can attach to a container:

```bash
$ docker attach <containerID>
```

* The container must be running.
* There *can* be multiple clients attached to the same container.
* If you don't specify `--detach-keys` when attaching, it defaults back to `^P^Q`.

Try it on our previous container:

```bash
$ docker attach $(docker ps -lq)
```

Check that `^X x` doesn't work, but `^P ^Q` does.

.debug[[containers/Start_And_Attach.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Start_And_Attach.md)]
---

## Detaching from non-interactive containers

* **Warning:** if the container was started without `-it`...

  * You won't be able to detach with `^P^Q`.
  * If you hit `^C`, the signal will be proxied to the container.

* Remember: you can always detach by killing the Docker client.

.debug[[containers/Start_And_Attach.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Start_And_Attach.md)]
---

## Checking container output

* Use `docker attach` if you intend to send input to the container.

* If you just want to see the output of a container, use `docker logs`.

```bash
$ docker logs --tail 1 --follow <containerID>
```

.debug[[containers/Start_And_Attach.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Start_And_Attach.md)]
---

## Restarting a container

When a container has exited, it is in stopped state.

It can then be restarted with the `start` command.

```bash
$ docker start <yourContainerID>
```

The container will be restarted using the same options you launched it
with.

You can re-attach to it if you want to interact with it:

```bash
$ docker attach <yourContainerID>
```

Use `docker ps -a` to identify the container ID of a previous `jpetazzo/clock` container,
and try those commands.

.debug[[containers/Start_And_Attach.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Start_And_Attach.md)]
---

## Attaching to a REPL

* REPL = Read Eval Print Loop

* Shells, interpreters, TUI ...

* Symptom: you `docker attach`, and see nothing

* The REPL doesn't know that you just attached, and doesn't print anything

* Try hitting `^L` or `Enter`

.debug[[containers/Start_And_Attach.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Start_And_Attach.md)]
---

class: extra-details

## SIGWINCH

* When you `docker attach`, the Docker Engine sends SIGWINCH signals to the container.

* SIGWINCH = WINdow CHange; indicates a change in window size.

* This will cause some CLI and TUI programs to redraw the screen.

* But not all of them.

.debug[[containers/Start_And_Attach.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Start_And_Attach.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-housing.jpg)]

---

name: toc-understanding-docker-images
class: title

Understanding Docker images

.nav[
[Previous section](#toc-restarting-and-attaching-to-containers)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-building-images-interactively)
]

.debug[(automatically generated title slide)]

---

class: title

# Understanding Docker images

![image](images/title-understanding-docker-images.png)

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Objectives

In this section, we will explain:

* What is an image.

* What is a layer.

* The various image namespaces.

* How to search and download images.

* Image tags and when to use them.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## What is an image?

* Image = files + metadata

* These files form the root filesystem of our container.

* The metadata can indicate a number of things, e.g.:

  * the author of the image
  * the command to execute in the container when starting it
  * environment variables to be set
  * etc.

* Images are made of *layers*, conceptually stacked on top of each other.

* Each layer can add, change, and remove files and/or metadata.

* Images can share layers to optimize disk usage, transfer times, and memory use.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Example for a Java webapp

Each of the following items will correspond to one layer:

* CentOS base layer
* Packages and configuration files added by our local IT
* JRE
* Tomcat
* Our application's dependencies
* Our application code and assets
* Our application configuration

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

class: pic

## The read-write layer

![layers](images/container-layers.jpg)

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

class: pic

## Multiple containers sharing the same image

![layers](images/sharing-layers.jpg)

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Differences between containers and images

* An image is a read-only filesystem.

* A container is an encapsulated set of processes running in a
  read-write copy of that filesystem.

* To optimize container boot time, *copy-on-write* is used
  instead of regular copy.

* `docker run` starts a container from a given image.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Comparison with object-oriented programming

* Images are conceptually similar to *classes*.

* Layers are conceptually similar to *inheritance*.

* Containers are conceptually similar to *instances*.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Wait a minute...

If an image is read-only, how do we change it?

* We don't.

* We create a new container from that image.

* Then we make changes to that container.

* When we are satisfied with those changes, we transform them into a new layer.

* A new image is created by stacking the new layer on top of the old image.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## A chicken-and-egg problem

* The only way to create an image is by "freezing" a container.

* The only way to create a container is by instanciating an image.

* Help!

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Creating the first images

There is a special empty image called `scratch`.

* It allows to *build from scratch*.

The `docker import` command loads a tarball into Docker.

* The imported tarball becomes a standalone image.
* That new image has a single layer.

Note: you will probably never have to do this yourself.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Creating other images

`docker commit`

* Saves all the changes made to a container into a new layer.
* Creates a new image (effectively a copy of the container).

`docker build` **(used 99% of the time)**

* Performs a repeatable build sequence.
* This is the preferred method!

We will explain both methods in a moment.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Images namespaces

There are three namespaces:

* Official images

    e.g. `ubuntu`, `busybox` ...

* User (and organizations) images

    e.g. `jpetazzo/clock`

* Self-hosted images

    e.g. `registry.example.com:5000/my-private/image`

Let's explain each of them.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Root namespace

The root namespace is for official images. They are put there by Docker Inc.,
but they are generally authored and maintained by third parties.

Those images include:

* Small, "swiss-army-knife" images like busybox.

* Distro images to be used as bases for your builds, like ubuntu, fedora...

* Ready-to-use components and services, like redis, postgresql...

* Over 130 at this point!

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## User namespace

The user namespace holds images for Docker Hub users and organizations.

For example:

```bash
jpetazzo/clock
```

The Docker Hub user is:

```bash
jpetazzo
```

The image name is:

```bash
clock
```

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Self-Hosted namespace

This namespace holds images which are not hosted on Docker Hub, but on third
party registries.

They contain the hostname (or IP address), and optionally the port, of the
registry server.

For example:

```bash
localhost:5000/wordpress
```

* `localhost:5000` is the host and port of the registry
* `wordpress` is the name of the image

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## How do you store and manage images?

Images can be stored:

* On your Docker host.
* In a Docker registry.

You can use the Docker client to download (pull) or upload (push) images.

To be more accurate: you can use the Docker client to tell a Docker Engine
to push and pull images to and from a registry.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Showing current images

Let's look at what images are on our host now.

```bash
$ docker images
REPOSITORY       TAG       IMAGE ID       CREATED         SIZE
fedora           latest    ddd5c9c1d0f2   3 days ago      204.7 MB
centos           latest    d0e7f81ca65c   3 days ago      196.6 MB
ubuntu           latest    07c86167cdc4   4 days ago      188 MB
redis            latest    4f5f397d4b7c   5 days ago      177.6 MB
postgres         latest    afe2b5e1859b   5 days ago      264.5 MB
alpine           latest    70c557e50ed6   5 days ago      4.798 MB
debian           latest    f50f9524513f   6 days ago      125.1 MB
busybox          latest    3240943c9ea3   2 weeks ago     1.114 MB
training/namer   latest    902673acc741   9 months ago    289.3 MB
jpetazzo/clock   latest    12068b93616f   12 months ago   2.433 MB
```

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Searching for images

We cannot list *all* images on a remote registry, but
we can search for a specific keyword:

```bash
$ docker search marathon
NAME                     DESCRIPTION                     STARS  OFFICIAL  AUTOMATED
mesosphere/marathon      A cluster-wide init and co...   105              [OK]
mesoscloud/marathon      Marathon                        31               [OK]
mesosphere/marathon-lb   Script to update haproxy b...   22               [OK]
tobilg/mongodb-marathon  A Docker image to start a ...   4                [OK]
```


* "Stars" indicate the popularity of the image.

* "Official" images are those in the root namespace.

* "Automated" images are built automatically by the Docker Hub.
  <br/>(This means that their build recipe is always available.)

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Downloading images

There are two ways to download images.

* Explicitly, with `docker pull`.

* Implicitly, when executing `docker run` and the image is not found locally.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Pulling an image

```bash
$ docker pull debian:jessie
Pulling repository debian
b164861940b8: Download complete
b164861940b8: Pulling image (jessie) from debian
d1881793a057: Download complete
```

* As seen previously, images are made up of layers.

* Docker has downloaded all the necessary layers.

* In this example, `:jessie` indicates which exact version of Debian
  we would like.

  It is a *version tag*.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Image and tags

* Images can have tags.

* Tags define image versions or variants.

* `docker pull ubuntu` will refer to `ubuntu:latest`.

* The `:latest` tag is generally updated often.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## When to (not) use tags

Don't specify tags:

* When doing rapid testing and prototyping.
* When experimenting.
* When you want the latest version.

Do specify tags:

* When recording a procedure into a script.
* When going to production.
* To ensure that the same version will be used everywhere.
* To ensure repeatability later.

.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

## Section summary

We've learned how to:

* Understand images and layers.
* Understand Docker image namespacing.
* Search and download images.


.debug[[containers/Initial_Images.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Initial_Images.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/containers-by-the-water.jpg)]

---

name: toc-building-images-interactively
class: title

Building images interactively

.nav[
[Previous section](#toc-understanding-docker-images)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-building-docker-images-with-a-dockerfile)
]

.debug[(automatically generated title slide)]

---
# Building images interactively

In this section, we will create our first container image.

It will be a basic distribution image, but we will pre-install
the package `figlet`.

We will: 

* Create a container from a base image.

* Install software manually in the container, and turn it
  into a new image.

* Learn about new commands: `docker commit`, `docker tag`, and `docker diff`.

.debug[[containers/Building_Images_Interactively.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_Interactively.md)]
---

## The plan

1. Create a container (with `docker run`) using our base distro of choice.

2. Run a bunch of commands to install and set up our software in the container.

3. (Optionally) review changes in the container with `docker diff`.

4. Turn the container into a new image with `docker commit`.

5. (Optionally) add tags to the image with `docker tag`.

.debug[[containers/Building_Images_Interactively.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_Interactively.md)]
---

## Setting up our container

Start an Ubuntu container:

```bash
$ docker run -it ubuntu
root@<yourContainerId>:#/
```

Run the command `apt-get update` to refresh the list of packages available to install.

Then run the command `apt-get install figlet` to install the program we are interested in.

```bash
root@<yourContainerId>:#/ apt-get update && apt-get install figlet
.... OUTPUT OF APT-GET COMMANDS ....
```

.debug[[containers/Building_Images_Interactively.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_Interactively.md)]
---

## Inspect the changes

Type `exit` at the container prompt to leave the interactive session.

Now let's run `docker diff` to see the difference between the base image
and our container.

```bash
$ docker diff <yourContainerId>
C /root
A /root/.bash_history
C /tmp
C /usr
C /usr/bin
A /usr/bin/figlet
...
```

.debug[[containers/Building_Images_Interactively.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_Interactively.md)]
---

class: x-extra-details

## Docker tracks filesystem changes

As explained before:

* An image is read-only.

* When we make changes, they happen in a copy of the image.

* Docker can show the difference between the image, and its copy.

* For performance, Docker uses copy-on-write systems.
  <br/>(i.e. starting a container based on a big image
  doesn't incur a huge copy.)

.debug[[containers/Building_Images_Interactively.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_Interactively.md)]
---

## Copy-on-write security benefits

* `docker diff` gives us an easy way to audit changes

  (√† la Tripwire)

* Containers can also be started in read-only mode

  (their root filesystem will be read-only, but they can still have read-write data volumes)


.debug[[containers/Building_Images_Interactively.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_Interactively.md)]
---

## Commit our changes into a new image

The `docker commit` command will create a new layer with those changes,
and a new image using this new layer.

```bash
$ docker commit <yourContainerId>
<newImageId>
```

The output of the `docker commit` command will be the ID for your newly created image.

We can use it as an argument to `docker run`.

.debug[[containers/Building_Images_Interactively.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_Interactively.md)]
---

## Testing our new image

Let's run this image:

```bash
$ docker run -it <newImageId>
root@fcfb62f0bfde:/# figlet hello
 _          _ _       
| |__   ___| | | ___  
| '_ \ / _ \ | |/ _ \ 
| | | |  __/ | | (_) |
|_| |_|\___|_|_|\___/ 
```

It works! .emoji[üéâ]

.debug[[containers/Building_Images_Interactively.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_Interactively.md)]
---

## Tagging images

Referring to an image by its ID is not convenient. Let's tag it instead.

We can use the `tag` command:

```bash
$ docker tag <newImageId> figlet
```

But we can also specify the tag as an extra argument to `commit`:

```bash
$ docker commit <containerId> figlet
```

And then run it using its tag:

```bash
$ docker run -it figlet
```

.debug[[containers/Building_Images_Interactively.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_Interactively.md)]
---

## What's next?

Manual process = bad.

Automated process = good.

In the next chapter, we will learn how to automate the build
process by writing a `Dockerfile`.

.debug[[containers/Building_Images_Interactively.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_Interactively.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/distillery-containers.jpg)]

---

name: toc-building-docker-images-with-a-dockerfile
class: title

Building Docker images with a Dockerfile

.nav[
[Previous section](#toc-building-images-interactively)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-cmd-and-entrypoint)
]

.debug[(automatically generated title slide)]

---

class: title

# Building Docker images with a Dockerfile

![Construction site with containers](images/title-building-docker-images-with-a-dockerfile.jpg)

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## Objectives

We will build a container image automatically, with a `Dockerfile`.

At the end of this lesson, you will be able to:

* Write a `Dockerfile`.

* Build an image from a `Dockerfile`.

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## `Dockerfile` overview

* A `Dockerfile` is a build recipe for a Docker image.

* It contains a series of instructions telling Docker how an image is constructed.

* The `docker build` command builds an image from a `Dockerfile`.

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## Writing our first `Dockerfile`

Our Dockerfile must be in a **new, empty directory**.

1. Create a directory to hold our `Dockerfile`.

```bash
$ mkdir myimage
```

2. Create a `Dockerfile` inside this directory.

```bash
$ cd myimage
$ vim Dockerfile
```

Of course, you can use any other editor of your choice.

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## Type this into our Dockerfile...

```dockerfile
FROM ubuntu
RUN apt-get update
RUN apt-get install figlet
```

* `FROM` indicates the base image for our build.

* Each `RUN` line will be executed by Docker during the build.

* Our `RUN` commands **must be non-interactive.**
  <br/>(No input can be provided to Docker during the build.)

* In many cases, we will add the `-y` flag to `apt-get`.

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## Build it!

Save our file, then execute:

```bash
$ docker build -t figlet .
```

* `-t` indicates the tag to apply to the image.

* `.` indicates the location of the *build context*.

We will talk more about the build context later.

To keep things simple for now: this is the directory where our Dockerfile is located.

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## What happens when we build the image?

The output of `docker build` looks like this:

.small[
```bash
docker build -t figlet .
Sending build context to Docker daemon  2.048kB
Step 1/3 : FROM ubuntu
 ---> f975c5035748
Step 2/3 : RUN apt-get update
 ---> Running in e01b294dbffd
(...output of the RUN command...)
Removing intermediate container e01b294dbffd
 ---> eb8d9b561b37
Step 3/3 : RUN apt-get install figlet
 ---> Running in c29230d70f9b
(...output of the RUN command...)
Removing intermediate container c29230d70f9b
 ---> 0dfd7a253f21
Successfully built 0dfd7a253f21
Successfully tagged figlet:latest
```
]

* The output of the `RUN` commands has been omitted.
* Let's explain what this output means.

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## Sending the build context to Docker

```bash
Sending build context to Docker daemon 2.048 kB
```

* The build context is the `.` directory given to `docker build`.

* It is sent (as an archive) by the Docker client to the Docker daemon.

* This allows to use a remote machine to build using local files.

* Be careful (or patient) if that directory is big and your link is slow.

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## Executing each step

```bash
Step 2/3 : RUN apt-get update
 ---> Running in e01b294dbffd
(...output of the RUN command...)
Removing intermediate container e01b294dbffd
 ---> eb8d9b561b37
```

* A container (`e01b294dbffd`) is created from the base image.

* The `RUN` command is executed in this container.

* The container is committed into an image (`eb8d9b561b37`).

* The build container (`e01b294dbffd`) is removed.

* The output of this step will be the base image for the next one.

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## The caching system

If you run the same build again, it will be instantaneous. Why?

* After each build step, Docker takes a snapshot of the resulting image.

* Before executing a step, Docker checks if it has already built the same sequence.

* Docker uses the exact strings defined in your Dockerfile, so:

  * `RUN apt-get install figlet cowsay ` 
    <br/> is different from
    <br/> `RUN apt-get install cowsay figlet`
  
  * `RUN apt-get update` is not re-executed when the mirrors are updated

You can force a rebuild with `docker build --no-cache ...`.

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## Running the image

The resulting image is not different from the one produced manually.

```bash
$ docker run -ti figlet
root@91f3c974c9a1:/# figlet hello
 _          _ _       
| |__   ___| | | ___  
| '_ \ / _ \ | |/ _ \ 
| | | |  __/ | | (_) |
|_| |_|\___|_|_|\___/ 
```


Yay! .emoji[üéâ]

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## Using image and viewing history

The `history` command lists all the layers composing an image.

For each layer, it shows its creation time, size, and creation command.

When an image was built with a Dockerfile, each layer corresponds to
a line of the Dockerfile.

```bash
$ docker history figlet
IMAGE         CREATED            CREATED BY                     SIZE
f9e8f1642759  About an hour ago  /bin/sh -c apt-get install fi  1.627 MB
7257c37726a1  About an hour ago  /bin/sh -c apt-get update      21.58 MB
07c86167cdc4  4 days ago         /bin/sh -c #(nop) CMD ["/bin   0 B
<missing>     4 days ago         /bin/sh -c sed -i 's/^#\s*\(   1.895 kB
<missing>     4 days ago         /bin/sh -c echo '#!/bin/sh'    194.5 kB
<missing>     4 days ago         /bin/sh -c #(nop) ADD file:b   187.8 MB
```

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## Introducing JSON syntax

Most Dockerfile arguments can be passed in two forms:

* plain string:
  <br/>`RUN apt-get install figlet`

* JSON list:
  <br/>`RUN ["apt-get", "install", "figlet"]`

We are going to change our Dockerfile to see how it affects the resulting image.

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## Using JSON syntax in our Dockerfile

Let's change our Dockerfile as follows!

```dockerfile
FROM ubuntu
RUN apt-get update
RUN ["apt-get", "install", "figlet"]
```

Then build the new Dockerfile.

```bash
$ docker build -t figlet .
```

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## JSON syntax vs string syntax

Compare the new history:

```bash
$ docker history figlet
IMAGE         CREATED            CREATED BY                     SIZE
27954bb5faaf  10 seconds ago     apt-get install figlet         1.627 MB
7257c37726a1  About an hour ago  /bin/sh -c apt-get update      21.58 MB
07c86167cdc4  4 days ago         /bin/sh -c #(nop) CMD ["/bin   0 B
<missing>     4 days ago         /bin/sh -c sed -i 's/^#\s*\(   1.895 kB
<missing>     4 days ago         /bin/sh -c echo '#!/bin/sh'    194.5 kB
<missing>     4 days ago         /bin/sh -c #(nop) ADD file:b   187.8 MB
```

* JSON syntax specifies an *exact* command to execute.

* String syntax specifies a command to be wrapped within `/bin/sh -c "..."`.

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

## When to use JSON syntax and string syntax

* String syntax:

  * is easier to write
  * interpolates environment variables and other shell expressions
  * creates an extra process (`/bin/sh -c ...`) to parse the string
  * requires `/bin/sh` to exist in the container

* JSON syntax:

  * is harder to write (and read!)
  * passes all arguments without extra processing
  * doesn't create an extra process
  * doesn't require `/bin/sh` to exist in the container

.debug[[containers/Building_Images_With_Dockerfiles.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Building_Images_With_Dockerfiles.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/lots-of-containers.jpg)]

---

name: toc-cmd-and-entrypoint
class: title

`CMD` and `ENTRYPOINT`

.nav[
[Previous section](#toc-building-docker-images-with-a-dockerfile)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-copying-files-during-the-build)
]

.debug[(automatically generated title slide)]

---

class: title

# `CMD` and `ENTRYPOINT`

![Container entry doors](images/entrypoint.jpg)

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Objectives

In this lesson, we will learn about two important
Dockerfile commands:

`CMD` and `ENTRYPOINT`.

These commands allow us to set the default command
to run in a container.

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Defining a default command

When people run our container, we want to greet them with a nice hello message, and using a custom font.

For that, we will execute:

```bash
figlet -f script hello
```

* `-f script` tells figlet to use a fancy font.

* `hello` is the message that we want it to display.

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Adding `CMD` to our Dockerfile

Our new Dockerfile will look like this:

```dockerfile
FROM ubuntu
RUN apt-get update
RUN ["apt-get", "install", "figlet"]
CMD figlet -f script hello
```

* `CMD` defines a default command to run when none is given.

* It can appear at any point in the file.

* Each `CMD` will replace and override the previous one.

* As a result, while you can have multiple `CMD` lines, it is useless.

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Build and test our image

Let's build it:

```bash
$ docker build -t figlet .
...
Successfully built 042dff3b4a8d
Successfully tagged figlet:latest
```

And run it:

```bash
$ docker run figlet
 _          _   _       
| |        | | | |      
| |     _  | | | |  __  
|/ \   |/  |/  |/  /  \_
|   |_/|__/|__/|__/\__/ 
```

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Overriding `CMD`

If we want to get a shell into our container (instead of running
`figlet`), we just have to specify a different program to run:

```bash
$ docker run -it figlet bash
root@7ac86a641116:/# 
```

* We specified `bash`.

* It replaced the value of `CMD`.

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Using `ENTRYPOINT`

We want to be able to specify a different message on the command line,
while retaining `figlet` and some default parameters.

In other words, we would like to be able to do this:

```bash
$ docker run figlet salut
           _            
          | |           
 ,   __,  | |       _|_ 
/ \_/  |  |/  |   |  |  
 \/ \_/|_/|__/ \_/|_/|_/
```


We will use the `ENTRYPOINT` verb in Dockerfile.

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Adding `ENTRYPOINT` to our Dockerfile

Our new Dockerfile will look like this:

```dockerfile
FROM ubuntu
RUN apt-get update
RUN ["apt-get", "install", "figlet"]
ENTRYPOINT ["figlet", "-f", "script"]
```

* `ENTRYPOINT` defines a base command (and its parameters) for the container.

* The command line arguments are appended to those parameters.

* Like `CMD`, `ENTRYPOINT` can appear anywhere, and replaces the previous value.

Why did we use JSON syntax for our `ENTRYPOINT`?

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Implications of JSON vs string syntax

* When CMD or ENTRYPOINT use string syntax, they get wrapped in `sh -c`.

* To avoid this wrapping, we can use JSON syntax.

What if we used `ENTRYPOINT` with string syntax?

```bash
$ docker run figlet salut
```

This would run the following command in the `figlet` image:

```bash
sh -c "figlet -f script" salut
```

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Build and test our image

Let's build it:

```bash
$ docker build -t figlet .
...
Successfully built 36f588918d73
Successfully tagged figlet:latest
```

And run it:

```bash
$ docker run figlet salut
           _            
          | |           
 ,   __,  | |       _|_ 
/ \_/  |  |/  |   |  |  
 \/ \_/|_/|__/ \_/|_/|_/
```

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Using `CMD` and `ENTRYPOINT` together

What if we want to define a default message for our container?

Then we will use `ENTRYPOINT` and `CMD` together.

* `ENTRYPOINT` will define the base command for our container.

* `CMD` will define the default parameter(s) for this command.

* They *both* have to use JSON syntax.

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## `CMD` and `ENTRYPOINT` together

Our new Dockerfile will look like this:

```dockerfile
FROM ubuntu
RUN apt-get update
RUN ["apt-get", "install", "figlet"]
ENTRYPOINT ["figlet", "-f", "script"]
CMD ["hello world"]
```

* `ENTRYPOINT` defines a base command (and its parameters) for the container.

* If we don't specify extra command-line arguments when starting the container,
  the value of `CMD` is appended.

* Otherwise, our extra command-line arguments are used instead of `CMD`.

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Build and test our image

Let's build it:

```bash
$ docker build -t figlet .
...
Successfully built 6e0b6a048a07
Successfully tagged figlet:latest
```

Run it without parameters:

```bash
$ docker run figlet
 _          _   _                             _        
| |        | | | |                           | |    |  
| |     _  | | | |  __             __   ,_   | |  __|  
|/ \   |/  |/  |/  /  \_  |  |  |_/  \_/  |  |/  /  |  
|   |_/|__/|__/|__/\__/    \/ \/  \__/    |_/|__/\_/|_/
```

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Overriding the image default parameters

Now let's pass extra arguments to the image.

```bash
$ docker run figlet hola mundo
 _           _                                               
| |         | |                                      |       
| |     __  | |  __,     _  _  _           _  _    __|   __  
|/ \   /  \_|/  /  |    / |/ |/ |  |   |  / |/ |  /  |  /  \_
|   |_/\__/ |__/\_/|_/    |  |  |_/ \_/|_/  |  |_/\_/|_/\__/ 
```

We overrode `CMD` but still used `ENTRYPOINT`.

.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

## Overriding `ENTRYPOINT`

What if we want to run a shell in our container?

We cannot just do `docker run figlet bash` because
that would just tell figlet to display the word "bash."

We use the `--entrypoint` parameter:

```bash
$ docker run -it --entrypoint bash figlet
root@6027e44e2955:/# 
```


.debug[[containers/Cmd_And_Entrypoint.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Cmd_And_Entrypoint.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/plastic-containers.JPG)]

---

name: toc-copying-files-during-the-build
class: title

Copying files during the build

.nav[
[Previous section](#toc-cmd-and-entrypoint)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-publishing-images-to-the-docker-hub)
]

.debug[(automatically generated title slide)]

---

class: title

# Copying files during the build

![Monks copying books](images/title-copying-files-during-build.jpg)

.debug[[containers/Copying_Files_During_Build.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Copying_Files_During_Build.md)]
---

## Objectives

So far, we have installed things in our container images
by downloading packages.

We can also copy files from the *build context* to the
container that we are building.

Remember: the *build context* is the directory containing
the Dockerfile.

In this chapter, we will learn a new Dockerfile keyword: `COPY`.

.debug[[containers/Copying_Files_During_Build.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Copying_Files_During_Build.md)]
---

## Build some C code

We want to build a container that compiles a basic "Hello world" program in C.

Here is the program, `hello.c`:

```bash
int main () {
  puts("Hello, world!");
  return 0;
}
```

Let's create a new directory, and put this file in there.

Then we will write the Dockerfile.

.debug[[containers/Copying_Files_During_Build.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Copying_Files_During_Build.md)]
---

## The Dockerfile

On Debian and Ubuntu, the package `build-essential` will get us a compiler.

When installing it, don't forget to specify the `-y` flag, otherwise the build will fail (since the build cannot be interactive).

Then we will use `COPY` to place the source file into the container.

```bash
FROM ubuntu
RUN apt-get update
RUN apt-get install -y build-essential
COPY hello.c /
RUN make hello
CMD /hello
```

Create this Dockerfile.

.debug[[containers/Copying_Files_During_Build.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Copying_Files_During_Build.md)]
---

## Testing our C program

* Create `hello.c` and `Dockerfile` in the same directory.

* Run `docker build -t hello .` in this directory.

* Run `docker run hello`, you should see `Hello, world!`.

Success!

.debug[[containers/Copying_Files_During_Build.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Copying_Files_During_Build.md)]
---

## `COPY` and the build cache

* Run the build again.

* Now, modify `hello.c` and run the build again.

* Docker can cache steps involving `COPY`.

* Those steps will not be executed again if the files haven't been changed.

.debug[[containers/Copying_Files_During_Build.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Copying_Files_During_Build.md)]
---

## Details

* You can `COPY` whole directories recursively.

* Older Dockerfiles also have the `ADD` instruction.
  <br/>It is similar but can automatically extract archives.

* If we really wanted to compile C code in a container, we would:

  * Place it in a different directory, with the `WORKDIR` instruction.

  * Even better, use the `gcc` official image.

.debug[[containers/Copying_Files_During_Build.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Copying_Files_During_Build.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-1.jpg)]

---

name: toc-publishing-images-to-the-docker-hub
class: title

Publishing images to the Docker Hub

.nav[
[Previous section](#toc-copying-files-during-the-build)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-tips-for-efficient-dockerfiles)
]

.debug[(automatically generated title slide)]

---
# Publishing images to the Docker Hub

We have built our first images.

We can now publish it to the Docker Hub!

*You don't have to do the exercises in this section,
because they require an account on the Docker Hub, and we
don't want to force anyone to create one.*

*Note, however, that creating an account on the Docker Hub
is free (and doesn't require a credit card), and hosting
public images is free as well.*

.debug[[containers/Publishing_To_Docker_Hub.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Publishing_To_Docker_Hub.md)]
---

## Logging into our Docker Hub account

* This can be done from the Docker CLI:
  ```bash
  docker login
  ```

.warning[When running Docker for Mac/Windows, or
Docker on a Linux workstation, it can (and will when
possible) integrate with your system's keyring to
store your credentials securely. However, on most Linux
servers, it will store your credentials in `~/.docker/config`.]

.debug[[containers/Publishing_To_Docker_Hub.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Publishing_To_Docker_Hub.md)]
---

## Image tags and registry addresses

* Docker images tags are like Git tags and branches.

* They are like *bookmarks* pointing at a specific image ID.

* Tagging an image doesn't *rename* an image: it adds another tag.

* When pushing an image to a registry, the registry address is in the tag.

  Example: `registry.example.net:5000/image`

* What about Docker Hub images?

--

* `jpetazzo/clock` is, in fact, `index.docker.io/jpetazzo/clock`

* `ubuntu` is, in fact, `library/ubuntu`, i.e. `index.docker.io/library/ubuntu`

.debug[[containers/Publishing_To_Docker_Hub.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Publishing_To_Docker_Hub.md)]
---

## Tagging an image to push it on the Hub

* Let's tag our `figlet` image (or any other to our liking):
  ```bash
  docker tag figlet jpetazzo/figlet
  ```

* And push it to the Hub:
  ```bash
  docker push jpetazzo/figlet
  ```

* That's it!

--

* Anybody can now `docker run jpetazzo/figlet` anywhere.

.debug[[containers/Publishing_To_Docker_Hub.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Publishing_To_Docker_Hub.md)]
---

## The goodness of automated builds

* You can link a Docker Hub repository with a GitHub or BitBucket repository

* Each push to GitHub or BitBucket will trigger a build on Docker Hub

* If the build succeeds, the new image is available on Docker Hub

* You can map tags and branches between source and container images

* If you work with public repositories, this is free

.debug[[containers/Publishing_To_Docker_Hub.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Publishing_To_Docker_Hub.md)]
---

class: extra-details

## Setting up an automated build

* We need a Dockerized repository!
* Let's go to https://github.com/jpetazzo/trainingwheels and fork it.
* Go to the Docker Hub (https://hub.docker.com/).
* Select "Create" in the top-right bar, and select "Create Automated Build."
* Connect your Docker Hub account to your GitHub account.
* Select your user and the repository that we just forked.
* Create.
* Then go to "Build Settings."
* Put `/www` in "Dockerfile Location" (or whichever directory the Dockerfile is in).
* Click "Trigger" to build the repository immediately (without waiting for a git push).
* Subsequent builds will happen automatically, thanks to GitHub hooks.

.debug[[containers/Publishing_To_Docker_Hub.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Publishing_To_Docker_Hub.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-2.jpg)]

---

name: toc-tips-for-efficient-dockerfiles
class: title

Tips for efficient Dockerfiles

.nav[
[Previous section](#toc-publishing-images-to-the-docker-hub)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-dockerfile-examples)
]

.debug[(automatically generated title slide)]

---
# Tips for efficient Dockerfiles

We will see how to:

* Reduce the number of layers.

* Leverage the build cache so that builds can be faster.

* Embed unit testing in the build process.

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## Reducing the number of layers

* Each line in a `Dockerfile` creates a new layer.

* Build your `Dockerfile` to take advantage of Docker's caching system.

* Combine commands by using `&&` to continue commands and `\` to wrap lines.

Note: it is frequent to build a Dockerfile line by line:

```dockerfile
RUN apt-get install thisthing
RUN apt-get install andthatthing andthatotherone
RUN apt-get install somemorestuff
```

And then refactor it trivially before shipping:

```dockerfile
RUN apt-get install thisthing andthatthing andthatotherone somemorestuff
```

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## Avoid re-installing dependencies at each build

* Classic Dockerfile problem:

  "each time I change a line of code, all my dependencies are re-installed!"

* Solution: `COPY` dependency lists (`package.json`, `requirements.txt`, etc.)
  by themselves to avoid reinstalling unchanged dependencies every time.

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## Example "bad" `Dockerfile`

The dependencies are reinstalled every time, because the build system does not know if `requirements.txt` has been updated.

```bash
FROM python
WORKDIR /src
COPY . .
RUN pip install -qr requirements.txt
EXPOSE 5000
CMD ["python", "app.py"]
```

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## Fixed `Dockerfile`

Adding the dependencies as a separate step means that Docker can cache more efficiently and only install them when `requirements.txt` changes.

```bash
FROM python
COPY requirements.txt /tmp/requirements.txt
RUN pip install -qr /tmp/requirements.txt
WORKDIR /src
COPY . .
EXPOSE 5000
CMD ["python", "app.py"]
```

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## Embedding unit tests in the build process

```dockerfile
FROM <baseimage>
RUN <install dependencies>
COPY <code>
RUN <build code>
RUN <install test dependencies>
COPY <test data sets and fixtures>
RUN <unit tests>
FROM <baseimage>
RUN <install dependencies>
COPY <code>
RUN <build code>
CMD, EXPOSE ...
```

* The build fails as soon as an instruction fails
* If `RUN <unit tests>` fails, the build doesn't produce an image
* If it succeeds, it produces a clean image (without test libraries and data)

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/two-containers-on-a-truck.jpg)]

---

name: toc-dockerfile-examples
class: title

Dockerfile examples

.nav[
[Previous section](#toc-tips-for-efficient-dockerfiles)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-naming-and-inspecting-containers)
]

.debug[(automatically generated title slide)]

---

# Dockerfile examples

There are a number of tips, tricks, and techniques that we can use in Dockerfiles.

But sometimes, we have to use different (and even opposed) practices depending on:

- the complexity of our project,

- the programming language or framework that we are using,

- the stage of our project (early MVP vs. super-stable production),

- whether we're building a final image or a base for further images,

- etc.

We are going to show a few examples using very different techniques.

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## When to optimize an image

When authoring official images, it is a good idea to reduce as much as possible:

- the number of layers,

- the size of the final image.

This is often done at the expense of build time and convenience for the image maintainer;
but when an image is downloaded millions of time, saving even a few seconds of pull time
can be worth it.

.small[
```dockerfile
RUN apt-get update && apt-get install -y libpng12-dev libjpeg-dev && rm -rf /var/lib/apt/lists/* \
	&& docker-php-ext-configure gd --with-png-dir=/usr --with-jpeg-dir=/usr \
	&& docker-php-ext-install gd
...
RUN curl -o wordpress.tar.gz -SL https://wordpress.org/wordpress-${WORDPRESS_UPSTREAM_VERSION}.tar.gz \
	&& echo "$WORDPRESS_SHA1 *wordpress.tar.gz" | sha1sum -c - \
	&& tar -xzf wordpress.tar.gz -C /usr/src/ \
	&& rm wordpress.tar.gz \
	&& chown -R www-data:www-data /usr/src/wordpress
```
]

(Source: [Wordpress official image](https://github.com/docker-library/wordpress/blob/618490d4bdff6c5774b84b717979bfe3d6ba8ad1/apache/Dockerfile))

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## When to *not* optimize an image

Sometimes, it is better to prioritize *maintainer convenience*.

In particular, if:

- the image changes a lot,

- the image has very few users (e.g. only 1, the maintainer!),

- the image is built and run on the same machine,

- the image is built and run on machines with a very fast link ...

In these cases, just keep things simple!

(Next slide: a Dockerfile that can be used to preview a Jekyll / github pages site.)

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

```dockerfile
FROM debian:sid

RUN apt-get update -q
RUN apt-get install -yq build-essential make
RUN apt-get install -yq zlib1g-dev
RUN apt-get install -yq ruby ruby-dev
RUN apt-get install -yq python-pygments
RUN apt-get install -yq nodejs
RUN apt-get install -yq cmake
RUN gem install --no-rdoc --no-ri github-pages

COPY . /blog
WORKDIR /blog

VOLUME /blog/_site

EXPOSE 4000
CMD ["jekyll", "serve", "--host", "0.0.0.0", "--incremental"]
```

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## Multi-dimensional versioning systems

Images can have a tag, indicating the version of the image.

But sometimes, there are multiple important components, and we need to indicate the versions
for all of them.

This can be done with environment variables:

```dockerfile
ENV PIP=9.0.3 \
    ZC_BUILDOUT=2.11.2 \
    SETUPTOOLS=38.7.0 \
    PLONE_MAJOR=5.1 \
    PLONE_VERSION=5.1.0 \
    PLONE_MD5=76dc6cfc1c749d763c32fff3a9870d8d
```

(Source: [Plone official image](https://github.com/plone/plone.docker/blob/master/5.1/5.1.0/alpine/Dockerfile))

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## Entrypoints and wrappers

It is very common to define a custom entrypoint.

That entrypoint will generally be a script, performing any combination of:

- pre-flights checks (if a required dependency is not available, display
  a nice error message early instead of an obscure one in a deep log file),

- generation or validation of configuration files,

- dropping privileges (with e.g. `su` or `gosu`, sometimes combined with `chown`),

- and more.

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## A typical entrypoint script

```dockerfile
 #!/bin/sh
 set -e
 
 # first arg is '-f' or '--some-option'
 # or first arg is 'something.conf'
 if [ "${1#-}" != "$1" ] || [ "${1%.conf}" != "$1" ]; then
 	set -- redis-server "$@"
 fi
 
 # allow the container to be started with '--user'
 if [ "$1" = 'redis-server' -a "$(id -u)" = '0' ]; then
 	chown -R redis .
 	exec su-exec redis "$0" "$@"
 fi
 
 exec "$@"
```

(Source: [Redis official image](https://github.com/docker-library/redis/blob/d24f2be82673ccef6957210cc985e392ebdc65e4/4.0/alpine/docker-entrypoint.sh))

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## Factoring information

To facilitate maintenance (and avoid human errors), avoid to repeat information like:

- version numbers,

- remote asset URLs (e.g. source tarballs) ...

Instead, use environment variables.

.small[
```dockerfile
ENV NODE_VERSION 10.2.1
...
RUN ...
    && curl -fsSLO --compressed "https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION.tar.xz" \
    && curl -fsSLO --compressed "https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc" \
    && gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \
    && grep " node-v$NODE_VERSION.tar.xz\$" SHASUMS256.txt | sha256sum -c - \
    && tar -xf "node-v$NODE_VERSION.tar.xz" \
    && cd "node-v$NODE_VERSION" \
...
```
]

(Source: [Nodejs official image](https://github.com/nodejs/docker-node/blob/master/10/alpine/Dockerfile))

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## Overrides

In theory, development and production images should be the same.

In practice, we often need to enable specific behaviors in development (e.g. debug statements).

One way to reconcile both needs is to use Compose to enable these behaviors.

Let's look at the [trainingwheels](https://github.com/jpetazzo/trainingwheels) demo app for an example.

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## Production image

This Dockerfile builds an image leveraging gunicorn:

```dockerfile
FROM python
RUN pip install flask
RUN pip install gunicorn
RUN pip install redis
COPY . /src
WORKDIR /src
CMD gunicorn --bind 0.0.0.0:5000 --workers 10 counter:app
EXPOSE 5000
```

(Source: [trainingwheels Dockerfile](https://github.com/jpetazzo/trainingwheels/blob/master/www/Dockerfile))

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## Development Compose file

This Compose file uses the same image, but with a few overrides for development:

- the Flask development server is used (overriding `CMD`),

- the `DEBUG` environment variable is set,

- a volume is used to provide a faster local development workflow.

.small[
```yaml
services:
  www:
    build: www
    ports:
      - 8000:5000
    user: nobody
    environment:
      DEBUG: 1
    command: python counter.py
    volumes:
      - ./www:/src
```
]

(Source: [trainingwheels Compose file](https://github.com/jpetazzo/trainingwheels/blob/master/docker-compose.yml))

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

## How to know which best practices are better?

- The main goal of containers is to make our lives easier.

- In this chapter, we showed many ways to write Dockerfiles.

- These Dockerfiles use sometimes diametrally opposed techniques.

- Yet, they were the "right" ones *for a specific situation.*

- It's OK (and even encouraged) to start simple and evolve as needed.

- Feel free to review this chapter later (after writing a few Dockerfiles) for inspiration!

.debug[[containers/Dockerfile_Tips.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Dockerfile_Tips.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/wall-of-containers.jpeg)]

---

name: toc-naming-and-inspecting-containers
class: title

Naming and inspecting containers

.nav[
[Previous section](#toc-dockerfile-examples)
|
[Back to table of contents](#toc-chapter-3)
|
[Next section](#toc-labels)
]

.debug[(automatically generated title slide)]

---

class: title

# Naming and inspecting containers

![Markings on container door](images/title-naming-and-inspecting-containers.jpg)

.debug[[containers/Naming_And_Inspecting.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Naming_And_Inspecting.md)]
---

## Objectives

In this lesson, we will learn about an important
Docker concept: container *naming*.

Naming allows us to:

* Reference easily a container.

* Ensure unicity of a specific container.

We will also see the `inspect` command, which gives a lot of details about a container.

.debug[[containers/Naming_And_Inspecting.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Naming_And_Inspecting.md)]
---

## Naming our containers

So far, we have referenced containers with their ID.

We have copy-pasted the ID, or used a shortened prefix.

But each container can also be referenced by its name.

If a container is named `thumbnail-worker`, I can do:

```bash
$ docker logs thumbnail-worker
$ docker stop thumbnail-worker
etc.
```

.debug[[containers/Naming_And_Inspecting.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Naming_And_Inspecting.md)]
---

## Default names

When we create a container, if we don't give a specific
name, Docker will pick one for us.

It will be the concatenation of:

* A mood (furious, goofy, suspicious, boring...)

* The name of a famous inventor (tesla, darwin, wozniak...)

Examples: `happy_curie`, `clever_hopper`, `jovial_lovelace` ...

.debug[[containers/Naming_And_Inspecting.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Naming_And_Inspecting.md)]
---

## Specifying a name

You can set the name of the container when you create it.

```bash
$ docker run --name ticktock jpetazzo/clock
```

If you specify a name that already exists, Docker will refuse
to create the container.

This lets us enforce unicity of a given resource.

.debug[[containers/Naming_And_Inspecting.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Naming_And_Inspecting.md)]
---

## Renaming containers

* You can rename containers with `docker rename`.

* This allows you to "free up" a name without destroying the associated container.

.debug[[containers/Naming_And_Inspecting.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Naming_And_Inspecting.md)]
---

## Inspecting a container

The `docker inspect` command will output a very detailed JSON map.

```bash
$ docker inspect <containerID>
[{
...
(many pages of JSON here)
...
```

There are multiple ways to consume that information.

.debug[[containers/Naming_And_Inspecting.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Naming_And_Inspecting.md)]
---

## Parsing JSON with the Shell

* You *could* grep and cut or awk the output of `docker inspect`.

* Please, don't.

* It's painful.

* If you really must parse JSON from the Shell, use JQ! (It's great.)

```bash
$ docker inspect <containerID> | jq .
```

* We will see a better solution which doesn't require extra tools.

.debug[[containers/Naming_And_Inspecting.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Naming_And_Inspecting.md)]
---

## Using `--format`

You can specify a format string, which will be parsed by 
Go's text/template package.

```bash
$ docker inspect --format '{{ json .Created }}' <containerID>
"2015-02-24T07:21:11.712240394Z"
```

* The generic syntax is to wrap the expression with double curly braces.

* The expression starts with a dot representing the JSON object.

* Then each field or member can be accessed in dotted notation syntax.

* The optional `json` keyword asks for valid JSON output.
  <br/>(e.g. here it adds the surrounding double-quotes.)

.debug[[containers/Naming_And_Inspecting.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Naming_And_Inspecting.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-labels
class: title

Labels

.nav[
[Previous section](#toc-naming-and-inspecting-containers)
|
[Back to table of contents](#toc-chapter-3)
|
[Next section](#toc-getting-inside-a-container)
]

.debug[(automatically generated title slide)]

---
# Labels

* Labels allow to attach arbitrary metadata to containers.

* Labels are key/value pairs.

* They are specified at container creation.

* You can query them with `docker inspect`.

* They can also be used as filters with some commands (e.g. `docker ps`).

.debug[[containers/Labels.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Labels.md)]
---

## Using labels

Let's create a few containers with a label `owner`.

```bash
docker run -d -l owner=alice nginx
docker run -d -l owner=bob nginx
docker run -d -l owner nginx
```

We didn't specify a value for the `owner` label in the last example.

This is equivalent to setting the value to be an empty string.

.debug[[containers/Labels.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Labels.md)]
---

## Querying labels

We can view the labels with `docker inspect`.

```bash
$ docker inspect $(docker ps -lq) | grep -A3 Labels
            "Labels": {
                "maintainer": "NGINX Docker Maintainers <docker-maint@nginx.com>",
                "owner": ""
            },
```

We can use the `--format` flag to list the value of a label.

```bash
$ docker inspect $(docker ps -q) --format 'OWNER={{.Config.Labels.owner}}'
```

.debug[[containers/Labels.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Labels.md)]
---

## Using labels to select containers

We can list containers having a specific label.

```bash
$ docker ps --filter label=owner
```

Or we can list containers having a specific label with a specific value.

```bash
$ docker ps --filter label=owner=alice
```

.debug[[containers/Labels.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Labels.md)]
---

## Use-cases for labels


* HTTP vhost of a web app or web service.

  (The label is used to generate the configuration for NGINX, HAProxy, etc.)

* Backup schedule for a stateful service.

  (The label is used by a cron job to determine if/when to backup container data.)

* Service ownership.

  (To determine internal cross-billing, or who to page in case of outage.)

* etc.

.debug[[containers/Labels.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Labels.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-getting-inside-a-container
class: title

Getting inside a container

.nav[
[Previous section](#toc-labels)
|
[Back to table of contents](#toc-chapter-3)
|
[Next section](#toc-container-networking-basics)
]

.debug[(automatically generated title slide)]

---
class: title

# Getting inside a container

![Person standing inside a container](images/getting-inside.png)

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

## Objectives

On a traditional server or VM, we sometimes need to:

* log into the machine (with SSH or on the console),

* analyze the disks (by removing them or rebooting with a rescue system).

In this chapter, we will see how to do that with containers.

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

## Getting a shell

Every once in a while, we want to log into a machine.

In an perfect world, this shouldn't be necessary.

* You need to install or update packages (and their configuration)?

  Use configuration management. (e.g. Ansible, Chef, Puppet, Salt...)

* You need to view logs and metrics?

  Collect and access them through a centralized platform.

In the real world, though ... we often need shell access!

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

## Not getting a shell

Even without a perfect deployment system, we can do many operations without getting a shell.

* Installing packages can (and should) be done in the container image.

* Configuration can be done at the image level, or when the container starts.

* Dynamic configuration can be stored in a volume (shared with another container).

* Logs written to stdout are automatically collected by the Docker Engine.

* Other logs can be written to a shared volume.

* Process information and metrics are visible from the host.

_Let's save logging, volumes ... for later, but let's have a look at process information!_

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

## Viewing container processes from the host

If you run Docker on Linux, container processes are visible on the host.

```bash
$ ps faux | less
```

* Scroll around the output of this command.

* You should see the `jpetazzo/clock` container.

* A containerized process is just like any other process on the host.

* We can use tools like `lsof`, `strace`, `gdb` ... To analyze them.

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

class: extra-details

## What's the difference between a container process and a host process?

* Each process (containerized or not) belongs to *namespaces* and *cgroups*.

* The namespaces and cgroups determine what a process can "see" and "do".

* Analogy: each process (containerized or not) runs with a specific UID (user ID).

* UID=0 is root, and has elevated privileges. Other UIDs are normal users.

_We will give more details about namespaces and cgroups later._

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

## Getting a shell in a running container

* Sometimes, we need to get a shell anyway.

* We _could_ run some SSH server in the container ...

* But it is easier to use `docker exec`.

```bash
$ docker exec -ti ticktock sh
```

* This creates a new process (running `sh`) _inside_ the container.

* This can also be done "manually" with the tool `nsenter`.

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

## Caveats

* The tool that you want to run needs to exist in the container.

* Some tools (like `ip netns exec`) let you attach to _one_ namespace at a time.

  (This lets you e.g. setup network interfaces, even if you don't have `ifconfig` or `ip` in the container.)

* Most importantly: the container needs to be running.

* What if the container is stopped or crashed?

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

## Getting a shell in a stopped container

* A stopped container is only _storage_ (like a disk drive).

* We cannot SSH into a disk drive or USB stick!

* We need to connect the disk to a running machine.

* How does that translate into the container world?

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

## Analyzing a stopped container

As an exercise, we are going to try to find out what's wrong with `jpetazzo/crashtest`.

```bash
docker run jpetazzo/crashtest
```

The container starts, but then stops immediately, without any output.

What would MacGyver&trade; do?

First, let's check the status of that container.

```bash
docker ps -l
```

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

## Viewing filesystem changes

* We can use `docker diff` to see files that were added / changed / removed.

```bash
docker diff <container_id>
```

* The container ID was shown by `docker ps -l`.

* We can also see it with `docker ps -lq`.

* The output of `docker diff` shows some interesting log files!

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

## Accessing files

* We can extract files with `docker cp`.

```bash
docker cp <container_id>:/var/log/nginx/error.log .
```

* Then we can look at that log file.

```bash
cat error.log
```

(The directory `/run/nginx` doesn't exist.)

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

## Exploring a crashed container

* We can restart a container with `docker start` ...

* ... But it will probably crash again immediately!

* We cannot specify a different program to run with `docker start`

* But we can create a new image from the crashed container

```bash
docker commit <container_id> debugimage
```

* Then we can run a new container from that image, with a custom entrypoint

```bash
docker run -ti --entrypoint sh debugimage
```

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

class: extra-details

## Obtaining a complete dump

* We can also dump the entire filesystem of a container.

* This is done with `docker export`.

* It generates a tar archive.

```bash
docker export <container_id> | tar tv
```

This will give a detailed listing of the content of the container.

.debug[[containers/Getting_Inside.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Getting_Inside.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-container-networking-basics
class: title

Container networking basics

.nav[
[Previous section](#toc-getting-inside-a-container)
|
[Back to table of contents](#toc-chapter-4)
|
[Next section](#toc-the-container-network-model)
]

.debug[(automatically generated title slide)]

---

class: title

# Container networking basics

![A dense graph network](images/title-container-networking-basics.jpg)

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## Objectives

We will now run network services (accepting requests) in containers.

At the end of this section, you will be able to:

* Run a network service in a container.

* Manipulate container networking basics.

* Find a container's IP address.

We will also explain the different network models used by Docker.

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## A simple, static web server

Run the Docker Hub image `nginx`, which contains a basic web server:

```bash
$ docker run -d -P nginx
66b1ce719198711292c8f34f84a7b68c3876cf9f67015e752b94e189d35a204e
```

* Docker will download the image from the Docker Hub.

* `-d` tells Docker to run the image in the background.

* `-P` tells Docker to make this service reachable from other computers.
  <br/>(`-P` is the short version of `--publish-all`.)

But, how do we connect to our web server now?

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## Finding our web server port

We will use `docker ps`:

```bash
$ docker ps
CONTAINER ID  IMAGE  ...  PORTS                  ...
e40ffb406c9e  nginx  ...  0.0.0.0:32768->80/tcp  ...
```


* The web server is running on port 80 inside the container.

* This port is mapped to port 32768 on our Docker host.

We will explain the whys and hows of this port mapping.

But first, let's make sure that everything works properly.

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## Connecting to our web server (GUI)

Point your browser to the IP address of your Docker host, on the port
shown by `docker ps` for container port 80.

![Screenshot](images/welcome-to-nginx.png)

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## Connecting to our web server (CLI)

You can also use `curl` directly from the Docker host.

Make sure to use the right port number if it is different
from the example below:

```bash
$ curl localhost:32768
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
```

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## How does Docker know which port to map?

* There is metadata in the image telling "this image has something on port 80".

* We can see that metadata with `docker inspect`:

```bash
$ docker inspect --format '{{.Config.ExposedPorts}}' nginx
map[80/tcp:{}]
```

* This metadata was set in the Dockerfile, with the `EXPOSE` keyword.

* We can see that with `docker history`:

```bash
$ docker history nginx
IMAGE               CREATED             CREATED BY
7f70b30f2cc6        11 days ago         /bin/sh -c #(nop)  CMD ["nginx" "-g" "‚Ä¶
<missing>           11 days ago         /bin/sh -c #(nop)  STOPSIGNAL [SIGTERM]
<missing>           11 days ago         /bin/sh -c #(nop)  EXPOSE 80/tcp
```

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## Why are we mapping ports?

* We are out of IPv4 addresses.

* Containers cannot have public IPv4 addresses.

* They have private addresses.

* Services have to be exposed port by port.

* Ports have to be mapped to avoid conflicts.

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## Finding the web server port in a script

Parsing the output of `docker ps` would be painful.

There is a command to help us:

```bash
$ docker port <containerID> 80
32768
```

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## Manual allocation of port numbers

If you want to set port numbers yourself, no problem:

```bash
$ docker run -d -p 80:80 nginx
$ docker run -d -p 8000:80 nginx
$ docker run -d -p 8080:80 -p 8888:80 nginx
```

* We are running three NGINX web servers.
* The first one is exposed on port 80.
* The second one is exposed on port 8000.
* The third one is exposed on ports 8080 and 8888.

Note: the convention is `port-on-host:port-on-container`.

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## Plumbing containers into your infrastructure

There are many ways to integrate containers in your network.

* Start the container, letting Docker allocate a public port for it.
  <br/>Then retrieve that port number and feed it to your configuration.

* Pick a fixed port number in advance, when you generate your configuration.
  <br/>Then start your container by setting the port numbers manually.

* Use a network plugin, connecting your containers with e.g. VLANs, tunnels...

* Enable *Swarm Mode* to deploy across a cluster.
  <br/>The container will then be reachable through any node of the cluster.

When using Docker through an extra management layer like Mesos or Kubernetes,
these will usually provide their own mechanism to expose containers.

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## Finding the container's IP address

We can use the `docker inspect` command to find the IP address of the
container.

```bash
$ docker inspect --format '{{ .NetworkSettings.IPAddress }}' <yourContainerID>
172.17.0.3
```

* `docker inspect` is an advanced command, that can retrieve a ton
  of information about our containers.

* Here, we provide it with a format string to extract exactly the
  private IP address of the container.

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## Pinging our container

We can test connectivity to the container using the IP address we've
just discovered. Let's see this now by using the `ping` tool.

```bash
$ ping <ipAddress>
64 bytes from <ipAddress>: icmp_req=1 ttl=64 time=0.085 ms
64 bytes from <ipAddress>: icmp_req=2 ttl=64 time=0.085 ms
64 bytes from <ipAddress>: icmp_req=3 ttl=64 time=0.085 ms
```

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

## Section summary

We've learned how to:

* Expose a network port.

* Manipulate container networking basics.

* Find a container's IP address.

In the next chapter, we will see how to connect
containers together without exposing their ports.

.debug[[containers/Container_Networking_Basics.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Networking_Basics.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-the-container-network-model
class: title

The Container Network Model

.nav[
[Previous section](#toc-container-networking-basics)
|
[Back to table of contents](#toc-chapter-4)
|
[Next section](#toc-service-discovery-with-containers)
]

.debug[(automatically generated title slide)]

---

class: title

# The Container Network Model

![A denser graph network](images/title-the-container-network-model.jpg)

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Objectives

We will learn about the CNM (Container Network Model).

At the end of this lesson, you will be able to:

* Create a private network for a group of containers.

* Use container naming to connect services together.

* Dynamically connect and disconnect containers to networks.

* Set the IP address of a container.

We will also explain the principle of overlay networks and network plugins.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## The Container Network Model

The CNM was introduced in Engine 1.9.0 (November 2015).

The CNM adds the notion of a *network*, and a new top-level command to manipulate and see those networks: `docker network`.

```bash
$ docker network ls
NETWORK ID          NAME                DRIVER
6bde79dfcf70        bridge              bridge
8d9c78725538        none                null
eb0eeab782f4        host                host
4c1ff84d6d3f        blog-dev            overlay
228a4355d548        blog-prod           overlay
```

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## What's in a network?

* Conceptually, a network is a virtual switch.

* It can be local (to a single Engine) or global (spanning multiple hosts).

* A network has an IP subnet associated to it.

* Docker will allocate IP addresses to the containers connected to a network.

* Containers can be connected to multiple networks.

* Containers can be given per-network names and aliases.

* The names and aliases can be resolved via an embedded DNS server.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Network implementation details

* A network is managed by a *driver*.

* The built-in drivers include:

  * `bridge` (default)

  * `none`

  * `host`

  * `macvlan`

* A multi-host driver, *overlay*, is available out of the box (for Swarm clusters).

* More drivers can be provided by plugins (OVS, VLAN...)

* A network can have a custom IPAM (IP allocator).

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Differences with the CNI

* CNI = Container Network Interface

* CNI is used notably by Kubernetes

* With CNI, all the nodes and containers are on a single IP network

* Both CNI and CNM offer the same functionality, but with very different methods

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: pic

## Single container in a Docker network

![bridge0](images/bridge1.png)

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: pic

## Two containers on two Docker networks

![bridge3](images/bridge2.png)

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Creating a network

Let's create a network called `dev`.

```bash
$ docker network create dev
4c1ff84d6d3f1733d3e233ee039cac276f425a9d5228a4355d54878293a889ba
```

The network is now visible with the `network ls` command:

```bash
$ docker network ls
NETWORK ID          NAME                DRIVER
6bde79dfcf70        bridge              bridge
8d9c78725538        none                null
eb0eeab782f4        host                host
4c1ff84d6d3f        dev                 bridge
```

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Placing containers on a network

We will create a *named* container on this network.

It will be reachable with its name, `es`.

```bash
$ docker run -d --name es --net dev elasticsearch:2
8abb80e229ce8926c7223beb69699f5f34d6f1d438bfc5682db893e798046863
```

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Communication between containers

Now, create another container on this network.

.small[
```bash
$ docker run -ti --net dev alpine sh
root@0ecccdfa45ef:/#
```
]

From this new container, we can resolve and ping the other one, using its assigned name:

.small[
```bash
/ # ping es
PING es (172.18.0.2) 56(84) bytes of data.
64 bytes from es.dev (172.18.0.2): icmp_seq=1 ttl=64 time=0.221 ms
64 bytes from es.dev (172.18.0.2): icmp_seq=2 ttl=64 time=0.114 ms
64 bytes from es.dev (172.18.0.2): icmp_seq=3 ttl=64 time=0.114 ms
^C
--- es ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2000ms
rtt min/avg/max/mdev = 0.114/0.149/0.221/0.052 ms
root@0ecccdfa45ef:/#
```
]

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Resolving container addresses

In Docker Engine 1.9, name resolution is implemented with `/etc/hosts`, and
updating it each time containers are added/removed.

.small[
```bash
[root@0ecccdfa45ef /]# cat /etc/hosts
172.18.0.3  0ecccdfa45ef
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.18.0.2      es
172.18.0.2      es.dev
```
]

In Docker Engine 1.10, this has been replaced by a dynamic resolver.

(This avoids race conditions when updating `/etc/hosts`.)

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/chinook-helicopter-container.jpg)]

---

name: toc-service-discovery-with-containers
class: title

Service discovery with containers

.nav[
[Previous section](#toc-the-container-network-model)
|
[Back to table of contents](#toc-chapter-4)
|
[Next section](#toc-links-and-resources)
]

.debug[(automatically generated title slide)]

---

# Service discovery with containers

* Let's try to run an application that requires two containers.

* The first container is a web server.

* The other one is a redis data store.

* We will place them both on the `dev` network created before.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Running the web server

* The application is provided by the container image `jpetazzo/trainingwheels`.

* We don't know much about it so we will try to run it and see what happens!

Start the container, exposing all its ports:

```bash
$ docker run --net dev -d -P jpetazzo/trainingwheels
```

Check the port that has been allocated to it:

```bash
$ docker ps -l
```

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Test the web server

* If we connect to the application now, we will see an error page:

![Trainingwheels error](images/trainingwheels-error.png)

* This is because the Redis service is not running.
* This container tries to resolve the name `redis`.

Note: we're not using a FQDN or an IP address here; just `redis`.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Start the data store

* We need to start a Redis container.

* That container must be on the same network as the web server.

* It must have the right name (`redis`) so the application can find it.

Start the container:

```bash
$ docker run --net dev --name redis -d redis
```

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Test the web server again

* If we connect to the application now, we should see that the app is working correctly:

![Trainingwheels OK](images/trainingwheels-ok.png)

* When the app tries to resolve `redis`, instead of getting a DNS error, it gets the IP address of our Redis container.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## A few words on *scope*

* What if we want to run multiple copies of our application?

* Since names are unique, there can be only one container named `redis` at a time.

* However, we can specify the network name of our container with `--net-alias`.

* `--net-alias` is scoped per network, and independent from the container name.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Using a network alias instead of a name

Let's remove the `redis` container:

```bash
$ docker rm -f redis
```

And create one that doesn't block the `redis` name:

```bash
$ docker run --net dev --net-alias redis -d redis
```

Check that the app still works (but the counter is back to 1,
since we wiped out the old Redis container).

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Names are *local* to each network

Let's try to ping our `es` container from another container, when that other container is *not* on the `dev` network.

```bash
$ docker run --rm alpine ping es
ping: bad address 'es'
```

Names can be resolved only when containers are on the same network.

Containers can contact each other only when they are on the same network (you can try to ping using the IP address to verify).

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Network aliases

We would like to have another network, `prod`, with its own `es` container. But there can be only one container named `es`!

We will use *network aliases*.

A container can have multiple network aliases.

Network aliases are *local* to a given network (only exist in this network).

Multiple containers can have the same network alias (even on the same network). In Docker Engine 1.11, resolving a network alias yields the IP addresses of all containers holding this alias.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Creating containers on another network

Create the `prod` network.

```bash
$ docker network create prod
5a41562fecf2d8f115bedc16865f7336232a04268bdf2bd816aecca01b68d50c
```

We can now create multiple containers with the `es` alias on the new `prod` network.

```bash
$ docker run -d --name prod-es-1 --net-alias es --net prod elasticsearch:2
38079d21caf0c5533a391700d9e9e920724e89200083df73211081c8a356d771
$ docker run -d --name prod-es-2 --net-alias es --net prod elasticsearch:2
1820087a9c600f43159688050dcc164c298183e1d2e62d5694fd46b10ac3bc3d
```

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Resolving network aliases

Let's try DNS resolution first, using the `nslookup` tool that ships with the `alpine` image.

```bash
$ docker run --net prod --rm alpine nslookup es
Name:      es
Address 1: 172.23.0.3 prod-es-2.prod
Address 2: 172.23.0.2 prod-es-1.prod
```

(You can ignore the `can't resolve '(null)'` errors.)

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Connecting to aliased containers

Each ElasticSearch instance has a name (generated when it is started). This name can be seen when we issue a simple HTTP request on the ElasticSearch API endpoint.

Try the following command a few times:

.small[
```bash
$ docker run --rm --net dev centos curl -s es:9200
{
  "name" : "Tarot",
...
}
```
]

Then try it a few times by replacing `--net dev` with `--net prod`:

.small[
```bash
$ docker run --rm --net prod centos curl -s es:9200
{
  "name" : "The Symbiote",
...
}
```
]

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Good to know ...

* Docker will not create network names and aliases on the default `bridge` network.

* Therefore, if you want to use those features, you have to create a custom network first.

* Network aliases are *not* unique on a given network.

* i.e., multiple containers can have the same alias on the same network.

* In that scenario, the Docker DNS server will return multiple records.
  <br/>
  (i.e. you will get DNS round robin out of the box.)

* Enabling *Swarm Mode* gives access to clustering and load balancing with IPVS.

* Creation of networks and network aliases is generally automated with tools like Compose.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## A few words about round robin DNS

Don't rely exclusively on round robin DNS to achieve load balancing.

Many factors can affect DNS resolution, and you might see:

- all traffic going to a single instance;
- traffic being split (unevenly) between some instances;
- different behavior depending on your application language;
- different behavior depending on your base distro;
- different behavior depending on other factors (sic).

It's OK to use DNS to discover available endpoints, but remember that you have to re-resolve every now and then to discover new endpoints.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Custom networks

When creating a network, extra options can be provided.

* `--internal` disables outbound traffic (the network won't have a default gateway).

* `--gateway` indicates which address to use for the gateway (when outbound traffic is allowed).

* `--subnet` (in CIDR notation) indicates the subnet to use.

* `--ip-range` (in CIDR notation) indicates the subnet to allocate from.

* `--aux-address` allows to specify a list of reserved addresses (which won't be allocated to containers).

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Setting containers' IP address

* It is possible to set a container's address with `--ip`.
* The IP address has to be within the subnet used for the container.

A full example would look like this.

```bash
$ docker network create --subnet 10.66.0.0/16 pubnet
42fb16ec412383db6289a3e39c3c0224f395d7f85bcb1859b279e7a564d4e135
$ docker run --net pubnet --ip 10.66.66.66 -d nginx
b2887adeb5578a01fd9c55c435cad56bbbe802350711d2743691f95743680b09
```

*Note: don't hard code container IP addresses in your code!*

*I repeat: don't hard code container IP addresses in your code!*

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Overlay networks

* The features we've seen so far only work when all containers are on a single host.

* If containers span multiple hosts, we need an *overlay* network to connect them together.

* Docker ships with a default network plugin, `overlay`, implementing an overlay network leveraging
  VXLAN, *enabled with Swarm Mode*.

* Other plugins (Weave, Calico...) can provide overlay networks as well.

* Once you have an overlay network, *all the features that we've used in this chapter work identically
  across multiple hosts.*

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Multi-host networking (overlay)

Out of the scope for this intro-level workshop!

Very short instructions:

- enable Swarm Mode (`docker swarm init` then `docker swarm join` on other nodes)
- `docker network create mynet --driver overlay`
- `docker service create --network mynet myimage`

See http://jpetazzo.github.io/container.training for all the deets about clustering!

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Multi-host networking (plugins)

Out of the scope for this intro-level workshop!

General idea:

- install the plugin (they often ship within containers)

- run the plugin (if it's in a container, it will often require extra parameters; don't just `docker run` it blindly!)

- some plugins require configuration or activation (creating a special file that tells Docker "use the plugin whose control socket is at the following location")

- you can then `docker network create --driver pluginname`

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Connecting and disconnecting dynamically

* So far, we have specified which network to use when starting the container.

* The Docker Engine also allows to connect and disconnect while the container runs.

* This feature is exposed through the Docker API, and through two Docker CLI commands:

  * `docker network connect <network> <container>`

  * `docker network disconnect <network> <container>`

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Dynamically connecting to a network

* We have a container named `es` connected to a network named `dev`.

* Let's start a simple alpine container on the default network:

  ```bash
  $ docker run -ti alpine sh
  / #
  ```

* In this container, try to ping the `es` container:

  ```bash
  / # ping es
  ping: bad address 'es'
  ```

  This doesn't work, but we will change that by connecting the container.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Finding the container ID and connecting it

* Figure out the ID of our alpine container; here are two methods:

  * looking at `/etc/hostname` in the container,

  * running `docker ps -lq` on the host.

* Run the following command on the host:

  ```bash
  $ docker network connect dev `<container_id>`
  ```

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Checking what we did

* Try again to `ping es` from the container.

* It should now work correctly:

  ```bash
  / # ping es
  PING es (172.20.0.3): 56 data bytes
  64 bytes from 172.20.0.3: seq=0 ttl=64 time=0.376 ms
  64 bytes from 172.20.0.3: seq=1 ttl=64 time=0.130 ms
  ^C
  ```

* Interrupt it with Ctrl-C.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Looking at the network setup in the container

We can look at the list of network interfaces with `ifconfig`, `ip a`, or `ip l`:

.small[
```bash
/ # ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
18: eth0@if19: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
20: eth1@if21: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP
    link/ether 02:42:ac:14:00:04 brd ff:ff:ff:ff:ff:ff
    inet 172.20.0.4/16 brd 172.20.255.255 scope global eth1
       valid_lft forever preferred_lft forever
/ #
```
]

Each network connection is materialized with a virtual network interface.

As we can see, we can be connected to multiple networks at the same time.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

## Disconnecting from a network

* Let's try the symmetrical command to disconnect the container:
  ```bash
  $ docker network disconnect dev <container_id>
  ```

* From now on, if we try to ping `es`, it will not resolve:
  ```bash
  / # ping es
  ping: bad address 'es'
  ```

* Trying to ping the IP address directly won't work either:
  ```bash
  / # ping 172.20.0.3
  ... (nothing happens until we interrupt it with Ctrl-C)
  ```

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Network aliases are scoped per network

* Each network has its own set of network aliases.

* We saw this earlier: `es` resolves to different addresses in `dev` and `prod`.

* If we are connected to multiple networks, the resolver looks up names in each of them
  (as of Docker Engine 18.03, it is the connection order) and stops as soon as the name
  is found.

* Therefore, if we are connected to both `dev` and `prod`, resolving `es` will **not**
  give us the addresses of all the `es` services; but only the ones in `dev` or `prod`.

* However, we can lookup `es.dev` or `es.prod` if we need to.

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---

class: extra-details

## Finding out about our networks and names

* We can do reverse DNS lookups on containers' IP addresses.

* If the IP address belongs to a network (other than the default bridge), the result will be:

  ```
  name-or-first-alias-or-container-id.network-name
  ```

* Example:

.small[
```bash
$ docker run -ti --net prod --net-alias hello alpine
/ # apk add --no-cache drill
...
OK: 5 MiB in 13 packages
/ # ifconfig
eth0      Link encap:Ethernet  HWaddr 02:42:AC:15:00:03
          inet addr:`172.21.0.3`  Bcast:172.21.255.255  Mask:255.255.0.0
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
...
/ # drill -t ptr `3.0.21.172`.in-addr.arpa
...
;; ANSWER SECTION:
3.0.21.172.in-addr.arpa.	600	IN	PTR	`hello.prod`.
...
```
]

.debug[[containers/Container_Network_Model.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/Container_Network_Model.md)]
---
class: title, self-paced

Thank you!

.debug[[shared/thankyou.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/thankyou.md)]
---

class: title, in-person

That's all, folks! <br/> Questions?

![end](images/end.jpg)

.debug[[shared/thankyou.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/thankyou.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-cranes.jpg)]

---

name: toc-links-and-resources
class: title

Links and resources

.nav[
[Previous section](#toc-service-discovery-with-containers)
|
[Back to table of contents](#toc-chapter-8)
|
[Next section](#toc-pre-requirements)
]

.debug[(automatically generated title slide)]

---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/two-containers-on-a-truck.jpg)]

---

name: toc-links-and-resources
class: title

Links and resources

.nav[
[Previous section](#toc-service-discovery-with-containers)
|
[Back to table of contents](#toc-chapter-8)
|
[Next section](#toc-pre-requirements)
]

.debug[(automatically generated title slide)]

---
# Links and resources

- [Docker Community Slack](https://community.docker.com/registrations/groups/4316)
- [Docker Community Forums](https://forums.docker.com/)
- [Docker Hub](https://hub.docker.com)
- [Docker Blog](http://blog.docker.com/)
- [Docker documentation](http://docs.docker.com/)
- [Docker on StackOverflow](https://stackoverflow.com/questions/tagged/docker)
- [Docker on Twitter](http://twitter.com/docker)
- [Play With Docker Hands-On Labs](http://training.play-with-docker.com/)

.footnote[These slides (and future updates) are on ‚Üí http://container.training/]

.debug[[containers/links.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/containers/links.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-housing.jpg)]

---

name: toc-pre-requirements
class: title

Pre-requirements

.nav[
[Previous section](#toc-links-and-resources)
|
[Back to table of contents](#toc-chapter-5)
|
[Next section](#toc-our-sample-application)
]

.debug[(automatically generated title slide)]

---
# Pre-requirements

- Be comfortable with the UNIX command line

  - navigating directories

  - editing files

  - a little bit of bash-fu (environment variables, loops)

- Some Docker knowledge

  - `docker run`, `docker ps`, `docker build`

  - ideally, you know how to write a Dockerfile and build it
    <br/>
    (even if it's a `FROM` line and a couple of `RUN` commands)

- It's totally OK if you are not a Docker expert!

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

class: title

*Tell me and I forget.*
<br/>
*Teach me and I remember.*
<br/>
*Involve me and I learn.*

Misattributed to Benjamin Franklin

[(Probably inspired by Chinese Confucian philosopher Xunzi)](https://www.barrypopik.com/index.php/new_york_city/entry/tell_me_and_i_forget_teach_me_and_i_may_remember_involve_me_and_i_will_lear/)

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

## Hands-on sections

- The whole workshop is hands-on

- We are going to build, ship, and run containers!

- You are invited to reproduce all the demos

- All hands-on sections are clearly identified, like the gray rectangle below

.exercise[

- This is the stuff you're supposed to do!

- Go to http://container.training/ to view these slides

- Join the chat room: [Slack](https://dockercommunity.slack.com/messages/C7GKACWDV)

<!-- ```open http://container.training/``` -->

]

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

class: in-person

## Where are we going to run our containers?

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

class: in-person, pic

![You get a cluster](images/you-get-a-cluster.jpg)

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

class: in-person

## You get a cluster of cloud VMs

- Each person gets a private cluster of cloud VMs (not shared with anybody else)

- They'll remain up for the duration of the workshop

- You should have a little card with login+password+IP addresses

- You can automatically SSH from one VM to another

- The nodes have aliases: `node1`, `node2`, etc.

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

class: in-person

## Why don't we run containers locally?

- Installing that stuff can be hard on some machines

  (32 bits CPU or OS... Laptops without administrator access... etc.)

- *"The whole team downloaded all these container images from the WiFi!
  <br/>... and it went great!"* (Literally no-one ever)

- All you need is a computer (or even a phone or tablet!), with:

  - an internet connection

  - a web browser

  - an SSH client

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

class: in-person

## SSH clients

- On Linux, OS X, FreeBSD... you are probably all set

- On Windows, get one of these:

  - [putty](http://www.putty.org/)
  - Microsoft [Win32 OpenSSH](https://github.com/PowerShell/Win32-OpenSSH/wiki/Install-Win32-OpenSSH)
  - [Git BASH](https://git-for-windows.github.io/)
  - [MobaXterm](http://mobaxterm.mobatek.net/)

- On Android, [JuiceSSH](https://juicessh.com/)
  ([Play Store](https://play.google.com/store/apps/details?id=com.sonelli.juicessh))
  works pretty well

- Nice-to-have: [Mosh](https://mosh.org/) instead of SSH, if your internet connection tends to lose packets

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

class: in-person, extra-details

## What is this Mosh thing?

*You don't have to use Mosh or even know about it to follow along.
<br/>
We're just telling you about it because some of us think it's cool!*

- Mosh is "the mobile shell"

- It is essentially SSH over UDP, with roaming features

- It retransmits packets quickly, so it works great even on lossy connections

  (Like hotel or conference WiFi)

- It has intelligent local echo, so it works great even in high-latency connections

  (Like hotel or conference WiFi)

- It supports transparent roaming when your client IP address changes

  (Like when you hop from hotel to conference WiFi)

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

class: in-person, extra-details

## Using Mosh

- To install it: `(apt|yum|brew) install mosh`

- It has been pre-installed on the VMs that we are using

- To connect to a remote machine: `mosh user@host`

  (It is going to establish an SSH connection, then hand off to UDP)

- It requires UDP ports to be open

  (By default, it uses a UDP port between 60000 and 61000)

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

class: in-person

## Connecting to our lab environment

.exercise[

- Log into the first VM (`node1`) with your SSH client

<!--
```bash
for N in $(awk '/\Wnode/{print $2}' /etc/hosts); do
  ssh -o StrictHostKeyChecking=no $N true
done
```

```bash
if which kubectl; then
  kubectl get deploy,ds -o name | xargs -rn1 kubectl delete
  kubectl get all -o name | grep -v service/kubernetes | xargs -rn1 kubectl delete --ignore-not-found=true
  kubectl -n kube-system get deploy,svc -o name | grep -v dns | xargs -rn1 kubectl -n kube-system delete
fi
```
-->

- Check that you can SSH (without password) to `node2`:
  ```bash
  ssh node2
  ```
- Type `exit` or `^D` to come back to `node1`

<!-- ```bash exit``` -->

]

If anything goes wrong ‚Äî ask for help!

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

## Doing or re-doing the workshop on your own?

- Use something like
  [Play-With-Docker](http://play-with-docker.com/) or
  [Play-With-Kubernetes](https://training.play-with-kubernetes.com/)

  Zero setup effort; but environment are short-lived and
  might have limited resources

- Create your own cluster (local or cloud VMs)

  Small setup effort; small cost; flexible environments

- Create a bunch of clusters for you and your friends
    ([instructions](https://github.com/jpetazzo/container.training/tree/master/prepare-vms))

  Bigger setup effort; ideal for group training

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

class: self-paced

## Get your own Docker nodes

- If you already have some Docker nodes: great!

- If not: let's get some thanks to Play-With-Docker

.exercise[

- Go to http://www.play-with-docker.com/

- Log in

- Create your first node

<!-- ```open http://www.play-with-docker.com/``` -->

]

You will need a Docker ID to use Play-With-Docker.

(Creating a Docker ID is free.)

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

## We will (mostly) interact with node1 only

*These remarks apply only when using multiple nodes, of course.*

- Unless instructed, **all commands must be run from the first VM, `node1`**

- We will only checkout/copy the code on `node1`

- During normal operations, we do not need access to the other nodes

- If we had to troubleshoot issues, we would use a combination of:

  - SSH (to access system logs, daemon status...)
  
  - Docker API (to check running containers and container engine status)

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

## Terminals

Once in a while, the instructions will say:
<br/>"Open a new terminal."

There are multiple ways to do this:

- create a new window or tab on your machine, and SSH into the VM;

- use screen or tmux on the VM and open a new window from there.

You are welcome to use the method that you feel the most comfortable with.

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---

## Tmux cheatsheet

[Tmux](https://en.wikipedia.org/wiki/Tmux) is a terminal multiplexer like `screen`.

*You don't have to use it or even know about it to follow along.
<br/>
But some of us like to use it to switch between terminals.
<br/>
It has been preinstalled on your workshop nodes.*

- Ctrl-b c ‚Üí creates a new window
- Ctrl-b n ‚Üí go to next window
- Ctrl-b p ‚Üí go to previous window
- Ctrl-b " ‚Üí split window top/bottom
- Ctrl-b % ‚Üí split window left/right
- Ctrl-b Alt-1 ‚Üí rearrange windows in columns
- Ctrl-b Alt-2 ‚Üí rearrange windows in rows
- Ctrl-b arrows ‚Üí navigate to other windows
- Ctrl-b d ‚Üí detach session
- tmux attach ‚Üí reattach to session

.debug[[shared/prereqs.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/prereqs.md)]
---
## Versions installed

- Kubernetes 1.11.0
- Docker Engine 18.03.1-ce
- Docker Compose 1.21.1


.exercise[

- Check all installed versions:
  ```bash
  kubectl version
  docker version
  docker-compose -v
  ```

]

.debug[[k8s/versions-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/versions-k8s.md)]
---

class: extra-details

## Kubernetes and Docker compatibility

- Kubernetes 1.10.x only validates Docker Engine versions [1.11.2 to 1.13.1 and 17.03.x](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.10.md#external-dependencies)

--

class: extra-details

- Are we living dangerously?

--

class: extra-details

- "Validates" = continuous integration builds

- The Docker API is versioned, and offers strong backward-compatibility

  (If a client uses e.g. API v1.25, the Docker Engine will keep behaving the same way)

.debug[[k8s/versions-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/versions-k8s.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/containers-by-the-water.jpg)]

---

name: toc-our-sample-application
class: title

Our sample application

.nav[
[Previous section](#toc-pre-requirements)
|
[Back to table of contents](#toc-chapter-5)
|
[Next section](#toc-kubernetes-concepts)
]

.debug[(automatically generated title slide)]

---
# Our sample application

- We will clone the GitHub repository onto our `node1`

- The repository also contains scripts and tools that we will use through the workshop

.exercise[

<!--
```bash
cd ~
if [ -d container.training ]; then
  mv container.training container.training.$RANDOM
fi
```
-->

- Clone the repository on `node1`:
  ```bash
  git clone git://github.com/jpetazzo/container.training
  ```

]

(You can also fork the repository on GitHub and clone your fork if you prefer that.)

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

## Downloading and running the application

Let's start this before we look around, as downloading will take a little time...

.exercise[

- Go to the `dockercoins` directory, in the cloned repo:
  ```bash
  cd ~/container.training/dockercoins
  ```

- Use Compose to build and run all containers:
  ```bash
  docker-compose up
  ```

<!--
```longwait units of work done```
-->

]

Compose tells Docker to build all container images (pulling
the corresponding base images), then starts all containers,
and displays aggregated logs.

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

## More detail on our sample application

- Visit the GitHub repository with all the materials of this workshop:
  <br/>https://github.com/jpetazzo/container.training

- The application is in the [dockercoins](
  https://github.com/jpetazzo/container.training/tree/master/dockercoins)
  subdirectory

- Let's look at the general layout of the source code:

  there is a Compose file [docker-compose.yml](
  https://github.com/jpetazzo/container.training/blob/master/dockercoins/docker-compose.yml) ...

  ... and 4 other services, each in its own directory:

  - `rng` = web service generating random bytes
  - `hasher` = web service computing hash of POSTed data
  - `worker` = background process using `rng` and `hasher`
  - `webui` = web interface to watch progress

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

class: extra-details

## Compose file format version

*Particularly relevant if you have used Compose before...*

- Compose 1.6 introduced support for a new Compose file format (aka "v2")

- Services are no longer at the top level, but under a `services` section

- There has to be a `version` key at the top level, with value `"2"` (as a string, not an integer)

- Containers are placed on a dedicated network, making links unnecessary

- There are other minor differences, but upgrade is easy and straightforward

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

## Service discovery in container-land

- We do not hard-code IP addresses in the code

- We do not hard-code FQDN in the code, either

- We just connect to a service name, and container-magic does the rest

  (And by container-magic, we mean "a crafty, dynamic, embedded DNS server")

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

## Example in `worker/worker.py`

```python
redis = Redis("`redis`")


def get_random_bytes():
    r = requests.get("http://`rng`/32")
    return r.content


def hash_bytes(data):
    r = requests.post("http://`hasher`/",
                      data=data,
                      headers={"Content-Type": "application/octet-stream"})
```

(Full source code available [here](
https://github.com/jpetazzo/container.training/blob/8279a3bce9398f7c1a53bdd95187c53eda4e6435/dockercoins/worker/worker.py#L17
))

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

class: extra-details

## Links, naming, and service discovery

- Containers can have network aliases (resolvable through DNS)

- Compose file version 2+ makes each container reachable through its service name

- Compose file version 1 did require "links" sections

- Network aliases are automatically namespaced

  - you can have multiple apps declaring and using a service named `database`

  - containers in the blue app will resolve `database` to the IP of the blue database

  - containers in the green app will resolve `database` to the IP of the green database

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

## What's this application?

--

- It is a DockerCoin miner! .emoji[üí∞üê≥üì¶üö¢]

--

- No, you can't buy coffee with DockerCoins

--

- How DockerCoins works:

  - `worker` asks to `rng` to generate a few random bytes

  - `worker` feeds these bytes into `hasher`

  - and repeat forever!

  - every second, `worker` updates `redis` to indicate how many loops were done

  - `webui` queries `redis`, and computes and exposes "hashing speed" in your browser

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

## Our application at work

- On the left-hand side, the "rainbow strip" shows the container names

- On the right-hand side, we see the output of our containers

- We can see the `worker` service making requests to `rng` and `hasher`

- For `rng` and `hasher`, we see HTTP access logs

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

## Connecting to the web UI

- "Logs are exciting and fun!" (No-one, ever)

- The `webui` container exposes a web dashboard; let's view it

.exercise[

- With a web browser, connect to `node1` on port 8000

- Remember: the `nodeX` aliases are valid only on the nodes themselves

- In your browser, you need to enter the IP address of your node

<!-- ```open http://node1:8000``` -->

]

A drawing area should show up, and after a few seconds, a blue
graph will appear.

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

class: self-paced, extra-details

## If the graph doesn't load

If you just see a `Page not found` error, it might be because your
Docker Engine is running on a different machine. This can be the case if:

- you are using the Docker Toolbox

- you are using a VM (local or remote) created with Docker Machine

- you are controlling a remote Docker Engine

When you run DockerCoins in development mode, the web UI static files
are mapped to the container using a volume. Alas, volumes can only
work on a local environment, or when using Docker4Mac or Docker4Windows.

How to fix this?

Stop the app with `^C`, edit `dockercoins.yml`, comment out the `volumes` section, and try again.

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

class: extra-details

## Why does the speed seem irregular?

- It *looks like* the speed is approximately 4 hashes/second

- Or more precisely: 4 hashes/second, with regular dips down to zero

- Why?

--

class: extra-details

- The app actually has a constant, steady speed: 3.33 hashes/second
  <br/>
  (which corresponds to 1 hash every 0.3 seconds, for *reasons*)

- Yes, and?

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

class: extra-details

## The reason why this graph is *not awesome*

- The worker doesn't update the counter after every loop, but up to once per second

- The speed is computed by the browser, checking the counter about once per second

- Between two consecutive updates, the counter will increase either by 4, or by 0

- The perceived speed will therefore be 4 - 4 - 4 - 0 - 4 - 4 - 0 etc.

- What can we conclude from this?

--

class: extra-details

- "I'm clearly incapable of writing good frontend code!" üòÄ ‚Äî J√©r√¥me

.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---

## Stopping the application

- If we interrupt Compose (with `^C`), it will politely ask the Docker Engine to stop the app

- The Docker Engine will send a `TERM` signal to the containers

- If the containers do not exit in a timely manner, the Engine sends a `KILL` signal

.exercise[

- Stop the application by hitting `^C`

<!--
```keys ^C```
-->

]

--

Some containers exit immediately, others take longer.

The containers that do not handle `SIGTERM` end up being killed after a 10s timeout. If we are very impatient, we can hit `^C` a second time!


.debug[[shared/sampleapp.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/sampleapp.md)]
---
## Clean up

- Before moving on, let's remove those containers

.exercise[

- Tell Compose to remove everything:
  ```bash
  docker-compose down
  ```

]

.debug[[shared/composedown.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/composedown.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/distillery-containers.jpg)]

---

name: toc-kubernetes-concepts
class: title

Kubernetes concepts

.nav[
[Previous section](#toc-our-sample-application)
|
[Back to table of contents](#toc-chapter-5)
|
[Next section](#toc-declarative-vs-imperative)
]

.debug[(automatically generated title slide)]

---
# Kubernetes concepts

- Kubernetes is a container management system

- It runs and manages containerized applications on a cluster

--

- What does that really mean?

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Basic things we can ask Kubernetes to do

--

- Start 5 containers using image `atseashop/api:v1.3`

--

- Place an internal load balancer in front of these containers

--

- Start 10 containers using image `atseashop/webfront:v1.3`

--

- Place a public load balancer in front of these containers

--

- It's Black Friday (or Christmas), traffic spikes, grow our cluster and add containers

--

- New release! Replace my containers with the new image `atseashop/webfront:v1.4`

--

- Keep processing requests during the upgrade; update my containers one at a time

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Other things that Kubernetes can do for us

- Basic autoscaling

- Blue/green deployment, canary deployment

- Long running services, but also batch (one-off) jobs

- Overcommit our cluster and *evict* low-priority jobs

- Run services with *stateful* data (databases etc.)

- Fine-grained access control defining *what* can be done by *whom* on *which* resources

- Integrating third party services (*service catalog*)

- Automating complex tasks (*operators*)

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Kubernetes architecture

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

class: pic

![haha only kidding](images/k8s-arch1.png)

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Kubernetes architecture

- Ha ha ha ha

- OK, I was trying to scare you, it's much simpler than that ‚ù§Ô∏è

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

class: pic

![that one is more like the real thing](images/k8s-arch2.png)

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Credits

- The first schema is a Kubernetes cluster with storage backed by multi-path iSCSI

  (Courtesy of [Yongbok Kim](https://www.yongbok.net/blog/))

- The second one is a simplified representation of a Kubernetes cluster

  (Courtesy of [Imesh Gunaratne](https://medium.com/containermind/a-reference-architecture-for-deploying-wso2-middleware-on-kubernetes-d4dee7601e8e))

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Kubernetes architecture: the nodes

- The nodes executing our containers run a collection of services:

  - a container Engine (typically Docker)

  - kubelet (the "node agent")

  - kube-proxy (a necessary but not sufficient network component)

- Nodes were formerly called "minions"

  (You might see that word in older articles or documentation)

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Kubernetes architecture: the control plane

- The Kubernetes logic (its "brains") is a collection of services:

  - the API server (our point of entry to everything!)

  - core services like the scheduler and controller manager

  - `etcd` (a highly available key/value store; the "database" of Kubernetes)

- Together, these services form the control plane of our cluster

- The control plane is also called the "master"

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Running the control plane on special nodes

- It is common to reserve a dedicated node for the control plane

  (Except for single-node development clusters, like when using minikube)

- This node is then called a "master"

  (Yes, this is ambiguous: is the "master" a node, or the whole control plane?)

- Normal applications are restricted from running on this node

  (By using a mechanism called ["taints"](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/))

- When high availability is required, each service of the control plane must be resilient

- The control plane is then replicated on multiple nodes

  (This is sometimes called a "multi-master" setup)

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Running the control plane outside containers

- The services of the control plane can run in or out of containers

- For instance: since `etcd` is a critical service, some people
  deploy it directly on a dedicated cluster (without containers)

  (This is illustrated on the first "super complicated" schema)

- In some hosted Kubernetes offerings (e.g. AKS, GKE, EKS), the control plane is invisible

  (We only "see" a Kubernetes API endpoint)

- In that case, there is no "master node"

*For this reason, it is more accurate to say "control plane" rather than "master".*

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Do we need to run Docker at all?

No!

--

- By default, Kubernetes uses the Docker Engine to run containers

- We could also use `rkt` ("Rocket") from CoreOS

- Or leverage other pluggable runtimes through the *Container Runtime Interface*

  (like CRI-O, or containerd)

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Do we need to run Docker at all?

Yes!

--

- In this workshop, we run our app on a single node first

- We will need to build images and ship them around

- We can do these things without Docker
  <br/>
  (and get diagnosed with NIH¬π syndrome)

- Docker is still the most stable container engine today
  <br/>
  (but other options are maturing very quickly)

.footnote[¬π[Not Invented Here](https://en.wikipedia.org/wiki/Not_invented_here)]

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Do we need to run Docker at all?

- On our development environments, CI pipelines ... :

  *Yes, almost certainly*

- On our production servers:

  *Yes (today)*

  *Probably not (in the future)*

.footnote[More information about CRI [on the Kubernetes blog](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes)]

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Kubernetes resources

- The Kubernetes API defines a lot of objects called *resources*

- These resources are organized by type, or `Kind` (in the API)

- A few common resource types are:

  - node (a machine ‚Äî physical or virtual ‚Äî in our cluster)
  - pod (group of containers running together on a node)
  - service (stable network endpoint to connect to one or multiple containers)
  - namespace (more-or-less isolated group of things)
  - secret (bundle of sensitive data to be passed to a container)
 
  And much more!

- We can see the full list by running `kubectl api-resources`

  (In Kubernetes 1.10 and prior, the command to list API resources was `kubectl get`)

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

class: pic

![Node, pod, container](images/k8s-arch3-thanks-weave.png)

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

class: pic

![One of the best Kubernetes architecture diagrams available](images/k8s-arch4-thanks-luxas.png)

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

## Credits

- The first diagram is courtesy of Weave Works

  - a *pod* can have multiple containers working together

  - IP addresses are associated with *pods*, not with individual containers

- The second diagram is courtesy of Lucas K√§ldstr√∂m, in [this presentation](https://speakerdeck.com/luxas/kubeadm-cluster-creation-internals-from-self-hosting-to-upgradability-and-ha)

  - it's one of the best Kubernetes architecture diagrams available!

Both diagrams used with permission.

.debug[[k8s/concepts-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/concepts-k8s.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/lots-of-containers.jpg)]

---

name: toc-declarative-vs-imperative
class: title

Declarative vs imperative

.nav[
[Previous section](#toc-kubernetes-concepts)
|
[Back to table of contents](#toc-chapter-5)
|
[Next section](#toc-kubernetes-network-model)
]

.debug[(automatically generated title slide)]

---
# Declarative vs imperative

- Our container orchestrator puts a very strong emphasis on being *declarative*

- Declarative:

  *I would like a cup of tea.*

- Imperative:

  *Boil some water. Pour it in a teapot. Add tea leaves. Steep for a while. Serve in a cup.*

--

- Declarative seems simpler at first ... 

--

- ... As long as you know how to brew tea

.debug[[shared/declarative.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/declarative.md)]
---

## Declarative vs imperative

- What declarative would really be:

  *I want a cup of tea, obtained by pouring an infusion¬π of tea leaves in a cup.*

--

  *¬πAn infusion is obtained by letting the object steep a few minutes in hot¬≤ water.*

--

  *¬≤Hot liquid is obtained by pouring it in an appropriate container¬≥ and setting it on a stove.*

--

  *¬≥Ah, finally, containers! Something we know about. Let's get to work, shall we?*

--

.footnote[Did you know there was an [ISO standard](https://en.wikipedia.org/wiki/ISO_3103)
specifying how to brew tea?]

.debug[[shared/declarative.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/declarative.md)]
---

## Declarative vs imperative

- Imperative systems:

  - simpler

  - if a task is interrupted, we have to restart from scratch

- Declarative systems:

  - if a task is interrupted (or if we show up to the party half-way through),
    we can figure out what's missing and do only what's necessary

  - we need to be able to *observe* the system

  - ... and compute a "diff" between *what we have* and *what we want*

.debug[[shared/declarative.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/declarative.md)]
---
## Declarative vs imperative in Kubernetes

- Virtually everything we create in Kubernetes is created from a *spec*

- Watch for the `spec` fields in the YAML files later!

- The *spec* describes *how we want the thing to be*

- Kubernetes will *reconcile* the current state with the spec
  <br/>(technically, this is done by a number of *controllers*)

- When we want to change some resource, we update the *spec*

- Kubernetes will then *converge* that resource

.debug[[k8s/declarative.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/declarative.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/plastic-containers.JPG)]

---

name: toc-kubernetes-network-model
class: title

Kubernetes network model

.nav[
[Previous section](#toc-declarative-vs-imperative)
|
[Back to table of contents](#toc-chapter-5)
|
[Next section](#toc-first-contact-with-kubectl)
]

.debug[(automatically generated title slide)]

---
# Kubernetes network model

- TL,DR:

  *Our cluster (nodes and pods) is one big flat IP network.*

--

- In detail:

 - all nodes must be able to reach each other, without NAT

 - all pods must be able to reach each other, without NAT

 - pods and nodes must be able to reach each other, without NAT

 - each pod is aware of its IP address (no NAT)

- Kubernetes doesn't mandate any particular implementation

.debug[[k8s/kubenet.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubenet.md)]
---

## Kubernetes network model: the good

- Everything can reach everything

- No address translation

- No port translation

- No new protocol

- Pods cannot move from a node to another and keep their IP address

- IP addresses don't have to be "portable" from a node to another

  (We can use e.g. a subnet per node and use a simple routed topology)

- The specification is simple enough to allow many various implementations

.debug[[k8s/kubenet.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubenet.md)]
---

## Kubernetes network model: the less good

- Everything can reach everything

  - if you want security, you need to add network policies

  - the network implementation that you use needs to support them

- There are literally dozens of implementations out there

  (15 are listed in the Kubernetes documentation)

- Pods have level 3 (IP) connectivity, but *services* are level 4

  (Services map to a single UDP or TCP port; no port ranges or arbitrary IP packets)

- `kube-proxy` is on the data path when connecting to a pod or container,
  <br/>and it's not particularly fast (relies on userland proxying or iptables)

.debug[[k8s/kubenet.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubenet.md)]
---

## Kubernetes network model: in practice

- The nodes that we are using have been set up to use [Weave](https://github.com/weaveworks/weave)

- We don't endorse Weave in a particular way, it just Works For Us

- Don't worry about the warning about `kube-proxy` performance

- Unless you:

  - routinely saturate 10G network interfaces
  - count packet rates in millions per second
  - run high-traffic VOIP or gaming platforms
  - do weird things that involve millions of simultaneous connections
    <br/>(in which case you're already familiar with kernel tuning)

- If necessary, there are alternatives to `kube-proxy`; e.g.
  [`kube-router`](https://www.kube-router.io)

.debug[[k8s/kubenet.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubenet.md)]
---

## The Container Network Interface (CNI)

- The CNI has a well-defined [specification](https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration) for network plugins

- When a pod is created, Kubernetes delegates the network setup to CNI plugins

- Typically, a CNI plugin will:

  - allocate an IP address (by calling an IPAM plugin)

  - add a network interface into the pod's network namespace

  - configure the interface as well as required routes etc.

- Using multiple plugins can be done with "meta-plugins" like CNI-Genie or Multus

- Not all CNI plugins are equal

  (e.g. they don't all implement network policies, which are required to isolate pods)

.debug[[k8s/kubenet.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubenet.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-1.jpg)]

---

name: toc-first-contact-with-kubectl
class: title

First contact with `kubectl`

.nav[
[Previous section](#toc-kubernetes-network-model)
|
[Back to table of contents](#toc-chapter-5)
|
[Next section](#toc-setting-up-kubernetes)
]

.debug[(automatically generated title slide)]

---
# First contact with `kubectl`

- `kubectl` is (almost) the only tool we'll need to talk to Kubernetes

- It is a rich CLI tool around the Kubernetes API

  (Everything you can do with `kubectl`, you can do directly with the API)

- On our machines, there is a `~/.kube/config` file with:

  - the Kubernetes API address

  - the path to our TLS certificates used to authenticate

- You can also use the `--kubeconfig` flag to pass a config file

- Or directly `--server`, `--user`, etc.

- `kubectl` can be pronounced "Cube C T L", "Cube cuttle", "Cube cuddle"...

.debug[[k8s/kubectlget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlget.md)]
---

## `kubectl get`

- Let's look at our `Node` resources with `kubectl get`!

.exercise[

- Look at the composition of our cluster:
  ```bash
  kubectl get node
  ```

- These commands are equivalent:
  ```bash
  kubectl get no
  kubectl get node
  kubectl get nodes
  ```

]

.debug[[k8s/kubectlget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlget.md)]
---

## Obtaining machine-readable output

- `kubectl get` can output JSON, YAML, or be directly formatted

.exercise[

- Give us more info about the nodes:
  ```bash
  kubectl get nodes -o wide
  ```

- Let's have some YAML:
  ```bash
  kubectl get no -o yaml
  ```
  See that `kind: List` at the end? It's the type of our result!

]

.debug[[k8s/kubectlget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlget.md)]
---

## (Ab)using `kubectl` and `jq`

- It's super easy to build custom reports

.exercise[

- Show the capacity of all our nodes as a stream of JSON objects:
  ```bash
    kubectl get nodes -o json | 
            jq ".items[] | {name:.metadata.name} + .status.capacity"
  ```

]

.debug[[k8s/kubectlget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlget.md)]
---

## What's available?

- `kubectl` has pretty good introspection facilities

- We can list all available resource types by running `kubectl api-resources`
  <br/>
  (In Kubernetes 1.10 and prior, this command used to be `kubectl get`)

- We can view details about a resource with:
  ```bash
  kubectl describe type/name
  kubectl describe type name
  ```

- We can view the definition for a resource type with:
  ```bash
  kubectl explain type
  ```

Each time, `type` can be singular, plural, or abbreviated type name.

.debug[[k8s/kubectlget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlget.md)]
---

## Services

- A *service* is a stable endpoint to connect to "something"

  (In the initial proposal, they were called "portals")

.exercise[

- List the services on our cluster with one of these commands:
  ```bash
  kubectl get services
  kubectl get svc
  ```

]

--

There is already one service on our cluster: the Kubernetes API itself.

.debug[[k8s/kubectlget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlget.md)]
---

## ClusterIP services

- A `ClusterIP` service is internal, available from the cluster only

- This is useful for introspection from within containers

.exercise[

- Try to connect to the API:
  ```bash
  curl -k https://`10.96.0.1`
  ```
  
  - `-k` is used to skip certificate verification

  - Make sure to replace 10.96.0.1 with the CLUSTER-IP shown by `kubectl get svc`

]

--

The error that we see is expected: the Kubernetes API requires authentication.

.debug[[k8s/kubectlget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlget.md)]
---

## Listing running containers

- Containers are manipulated through *pods*

- A pod is a group of containers:

 - running together (on the same node)

 - sharing resources (RAM, CPU; but also network, volumes)

.exercise[

- List pods on our cluster:
  ```bash
  kubectl get pods
  ```

]

--

*These are not the pods you're looking for.* But where are they?!?

.debug[[k8s/kubectlget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlget.md)]
---

## Namespaces

- Namespaces allow us to segregate resources

.exercise[

- List the namespaces on our cluster with one of these commands:
  ```bash
  kubectl get namespaces
  kubectl get namespace
  kubectl get ns
  ```

]

--

*You know what ... This `kube-system` thing looks suspicious.*

.debug[[k8s/kubectlget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlget.md)]
---

## Accessing namespaces

- By default, `kubectl` uses the `default` namespace

- We can switch to a different namespace with the `-n` option

.exercise[

- List the pods in the `kube-system` namespace:
  ```bash
  kubectl -n kube-system get pods
  ```

]

--

*Ding ding ding ding ding!*

The `kube-system` namespace is used for the control plane.

.debug[[k8s/kubectlget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlget.md)]
---

## What are all these control plane pods?

- `etcd` is our etcd server

- `kube-apiserver` is the API server

- `kube-controller-manager` and `kube-scheduler` are other master components

- `coredns` provides DNS-based service discovery ([replacing kube-dns as of 1.11](https://kubernetes.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/))

- `kube-proxy` is the (per-node) component managing port mappings and such

- `weave` is the (per-node) component managing the network overlay

- the `READY` column indicates the number of containers in each pod

- the pods with a name ending with `-node1` are the master components
  <br/>
  (they have been specifically "pinned" to the master node)

.debug[[k8s/kubectlget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlget.md)]
---

## What about `kube-public`?

.exercise[

- List the pods in the `kube-public` namespace:
  ```bash
  kubectl -n kube-public get pods
  ```

]

--

- Maybe it doesn't have pods, but what secrets is `kube-public` keeping?

--

.exercise[

- List the secrets in the `kube-public` namespace:
  ```bash
  kubectl -n kube-public get secrets
  ```

]
--

- `kube-public` is created by kubeadm & [used for security bootstrapping](https://kubernetes.io/blog/2017/01/stronger-foundation-for-creating-and-managing-kubernetes-clusters)

.debug[[k8s/kubectlget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlget.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-2.jpg)]

---

name: toc-setting-up-kubernetes
class: title

Setting up Kubernetes

.nav[
[Previous section](#toc-first-contact-with-kubectl)
|
[Back to table of contents](#toc-chapter-5)
|
[Next section](#toc-running-our-first-containers-on-kubernetes)
]

.debug[(automatically generated title slide)]

---
# Setting up Kubernetes

- How did we set up these Kubernetes clusters that we're using?

--

- We used `kubeadm` on freshly installed VM instances running Ubuntu 16.04 LTS

    1. Install Docker

    2. Install Kubernetes packages

    3. Run `kubeadm init` on the first node (it deploys the control plane on that node)

    4. Set up Weave (the overlay network)
       <br/>
       (that step is just one `kubectl apply` command; discussed later)

    5. Run `kubeadm join` on the other nodes (with the token produced by `kubeadm init`)

    6. Copy the configuration file generated by `kubeadm init`

- Check the [prepare VMs README](https://github.com/jpetazzo/container.training/blob/master/prepare-vms/README.md) for more details

.debug[[k8s/setup-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/setup-k8s.md)]
---

## `kubeadm` drawbacks

- Doesn't set up Docker or any other container engine

- Doesn't set up the overlay network

- Doesn't set up multi-master (no high availability)

--

  (At least ... not yet!)

--

- "It's still twice as many steps as setting up a Swarm cluster üòï" -- J√©r√¥me

.debug[[k8s/setup-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/setup-k8s.md)]
---

## Other deployment options

- If you are on Azure:
  [AKS](https://azure.microsoft.com/services/container-service/)

- If you are on Google Cloud:
  [GKE](https://cloud.google.com/kubernetes-engine/)

- If you are on AWS:
  [EKS](https://aws.amazon.com/eks/)
  or
  [kops](https://github.com/kubernetes/kops)

- On a local machine:
  [minikube](https://kubernetes.io/docs/getting-started-guides/minikube/),
  [kubespawn](https://github.com/kinvolk/kube-spawn),
  [Docker4Mac](https://docs.docker.com/docker-for-mac/kubernetes/)

- If you want something customizable:
  [kubicorn](https://github.com/kubicorn/kubicorn)

  Probably the closest to a multi-cloud/hybrid solution so far, but in development

.debug[[k8s/setup-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/setup-k8s.md)]
---

## Even more deployment options

- If you like Ansible:
  [kubespray](https://github.com/kubernetes-incubator/kubespray)

- If you like Terraform:
  [typhoon](https://github.com/poseidon/typhoon/)

- You can also learn how to install every component manually, with
  the excellent tutorial [Kubernetes The Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way)

  *Kubernetes The Hard Way is optimized for learning, which means taking the long route to ensure you understand each task required to bootstrap a Kubernetes cluster.*

- There are also many commercial options available!

- For a longer list, check the Kubernetes documentation:
  <br/>
  it has a great guide to [pick the right solution](https://kubernetes.io/docs/setup/pick-right-solution/) to set up Kubernetes.

.debug[[k8s/setup-k8s.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/setup-k8s.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/two-containers-on-a-truck.jpg)]

---

name: toc-running-our-first-containers-on-kubernetes
class: title

Running our first containers on Kubernetes

.nav[
[Previous section](#toc-setting-up-kubernetes)
|
[Back to table of contents](#toc-chapter-6)
|
[Next section](#toc-exposing-containers)
]

.debug[(automatically generated title slide)]

---
# Running our first containers on Kubernetes

- First things first: we cannot run a container

--

- We are going to run a pod, and in that pod there will be a single container

--

- In that container in the pod, we are going to run a simple `ping` command

- Then we are going to start additional copies of the pod

.debug[[k8s/kubectlrun.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlrun.md)]
---

## Starting a simple pod with `kubectl run`

- We need to specify at least a *name* and the image we want to use

.exercise[

- Let's ping `1.1.1.1`, Cloudflare's 
  [public DNS resolver](https://blog.cloudflare.com/announcing-1111/):
  ```bash
  kubectl run pingpong --image alpine ping 1.1.1.1
  ```

<!-- ```hide kubectl wait deploy/pingpong --for condition=available``` -->

]

--

OK, what just happened?

.debug[[k8s/kubectlrun.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlrun.md)]
---

## Behind the scenes of `kubectl run`

- Let's look at the resources that were created by `kubectl run`

.exercise[

- List most resource types:
  ```bash
  kubectl get all
  ```

]

--

We should see the following things:
- `deployment.apps/pingpong` (the *deployment* that we just created)
- `replicaset.apps/pingpong-xxxxxxxxxx` (a *replica set* created by the deployment)
- `pod/pingpong-xxxxxxxxxx-yyyyy` (a *pod* created by the replica set)

Note: as of 1.10.1, resource types are displayed in more detail.

.debug[[k8s/kubectlrun.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlrun.md)]
---

## What are these different things?

- A *deployment* is a high-level construct

  - allows scaling, rolling updates, rollbacks

  - multiple deployments can be used together to implement a
    [canary deployment](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments)

  - delegates pods management to *replica sets*

- A *replica set* is a low-level construct

  - makes sure that a given number of identical pods are running

  - allows scaling

  - rarely used directly

- A *replication controller* is the (deprecated) predecessor of a replica set

.debug[[k8s/kubectlrun.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlrun.md)]
---

## Our `pingpong` deployment

- `kubectl run` created a *deployment*, `deployment.apps/pingpong`

```
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   1         1         1            1           10m
```

- That deployment created a *replica set*, `replicaset.apps/pingpong-xxxxxxxxxx`

```
NAME                                  DESIRED   CURRENT   READY     AGE
replicaset.apps/pingpong-7c8bbcd9bc   1         1         1         10m
```

- That replica set created a *pod*, `pod/pingpong-xxxxxxxxxx-yyyyy`

```
NAME                            READY     STATUS    RESTARTS   AGE
pod/pingpong-7c8bbcd9bc-6c9qz   1/1       Running   0          10m
```

- We'll see later how these folks play together for:

  - scaling, high availability, rolling updates

.debug[[k8s/kubectlrun.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlrun.md)]
---

## Viewing container output

- Let's use the `kubectl logs` command

- We will pass either a *pod name*, or a *type/name*

  (E.g. if we specify a deployment or replica set, it will get the first pod in it)

- Unless specified otherwise, it will only show logs of the first container in the pod

  (Good thing there's only one in ours!)

.exercise[

- View the result of our `ping` command:
  ```bash
  kubectl logs deploy/pingpong
  ```

]

.debug[[k8s/kubectlrun.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlrun.md)]
---

## Streaming logs in real time

- Just like `docker logs`, `kubectl logs` supports convenient options:

  - `-f`/`--follow` to stream logs in real time (√† la `tail -f`)

  - `--tail` to indicate how many lines you want to see (from the end)

  - `--since` to get logs only after a given timestamp

.exercise[

- View the latest logs of our `ping` command:
  ```bash
  kubectl logs deploy/pingpong --tail 1 --follow
  ```

<!--
```wait seq=3```
```keys ^C```
-->

]

.debug[[k8s/kubectlrun.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlrun.md)]
---

## Scaling our application

- We can create additional copies of our container (I mean, our pod) with `kubectl scale`

.exercise[

- Scale our `pingpong` deployment:
  ```bash
  kubectl scale deploy/pingpong --replicas 8
  ```

]

Note: what if we tried to scale `replicaset.apps/pingpong-xxxxxxxxxx`?

We could! But the *deployment* would notice it right away, and scale back to the initial level.

.debug[[k8s/kubectlrun.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlrun.md)]
---

## Resilience

- The *deployment* `pingpong` watches its *replica set*

- The *replica set* ensures that the right number of *pods* are running

- What happens if pods disappear?

.exercise[

- In a separate window, list pods, and keep watching them:
  ```bash
  kubectl get pods -w
  ```

<!--
```wait Running```
```keys ^C```
```hide kubectl wait deploy pingpong --for condition=available```
```keys kubectl delete pod ping```
```copypaste pong-..........-.....```
-->

- Destroy a pod:
  ```
  kubectl delete pod pingpong-xxxxxxxxxx-yyyyy
  ```
]

.debug[[k8s/kubectlrun.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlrun.md)]
---

## What if we wanted something different?

- What if we wanted to start a "one-shot" container that *doesn't* get restarted?

- We could use `kubectl run --restart=OnFailure` or `kubectl run --restart=Never`

- These commands would create *jobs* or *pods* instead of *deployments*

- Under the hood, `kubectl run` invokes "generators" to create resource descriptions

- We could also write these resource descriptions ourselves (typically in YAML),
  <br/>and create them on the cluster with `kubectl apply -f` (discussed later)

- With `kubectl run --schedule=...`, we can also create *cronjobs*

.debug[[k8s/kubectlrun.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlrun.md)]
---

## Viewing logs of multiple pods

- When we specify a deployment name, only one single pod's logs are shown

- We can view the logs of multiple pods by specifying a *selector*

- A selector is a logic expression using *labels*

- Conveniently, when you `kubectl run somename`, the associated objects have a `run=somename` label

.exercise[

- View the last line of log from all pods with the `run=pingpong` label:
  ```bash
  kubectl logs -l run=pingpong --tail 1
  ```

]

Unfortunately, `--follow` cannot (yet) be used to stream the logs from multiple containers.

.debug[[k8s/kubectlrun.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlrun.md)]
---

## Aren't we flooding 1.1.1.1?

- If you're wondering this, good question!

- Don't worry, though:

  *APNIC's research group held the IP addresses 1.1.1.1 and 1.0.0.1. While the addresses were valid, so many people had entered them into various random systems that they were continuously overwhelmed by a flood of garbage traffic. APNIC wanted to study this garbage traffic but any time they'd tried to announce the IPs, the flood would overwhelm any conventional network.*

  (Source: https://blog.cloudflare.com/announcing-1111/)

- It's very unlikely that our concerted pings manage to produce
  even a modest blip at Cloudflare's NOC!

.debug[[k8s/kubectlrun.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlrun.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/wall-of-containers.jpeg)]

---

name: toc-exposing-containers
class: title

Exposing containers

.nav[
[Previous section](#toc-running-our-first-containers-on-kubernetes)
|
[Back to table of contents](#toc-chapter-6)
|
[Next section](#toc-deploying-a-self-hosted-registry)
]

.debug[(automatically generated title slide)]

---
# Exposing containers

- `kubectl expose` creates a *service* for existing pods

- A *service* is a stable address for a pod (or a bunch of pods)

- If we want to connect to our pod(s), we need to create a *service*

- Once a service is created, CoreDNS will allow us to resolve it by name

  (i.e. after creating service `hello`, the name `hello` will resolve to something)

- There are different types of services, detailed on the following slides:

  `ClusterIP`, `NodePort`, `LoadBalancer`, `ExternalName`

.debug[[k8s/kubectlexpose.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlexpose.md)]
---

## Basic service types

- `ClusterIP` (default type)

  - a virtual IP address is allocated for the service (in an internal, private range)
  - this IP address is reachable only from within the cluster (nodes and pods)
  - our code can connect to the service using the original port number

- `NodePort`

  - a port is allocated for the service (by default, in the 30000-32768 range)
  - that port is made available *on all our nodes* and anybody can connect to it
  - our code must be changed to connect to that new port number

These service types are always available.

Under the hood: `kube-proxy` is using a userland proxy and a bunch of `iptables` rules.

.debug[[k8s/kubectlexpose.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlexpose.md)]
---

## More service types

- `LoadBalancer`

  - an external load balancer is allocated for the service
  - the load balancer is configured accordingly
    <br/>(e.g.: a `NodePort` service is created, and the load balancer sends traffic to that port)

- `ExternalName`

  - the DNS entry managed by CoreDNS will just be a `CNAME` to a provided record
  - no port, no IP address, no nothing else is allocated

The `LoadBalancer` type is currently only available on AWS, Azure, and GCE.

.debug[[k8s/kubectlexpose.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlexpose.md)]
---

## Running containers with open ports

- Since `ping` doesn't have anything to connect to, we'll have to run something else

.exercise[

- Start a bunch of ElasticSearch containers:
  ```bash
  kubectl run elastic --image=elasticsearch:2 --replicas=7
  ```

- Watch them being started:
  ```bash
  kubectl get pods -w
  ```

<!--
```wait elastic-```
```keys ^C```
-->

]

The `-w` option "watches" events happening on the specified resources.

Note: please DO NOT call the service `search`. It would collide with the TLD.

.debug[[k8s/kubectlexpose.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlexpose.md)]
---

## Exposing our deployment

- We'll create a default `ClusterIP` service

.exercise[

- Expose the ElasticSearch HTTP API port:
  ```bash
  kubectl expose deploy/elastic --port 9200
  ```

- Look up which IP address was allocated:
  ```bash
  kubectl get svc
  ```

]

.debug[[k8s/kubectlexpose.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlexpose.md)]
---

## Services are layer 4 constructs

- You can assign IP addresses to services, but they are still *layer 4*

  (i.e. a service is not an IP address; it's an IP address + protocol + port)

- This is caused by the current implementation of `kube-proxy`

  (it relies on mechanisms that don't support layer 3)

- As a result: you *have to* indicate the port number for your service
    
- Running services with arbitrary port (or port ranges) requires hacks

  (e.g. host networking mode)

.debug[[k8s/kubectlexpose.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlexpose.md)]
---

## Testing our service

- We will now send a few HTTP requests to our ElasticSearch pods

.exercise[

- Let's obtain the IP address that was allocated for our service, *programmatically:*
  ```bash
  IP=$(kubectl get svc elastic -o go-template --template '{{ .spec.clusterIP }}')
  ```

<!--
```hide kubectl wait deploy elastic --for condition=available```
```hide sleep 5``` (give some time for elasticsearch to start... hopefully this is enough!)
-->

- Send a few requests:
  ```bash
  curl http://$IP:9200/
  ```

]

--

We may see `curl: (7) Failed to connect to _IP_ port 9200: Connection refused`.

This is normal while the service starts up.

--

Once it's running, our requests are load balanced across multiple pods.

.debug[[k8s/kubectlexpose.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlexpose.md)]
---

class: extra-details

## If we don't need a load balancer

- Sometimes, we want to access our scaled services directly:

  - if we want to save a tiny little bit of latency (typically less than 1ms)

  - if we need to connect over arbitrary ports (instead of a few fixed ones)

  - if we need to communicate over another protocol than UDP or TCP

  - if we want to decide how to balance the requests client-side

  - ...

- In that case, we can use a "headless service"

.debug[[k8s/kubectlexpose.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlexpose.md)]
---

class: extra-details

## Headless services

- A headless service is obtained by setting the `clusterIP` field to `None`

  (Either with `--cluster-ip=None`, or by providing a custom YAML)

- As a result, the service doesn't have a virtual IP address

- Since there is no virtual IP address, there is no load balancer either

- CoreDNS will return the pods' IP addresses as multiple `A` records

- This gives us an easy way to discover all the replicas for a deployment

.debug[[k8s/kubectlexpose.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlexpose.md)]
---

class: extra-details

## Services and endpoints

- A service has a number of "endpoints"

- Each endpoint is a host + port where the service is available

- The endpoints are maintained and updated automatically by Kubernetes

.exercise[

- Check the endpoints that Kubernetes has associated with our `elastic` service:
  ```bash
  kubectl describe service elastic
  ```

]

In the output, there will be a line starting with `Endpoints:`.

That line will list a bunch of addresses in `host:port` format.

.debug[[k8s/kubectlexpose.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlexpose.md)]
---

class: extra-details

## Viewing endpoint details

- When we have many endpoints, our display commands truncate the list
  ```bash
  kubectl get endpoints
  ```

- If we want to see the full list, we can use one of the following commands:
  ```bash
  kubectl describe endpoints elastic
  kubectl get endpoints elastic -o yaml
  ```

- These commands will show us a list of IP addresses

- These IP addresses should match the addresses of the corresponding pods:
  ```bash
  kubectl get pods -l run=elastic -o wide
  ```

.debug[[k8s/kubectlexpose.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlexpose.md)]
---

class: extra-details

## `endpoints` not `endpoint`

- `endpoints` is the only resource that cannot be singular

```bash
$ kubectl get endpoint
error: the server doesn't have a resource type "endpoint"
```

- This is because the type itself is plural (unlike every other resource)

- There is no `endpoint` object: `type Endpoints struct`

- The type doesn't represent a single endpoint, but a list of endpoints

.debug[[k8s/kubectlexpose.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlexpose.md)]
---
class: title

Our app on Kube

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## What's on the menu?

In this part, we will:

- **build** images for our app,

- **ship** these images with a registry,

- **run** deployments using these images,

- expose these deployments so they can communicate with each other,

- expose the web UI so we can access it from outside.

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## The plan

- Build on our control node (`node1`)

- Tag images so that they are named `$REGISTRY/servicename`

- Upload them to a registry

- Create deployments using the images

- Expose (with a ClusterIP) the services that need to communicate

- Expose (with a NodePort) the WebUI

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## Which registry do we want to use?

- We could use the Docker Hub

- Or a service offered by our cloud provider (ACR, GCR, ECR...)

- Or we could just self-host that registry

*We'll self-host the registry because it's the most generic solution for this workshop.*

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## Using the open source registry

- We need to run a `registry:2` container
  <br/>(make sure you specify tag `:2` to run the new version!)

- It will store images and layers to the local filesystem
  <br/>(but you can add a config file to use S3, Swift, etc.)

- Docker *requires* TLS when communicating with the registry

  - unless for registries on `127.0.0.0/8` (i.e. `localhost`)

  - or with the Engine flag `--insecure-registry`

- Our strategy: publish the registry container on a NodePort,
  <br/>so that it's available through `127.0.0.1:xxxxx` on each node

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-deploying-a-self-hosted-registry
class: title

Deploying a self-hosted registry

.nav[
[Previous section](#toc-exposing-containers)
|
[Back to table of contents](#toc-chapter-6)
|
[Next section](#toc-exposing-services-internally)
]

.debug[(automatically generated title slide)]

---

# Deploying a self-hosted registry

- We will deploy a registry container, and expose it with a NodePort

.exercise[

- Create the registry service:
  ```bash
  kubectl run registry --image=registry:2
  ```

- Expose it on a NodePort:
  ```bash
  kubectl expose deploy/registry --port=5000 --type=NodePort
  ```

]

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## Connecting to our registry

- We need to find out which port has been allocated

.exercise[

- View the service details:
  ```bash
  kubectl describe svc/registry
  ```

- Get the port number programmatically:
  ```bash
  NODEPORT=$(kubectl get svc/registry -o json | jq .spec.ports[0].nodePort)
  REGISTRY=127.0.0.1:$NODEPORT
  ```

]

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## Testing our registry

- A convenient Docker registry API route to remember is `/v2/_catalog`

.exercise[

<!-- ```hide kubectl wait deploy/registry --for condition=available```-->

- View the repositories currently held in our registry:
  ```bash
  curl $REGISTRY/v2/_catalog
  ```

]

--

We should see:
```json
{"repositories":[]}
```

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## Testing our local registry

- We can retag a small image, and push it to the registry

.exercise[

- Make sure we have the busybox image, and retag it:
  ```bash
  docker pull busybox
  docker tag busybox $REGISTRY/busybox
  ```

- Push it:
  ```bash
  docker push $REGISTRY/busybox
  ```

]

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## Checking again what's on our local registry

- Let's use the same endpoint as before

.exercise[

- Ensure that our busybox image is now in the local registry:
  ```bash
  curl $REGISTRY/v2/_catalog
  ```

]

The curl command should now output:
```json
{"repositories":["busybox"]}
```

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## Building and pushing our images

- We are going to use a convenient feature of Docker Compose

.exercise[

- Go to the `stacks` directory:
  ```bash
  cd ~/container.training/stacks
  ```

- Build and push the images:
  ```bash
  export REGISTRY
  export TAG=v0.1
  docker-compose -f dockercoins.yml build
  docker-compose -f dockercoins.yml push
  ```

]

Let's have a look at the `dockercoins.yml` file while this is building and pushing.

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

```yaml
version: "3"

services:
  rng:
    build: dockercoins/rng
    image: ${REGISTRY-127.0.0.1:5000}/rng:${TAG-latest}
    deploy:
      mode: global
  ...
  redis:
    image: redis
  ...
  worker:
    build: dockercoins/worker
    image: ${REGISTRY-127.0.0.1:5000}/worker:${TAG-latest}
    ...
    deploy:
      replicas: 10
```

.warning[Just in case you were wondering ... Docker "services" are not Kubernetes "services".]

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

class: extra-details

## Avoiding the `latest` tag

.warning[Make sure that you've set the `TAG` variable properly!]

- If you don't, the tag will default to `latest`

- The problem with `latest`: nobody knows what it points to!

  - the latest commit in the repo?

  - the latest commit in some branch? (Which one?)

  - the latest tag?

  - some random version pushed by a random team member?

- If you keep pushing the `latest` tag, how do you roll back?

- Image tags should be meaningful, i.e. correspond to code branches, tags, or hashes

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## Deploying all the things

- We can now deploy our code (as well as a redis instance)

.exercise[

- Deploy `redis`:
  ```bash
  kubectl run redis --image=redis
  ```

- Deploy everything else:
  ```bash
    for SERVICE in hasher rng webui worker; do
      kubectl run $SERVICE --image=$REGISTRY/$SERVICE:$TAG
    done
  ```

]

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## Is this working?

- After waiting for the deployment to complete, let's look at the logs!

  (Hint: use `kubectl get deploy -w` to watch deployment events)

.exercise[

<!-- ```hide
kubectl wait deploy/rng --for condition=available
kubectl wait deploy/worker --for condition=available
``` -->

- Look at some logs:
  ```bash
  kubectl logs deploy/rng
  kubectl logs deploy/worker
  ```

]

--

ü§î `rng` is fine ... But not `worker`.

--

üí° Oh right! We forgot to `expose`.

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-exposing-services-internally
class: title

Exposing services internally

.nav[
[Previous section](#toc-deploying-a-self-hosted-registry)
|
[Back to table of contents](#toc-chapter-6)
|
[Next section](#toc-exposing-services-for-external-access)
]

.debug[(automatically generated title slide)]

---

# Exposing services internally

- Three deployments need to be reachable by others: `hasher`, `redis`, `rng`

- `worker` doesn't need to be exposed

- `webui` will be dealt with later

.exercise[

- Expose each deployment, specifying the right port:
  ```bash
  kubectl expose deployment redis --port 6379
  kubectl expose deployment rng --port 80
  kubectl expose deployment hasher --port 80
  ```

]

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## Is this working yet?

- The `worker` has an infinite loop, that retries 10 seconds after an error

.exercise[

- Stream the worker's logs:
  ```bash
  kubectl logs deploy/worker --follow
  ```

  (Give it about 10 seconds to recover)

<!--
```wait units of work done, updating hash counter```
```keys ^C```
-->

]

--

We should now see the `worker`, well, working happily.

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-exposing-services-for-external-access
class: title

Exposing services for external access

.nav[
[Previous section](#toc-exposing-services-internally)
|
[Back to table of contents](#toc-chapter-6)
|
[Next section](#toc-the-kubernetes-dashboard)
]

.debug[(automatically generated title slide)]

---

# Exposing services for external access

- Now we would like to access the Web UI

- We will expose it with a `NodePort`

  (just like we did for the registry)

.exercise[

- Create a `NodePort` service for the Web UI:
  ```bash
  kubectl expose deploy/webui --type=NodePort --port=80
  ```

- Check the port that was allocated:
  ```bash
  kubectl get svc
  ```

]

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

## Accessing the web UI

- We can now connect to *any node*, on the allocated node port, to view the web UI

.exercise[

- Open the web UI in your browser (http://node-ip-address:3xxxx/)

<!-- ```open http://node1:3xxxx/``` -->

]

--

*Alright, we're back to where we started, when we were running on a single node!*

.debug[[k8s/ourapponkube.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/ourapponkube.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-the-kubernetes-dashboard
class: title

The Kubernetes dashboard

.nav[
[Previous section](#toc-exposing-services-for-external-access)
|
[Back to table of contents](#toc-chapter-7)
|
[Next section](#toc-security-implications-of-kubectl-apply)
]

.debug[(automatically generated title slide)]

---
# The Kubernetes dashboard

- Kubernetes resources can also be viewed with a web dashboard

- We are going to deploy that dashboard with *three commands:*

  1) actually *run* the dashboard

  2) bypass SSL for the dashboard

  3) bypass authentication for the dashboard

--

There is an additional step to make the dashboard available from outside (we'll get to that)

--

.footnote[.warning[Yes, this will open our cluster to all kinds of shenanigans. Don't do this at home.]]

.debug[[k8s/dashboard.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/dashboard.md)]
---

## 1) Running the dashboard

- We need to create a *deployment* and a *service* for the dashboard

- But also a *secret*, a *service account*, a *role* and a *role binding*

- All these things can be defined in a YAML file and created with `kubectl apply -f`

.exercise[

- Create all the dashboard resources, with the following command:
  ```bash
  kubectl apply -f https://goo.gl/Qamqab
  ```

]

The goo.gl URL expands to:
<br/>
.small[https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml]

.debug[[k8s/dashboard.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/dashboard.md)]
---


## 2) Bypassing SSL for the dashboard

- The Kubernetes dashboard uses HTTPS, but we don't have a certificate

- Recent versions of Chrome (63 and later) and Edge will refuse to connect

  (You won't even get the option to ignore a security warning!)

- We could (and should!) get a certificate, e.g. with [Let's Encrypt](https://letsencrypt.org/)

- ... But for convenience, for this workshop, we'll forward HTTP to HTTPS

.warning[Do not do this at home, or even worse, at work!]

.debug[[k8s/dashboard.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/dashboard.md)]
---

## Running the SSL unwrapper

- We are going to run [`socat`](http://www.dest-unreach.org/socat/doc/socat.html), telling it to accept TCP connections and relay them over SSL

- Then we will expose that `socat` instance with a `NodePort` service

- For convenience, these steps are neatly encapsulated into another YAML file

.exercise[

- Apply the convenient YAML file, and defeat SSL protection:
  ```bash
  kubectl apply -f https://goo.gl/tA7GLz
  ```

]

The goo.gl URL expands to:
<br/>
.small[.small[https://gist.githubusercontent.com/jpetazzo/c53a28b5b7fdae88bc3c5f0945552c04/raw/da13ef1bdd38cc0e90b7a4074be8d6a0215e1a65/socat.yaml]]

.warning[All our dashboard traffic is now clear-text, including passwords!]

.debug[[k8s/dashboard.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/dashboard.md)]
---

## Connecting to the dashboard

.exercise[

- Check which port the dashboard is on:
  ```bash
  kubectl -n kube-system get svc socat
  ```

]

You'll want the `3xxxx` port.


.exercise[

- Connect to http://oneofournodes:3xxxx/

<!-- ```open https://node1:3xxxx/``` -->

]

The dashboard will then ask you which authentication you want to use.

.debug[[k8s/dashboard.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/dashboard.md)]
---

## Dashboard authentication

- We have three authentication options at this point:

  - token (associated with a role that has appropriate permissions)

  - kubeconfig (e.g. using the `~/.kube/config` file from `node1`)

  - "skip" (use the dashboard "service account")

- Let's use "skip": we get a bunch of warnings and don't see much

.debug[[k8s/dashboard.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/dashboard.md)]
---

## 3) Bypass authentication for the dashboard

- The dashboard documentation [explains how to do this](https://github.com/kubernetes/dashboard/wiki/Access-control#admin-privileges)

- We just need to load another YAML file!

.exercise[

- Grant admin privileges to the dashboard so we can see our resources:
  ```bash
  kubectl apply -f https://goo.gl/CHsLTA
  ```

- Reload the dashboard and enjoy!

]

--

.warning[By the way, we just added a backdoor to our Kubernetes cluster!]

.debug[[k8s/dashboard.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/dashboard.md)]
---

## Exposing the dashboard over HTTPS

- We took a shortcut by forwarding HTTP to HTTPS inside the cluster

- Let's expose the dashboard over HTTPS!

- The dashboard is exposed through a `ClusterIP` service (internal traffic only)

- We will change that into a `NodePort` service (accepting outside traffic)

.exercise[

- Edit the service:
  ```
  kubectl edit service kubernetes-dashboard
  ```

]

--

`NotFound`?!? Y U NO WORK?!?

.debug[[k8s/dashboard.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/dashboard.md)]
---

## Editing the `kubernetes-dashboard` service

- If we look at the [YAML](https://goo.gl/Qamqab) that we loaded before, we'll get a hint

--

- The dashboard was created in the `kube-system` namespace

--

.exercise[

- Edit the service:
  ```bash
  kubectl -n kube-system edit service kubernetes-dashboard
  ```

- Change `ClusterIP` to `NodePort`, save, and exit

<!--
```wait Please edit the object below```
```keys /ClusterIP```
```keys ^J```
```keys cwNodePort```
```keys ^[ ``` ]
```keys :wq```
```keys ^J```
-->

- Check the port that was assigned with `kubectl -n kube-system get services`

- Connect to https://oneofournodes:3xxxx/ (yes, https)

]

.debug[[k8s/dashboard.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/dashboard.md)]
---

## Running the Kubernetes dashboard securely

- The steps that we just showed you are *for educational purposes only!*

- If you do that on your production cluster, people [can and will abuse it](https://blog.redlock.io/cryptojacking-tesla)

- For an in-depth discussion about securing the dashboard,
  <br/>
  check [this excellent post on Heptio's blog](https://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca)

.debug[[k8s/dashboard.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/dashboard.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/chinook-helicopter-container.jpg)]

---

name: toc-security-implications-of-kubectl-apply
class: title

Security implications of `kubectl apply`

.nav[
[Previous section](#toc-the-kubernetes-dashboard)
|
[Back to table of contents](#toc-chapter-7)
|
[Next section](#toc-scaling-a-deployment)
]

.debug[(automatically generated title slide)]

---

# Security implications of `kubectl apply`

- When we do `kubectl apply -f <URL>`, we create arbitrary resources

- Resources can be evil; imagine a `deployment` that ...

--

  - starts bitcoin miners on the whole cluster

--

  - hides in a non-default namespace

--

  - bind-mounts our nodes' filesystem

--

  - inserts SSH keys in the root account (on the node)

--

  - encrypts our data and ransoms it

--

  - ‚ò†Ô∏è‚ò†Ô∏è‚ò†Ô∏è

.debug[[k8s/dashboard.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/dashboard.md)]
---

## `kubectl apply` is the new `curl | sh`

- `curl | sh` is convenient

- It's safe if you use HTTPS URLs from trusted sources

--

- `kubectl apply -f` is convenient

- It's safe if you use HTTPS URLs from trusted sources

- Example: the official setup instructions for most pod networks

--

- It introduces new failure modes (like if you try to apply yaml from a link that's no longer valid)


.debug[[k8s/dashboard.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/dashboard.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-cranes.jpg)]

---

name: toc-scaling-a-deployment
class: title

Scaling a deployment

.nav[
[Previous section](#toc-security-implications-of-kubectl-apply)
|
[Back to table of contents](#toc-chapter-7)
|
[Next section](#toc-daemon-sets)
]

.debug[(automatically generated title slide)]

---
# Scaling a deployment

- We will start with an easy one: the `worker` deployment

.exercise[

- Open two new terminals to check what's going on with pods and deployments:
  ```bash
  kubectl get pods -w
  kubectl get deployments -w
  ```

<!--
```wait RESTARTS```
```keys ^C```
```wait AVAILABLE```
```keys ^C```
-->

- Now, create more `worker` replicas:
  ```bash
  kubectl scale deploy/worker --replicas=10
  ```

]

After a few seconds, the graph in the web UI should show up.
<br/>
(And peak at 10 hashes/second, just like when we were running on a single one.)

.debug[[k8s/kubectlscale.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/kubectlscale.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-housing.jpg)]

---

name: toc-daemon-sets
class: title

Daemon sets

.nav[
[Previous section](#toc-scaling-a-deployment)
|
[Back to table of contents](#toc-chapter-7)
|
[Next section](#toc-updating-a-service-through-labels-and-selectors)
]

.debug[(automatically generated title slide)]

---
# Daemon sets

- We want to scale `rng` in a way that is different from how we scaled `worker`

- We want one (and exactly one) instance of `rng` per node

- What if we just scale up `deploy/rng` to the number of nodes?

  - nothing guarantees that the `rng` containers will be distributed evenly

  - if we add nodes later, they will not automatically run a copy of `rng`

  - if we remove (or reboot) a node, one `rng` container will restart elsewhere

- Instead of a `deployment`, we will use a `daemonset`

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Daemon sets in practice

- Daemon sets are great for cluster-wide, per-node processes:

  - `kube-proxy`

  - `weave` (our overlay network)

  - monitoring agents

  - hardware management tools (e.g. SCSI/FC HBA agents)

  - etc.

- They can also be restricted to run [only on some nodes](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#running-pods-on-only-some-nodes)

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Creating a daemon set

- Unfortunately, as of Kubernetes 1.10, the CLI cannot create daemon sets

--

- More precisely: it doesn't have a subcommand to create a daemon set

--

- But any kind of resource can always be created by providing a YAML description:
  ```bash
  kubectl apply -f foo.yaml
  ```

--

- How do we create the YAML file for our daemon set?

--

  - option 1: [read the docs](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#create-a-daemonset)

--

  - option 2: `vi` our way out of it

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Creating the YAML file for our daemon set

- Let's start with the YAML file for the current `rng` resource

.exercise[

- Dump the `rng` resource in YAML:
  ```bash
  kubectl get deploy/rng -o yaml --export >rng.yml 
  ```

- Edit `rng.yml`

]

Note: `--export` will remove "cluster-specific" information, i.e.:
- namespace (so that the resource is not tied to a specific namespace)
- status and creation timestamp (useless when creating a new resource)
- resourceVersion and uid (these would cause... *interesting* problems)

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## "Casting" a resource to another

- What if we just changed the `kind` field?

  (It can't be that easy, right?)

.exercise[

- Change `kind: Deployment` to `kind: DaemonSet`

<!--
```bash vim rng.yml```
```wait kind: Deployment```
```keys /Deployment```
```keys ^J```
```keys cwDaemonSet```
```keys ^[``` ]
```keys :wq```
```keys ^J```
-->

- Save, quit

- Try to create our new resource:
  ```
  kubectl apply -f rng.yml
  ```

]

--

We all knew this couldn't be that easy, right!

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Understanding the problem

- The core of the error is:
  ```
  error validating data:
  [ValidationError(DaemonSet.spec):
  unknown field "replicas" in io.k8s.api.extensions.v1beta1.DaemonSetSpec,
  ...
  ```

--

- *Obviously,* it doesn't make sense to specify a number of replicas for a daemon set

--

- Workaround: fix the YAML

  - remove the `replicas` field
  - remove the `strategy` field (which defines the rollout mechanism for a deployment)
  - remove the `progressDeadlineSeconds` field (also used by the rollout mechanism)
  - remove the `status: {}` line at the end

--

- Or, we could also ...

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Use the `--force`, Luke

- We could also tell Kubernetes to ignore these errors and try anyway

- The `--force` flag's actual name is `--validate=false`

.exercise[

- Try to load our YAML file and ignore errors:
  ```bash
  kubectl apply -f rng.yml --validate=false
  ```

]

--

üé©‚ú®üêá

--

Wait ... Now, can it be *that* easy?

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Checking what we've done

- Did we transform our `deployment` into a `daemonset`?

.exercise[

- Look at the resources that we have now:
  ```bash
  kubectl get all
  ```

]

--

We have two resources called `rng`:

- the *deployment* that was existing before

- the *daemon set* that we just created

We also have one too many pods.
<br/>
(The pod corresponding to the *deployment* still exists.)

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## `deploy/rng` and `ds/rng`

- You can have different resource types with the same name

  (i.e. a *deployment* and a *daemon set* both named `rng`)

- We still have the old `rng` *deployment*

  ```
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/rng        1         1         1            1           18m
  ```

- But now we have the new `rng` *daemon set* as well

  ```
NAME                DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE
daemonset.apps/rng  2        2        2      2           2          <none>         9s
  ```

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Too many pods

- If we check with `kubectl get pods`, we see:

  - *one pod* for the deployment (named `rng-xxxxxxxxxx-yyyyy`)

  - *one pod per node* for the daemon set (named `rng-zzzzz`)

  ```
  NAME                        READY     STATUS    RESTARTS   AGE
  rng-54f57d4d49-7pt82        1/1       Running   0          11m
  rng-b85tm                   1/1       Running   0          25s
  rng-hfbrr                   1/1       Running   0          25s
  [...]
  ```

--

The daemon set created one pod per node, except on the master node.

The master node has [taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/) preventing pods from running there.

(To schedule a pod on this node anyway, the pod will require appropriate [tolerations](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/).)

.footnote[(Off by one? We don't run these pods on the node hosting the control plane.)]

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## What are all these pods doing?

- Let's check the logs of all these `rng` pods

- All these pods have a `run=rng` label:

  - the first pod, because that's what `kubectl run` does
  - the other ones (in the daemon set), because we
    *copied the spec from the first one*

- Therefore, we can query everybody's logs using that `run=rng` selector

.exercise[

- Check the logs of all the pods having a label `run=rng`:
  ```bash
  kubectl logs -l run=rng --tail 1
  ```

]

--

It appears that *all the pods* are serving requests at the moment.

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## The magic of selectors

- The `rng` *service* is load balancing requests to a set of pods

- This set of pods is defined as "pods having the label `run=rng`"

.exercise[

- Check the *selector* in the `rng` service definition:
  ```bash
  kubectl describe service rng
  ```

]

When we created additional pods with this label, they were
automatically detected by `svc/rng` and added as *endpoints*
to the associated load balancer.

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Removing the first pod from the load balancer

- What would happen if we removed that pod, with `kubectl delete pod ...`?

--

  The `replicaset` would re-create it immediately.

--

- What would happen if we removed the `run=rng` label from that pod?

--

  The `replicaset` would re-create it immediately.

--

  ... Because what matters to the `replicaset` is the number of pods *matching that selector.*

--

- But but but ... Don't we have more than one pod with `run=rng` now?

--

  The answer lies in the exact selector used by the `replicaset` ...

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Deep dive into selectors

- Let's look at the selectors for the `rng` *deployment* and the associated *replica set*

.exercise[

- Show detailed information about the `rng` deployment:
  ```bash
  kubectl describe deploy rng
  ```

- Show detailed information about the `rng` replica:
  <br/>(The second command doesn't require you to get the exact name of the replica set)
  ```bash
  kubectl describe rs rng-yyyy
  kubectl describe rs -l run=rng
  ```

]

--

The replica set selector also has a `pod-template-hash`, unlike the pods in our daemon set.

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/containers-by-the-water.jpg)]

---

name: toc-updating-a-service-through-labels-and-selectors
class: title

Updating a service through labels and selectors

.nav[
[Previous section](#toc-daemon-sets)
|
[Back to table of contents](#toc-chapter-7)
|
[Next section](#toc-rolling-updates)
]

.debug[(automatically generated title slide)]

---

# Updating a service through labels and selectors

- What if we want to drop the `rng` deployment from the load balancer?

- Option 1: 

  - destroy it

- Option 2: 

  - add an extra *label* to the daemon set

  - update the service *selector* to refer to that *label*

--

Of course, option 2 offers more learning opportunities. Right?

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Add an extra label to the daemon set

- We will update the daemon set "spec"

- Option 1:

  - edit the `rng.yml` file that we used earlier

  - load the new definition with `kubectl apply`

- Option 2: 

  - use `kubectl edit`

--

*If you feel like you got thisüíïüåà, feel free to try directly.*

*We've included a few hints on the next slides for your convenience!*

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## We've put resources in your resources

- Reminder: a daemon set is a resource that creates more resources!

- There is a difference between:

  - the label(s) of a resource (in the `metadata` block in the beginning)

  - the selector of a resource (in the `spec` block)

  - the label(s) of the resource(s) created by the first resource (in the `template` block)

- You need to update the selector and the template (metadata labels are not mandatory)

- The template must match the selector

  (i.e. the resource will refuse to create resources that it will not select)

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Adding our label

- Let's add a label `isactive: yes`

- In YAML, `yes` should be quoted; i.e. `isactive: "yes"`

.exercise[

- Update the daemon set to add `isactive: "yes"` to the selector and template label:
  ```bash
  kubectl edit daemonset rng
  ```

<!--
```wait Please edit the object below```
```keys /run: rng```
```keys ^J```
```keys noisactive: "yes"```
```keys ^[``` ]
```keys /run: rng```
```keys ^J```
```keys oisactive: "yes"```
```keys ^[``` ]
```keys :wq```
```keys ^J```
-->

- Update the service to add `isactive: "yes"` to its selector:
  ```bash
  kubectl edit service rng
  ```

<!--
```wait Please edit the object below```
```keys /run: rng```
```keys ^J```
```keys noisactive: "yes"```
```keys ^[``` ]
```keys :wq```
```keys ^J```
-->

]

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Checking what we've done

.exercise[

- Check the most recent log line of all `run=rng` pods to confirm that exactly one per node is now active:
  ```bash
  kubectl logs -l run=rng --tail 1
  ```

]

The timestamps should give us a hint about how many pods are currently receiving traffic.

.exercise[

- Look at the pods that we have right now:
  ```bash
  kubectl get pods
  ```

]

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Cleaning up

- The pods of the deployment and the "old" daemon set are still running

- We are going to identify them programmatically

.exercise[

- List the pods with `run=rng` but without `isactive=yes`:
  ```bash
  kubectl get pods -l run=rng,isactive!=yes
  ```

- Remove these pods:
  ```bash
  kubectl delete pods -l run=rng,isactive!=yes
  ```

]

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Cleaning up stale pods

```
$ kubectl get pods
NAME                        READY     STATUS        RESTARTS   AGE
rng-54f57d4d49-7pt82        1/1       Terminating   0          51m
rng-54f57d4d49-vgz9h        1/1       Running       0          22s
rng-b85tm                   1/1       Terminating   0          39m
rng-hfbrr                   1/1       Terminating   0          39m
rng-vplmj                   1/1       Running       0          7m
rng-xbpvg                   1/1       Running       0          7m
[...]
```

- The extra pods (noted `Terminating` above) are going away

- ... But a new one (`rng-54f57d4d49-vgz9h` above) was restarted immediately!

--

- Remember, the *deployment* still exists, and makes sure that one pod is up and running

- If we delete the pod associated to the deployment, it is recreated automatically

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Deleting a deployment

.exercise[

- Remove the `rng` deployment:
  ```bash
  kubectl delete deployment rng
  ```
]

--

- The pod that was created by the deployment is now being terminated:

```
$ kubectl get pods
NAME                        READY     STATUS        RESTARTS   AGE
rng-54f57d4d49-vgz9h        1/1       Terminating   0          4m
rng-vplmj                   1/1       Running       0          11m
rng-xbpvg                   1/1       Running       0          11m
[...]
```

Ding, dong, the deployment is dead! And the daemon set lives on.

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Avoiding extra pods

- When we changed the definition of the daemon set, it immediately created new pods. We had to remove the old ones manually.

- How could we have avoided this?

--

- By adding the `isactive: "yes"` label to the pods before changing the daemon set!

- This can be done programmatically with `kubectl patch`:

  ```bash
    PATCH='
    metadata:
      labels:
        isactive: "yes"
    '
    kubectl get pods -l run=rng -l controller-revision-hash -o name |
      xargs kubectl patch -p "$PATCH" 
  ```

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Labels and debugging

- When a pod is misbehaving, we can delete it: another one will be recreated

- But we can also change its labels

- It will be removed from the load balancer (it won't receive traffic anymore)

- Another pod will be recreated immediately

- But the problematic pod is still here, and we can inspect and debug it

- We can even re-add it to the rotation if necessary

  (Very useful to troubleshoot intermittent and elusive bugs)

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

## Labels and advanced rollout control

- Conversely, we can add pods matching a service's selector

- These pods will then receive requests and serve traffic

- Examples:

  - one-shot pod with all debug flags enabled, to collect logs

  - pods created automatically, but added to rotation in a second step
    <br/>
    (by setting their label accordingly)

- This gives us building blocks for canary and blue/green deployments

.debug[[k8s/daemonset.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/daemonset.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/distillery-containers.jpg)]

---

name: toc-rolling-updates
class: title

Rolling updates

.nav[
[Previous section](#toc-updating-a-service-through-labels-and-selectors)
|
[Back to table of contents](#toc-chapter-7)
|
[Next section](#toc-accessing-logs-from-the-cli)
]

.debug[(automatically generated title slide)]

---
# Rolling updates

- By default (without rolling updates), when a scaled resource is updated:

  - new pods are created

  - old pods are terminated
  
  - ... all at the same time
  
  - if something goes wrong, ¬Ø\\\_(„ÉÑ)\_/¬Ø

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

## Rolling updates

- With rolling updates, when a resource is updated, it happens progressively

- Two parameters determine the pace of the rollout: `maxUnavailable` and `maxSurge`

- They can be specified in absolute number of pods, or percentage of the `replicas` count

- At any given time ...

  - there will always be at least `replicas`-`maxUnavailable` pods available

  - there will never be more than `replicas`+`maxSurge` pods in total

  - there will therefore be up to `maxUnavailable`+`maxSurge` pods being updated

- We have the possibility to rollback to the previous version
  <br/>(if the update fails or is unsatisfactory in any way)

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

## Checking current rollout parameters

- Recall how we build custom reports with `kubectl` and `jq`:

.exercise[

- Show the rollout plan for our deployments:
  ```bash
    kubectl get deploy -o json |
            jq ".items[] | {name:.metadata.name} + .spec.strategy.rollingUpdate"
  ```

]

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---


## Rolling updates in practice

- As of Kubernetes 1.8, we can do rolling updates with:

  `deployments`, `daemonsets`, `statefulsets`

- Editing one of these resources will automatically result in a rolling update

- Rolling updates can be monitored with the `kubectl rollout` subcommand

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

## Building a new version of the `worker` service

.exercise[

- Go to the `stack` directory:
  ```bash
  cd ~/container.training/stacks
  ```

- Edit `dockercoins/worker/worker.py`, update the `sleep` line to sleep 1 second

- Build a new tag and push it to the registry:
  ```bash
  #export REGISTRY=localhost:3xxxx
  export TAG=v0.2
  docker-compose -f dockercoins.yml build
  docker-compose -f dockercoins.yml push
  ```

]

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

## Rolling out the new `worker` service

.exercise[

- Let's monitor what's going on by opening a few terminals, and run:
  ```bash
  kubectl get pods -w
  kubectl get replicasets -w
  kubectl get deployments -w
  ```

<!--
```wait NAME```
```keys ^C```
-->

- Update `worker` either with `kubectl edit`, or by running:
  ```bash
  kubectl set image deploy worker worker=$REGISTRY/worker:$TAG
  ```

]

--

That rollout should be pretty quick. What shows in the web UI?

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

## Give it some time

- At first, it looks like nothing is happening (the graph remains at the same level)

- According to `kubectl get deploy -w`, the `deployment` was updated really quickly

- But `kubectl get pods -w` tells a different story

- The old `pods` are still here, and they stay in `Terminating` state for a while

- Eventually, they are terminated; and then the graph decreases significantly

- This delay is due to the fact that our worker doesn't handle signals

- Kubernetes sends a "polite" shutdown request to the worker, which ignores it

- After a grace period, Kubernetes gets impatient and kills the container

  (The grace period is 30 seconds, but [can be changed](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods) if needed)

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

## Rolling out something invalid

- What happens if we make a mistake?

.exercise[

- Update `worker` by specifying a non-existent image:
  ```bash
  export TAG=v0.3
  kubectl set image deploy worker worker=$REGISTRY/worker:$TAG
  ```

- Check what's going on:
  ```bash
  kubectl rollout status deploy worker
  ```

<!--
```wait Waiting for deployment```
```keys ^C```
-->

]

--

Our rollout is stuck. However, the app is not dead.

(After a minute, it will stabilize to be 20-25% slower.)

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

## What's going on with our rollout?

- Why is our app a bit slower?

- Because `MaxUnavailable=25%`

  ... So the rollout terminated 2 replicas out of 10 available

- Okay, but why do we see 5 new replicas being rolled out?

- Because `MaxSurge=25%`

  ... So in addition to replacing 2 replicas, the rollout is also starting 3 more

- It rounded down the number of MaxUnavailable pods conservatively,
  <br/>
  but the total number of pods being rolled out is allowed to be 25+25=50%

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

class: extra-details

## The nitty-gritty details

- We start with 10 pods running for the `worker` deployment

- Current settings: MaxUnavailable=25% and MaxSurge=25%

- When we start the rollout:

  - two replicas are taken down (as per MaxUnavailable=25%)
  - two others are created (with the new version) to replace them
  - three others are created (with the new version) per MaxSurge=25%)

- Now we have 8 replicas up and running, and 5 being deployed

- Our rollout is stuck at this point!

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

## Checking the dashboard during the bad rollout

If you haven't deployed the Kubernetes dashboard earlier, just skip this slide.

.exercise[

- Check which port the dashboard is on:
  ```bash
  kubectl -n kube-system get svc socat
  ```

]

Note the `3xxxx` port.

.exercise[

- Connect to http://oneofournodes:3xxxx/

<!-- ```open https://node1:3xxxx/``` -->

]

--

- We have failures in Deployments, Pods, and Replica Sets

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

## Recovering from a bad rollout

- We could push some `v0.3` image

  (the pod retry logic will eventually catch it and the rollout will proceed)

- Or we could invoke a manual rollback

.exercise[

<!--
```keys
^C
```
-->

- Cancel the deployment and wait for the dust to settle down:
  ```bash
  kubectl rollout undo deploy worker
  kubectl rollout status deploy worker
  ```

]

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

## Changing rollout parameters

- We want to:

  - revert to `v0.1`
  - be conservative on availability (always have desired number of available workers)
  - go slow on rollout speed (update only one pod at a time) 
  - give some time to our workers to "warm up" before starting more

The corresponding changes can be expressed in the following YAML snippet:

.small[
```yaml
spec:
  template:
    spec:
      containers:
      - name: worker
        image: $REGISTRY/worker:v0.1
  strategy:
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  minReadySeconds: 10
```
]

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

## Applying changes through a YAML patch

- We could use `kubectl edit deployment worker`

- But we could also use `kubectl patch` with the exact YAML shown before

.exercise[

.small[

- Apply all our changes and wait for them to take effect:
  ```bash
  kubectl patch deployment worker -p "
    spec:
      template:
        spec:
          containers:
          - name: worker
            image: $REGISTRY/worker:v0.1
      strategy:
        rollingUpdate:
          maxUnavailable: 0
          maxSurge: 1
      minReadySeconds: 10
    "
  kubectl rollout status deployment worker
  kubectl get deploy -o json worker |
          jq "{name:.metadata.name} + .spec.strategy.rollingUpdate"
  ```
  ] 

]

.debug[[k8s/rollout.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/rollout.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/lots-of-containers.jpg)]

---

name: toc-accessing-logs-from-the-cli
class: title

Accessing logs from the CLI

.nav[
[Previous section](#toc-rolling-updates)
|
[Back to table of contents](#toc-chapter-8)
|
[Next section](#toc-managing-stacks-with-helm)
]

.debug[(automatically generated title slide)]

---
# Accessing logs from the CLI

- The `kubectl logs` commands has limitations:

  - it cannot stream logs from multiple pods at a time

  - when showing logs from multiple pods, it mixes them all together

- We are going to see how to do it better

.debug[[k8s/logs-cli.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/logs-cli.md)]
---

## Doing it manually

- We *could* (if we were so inclined), write a program or script that would:

  - take a selector as an argument

  - enumerate all pods matching that selector (with `kubectl get -l ...`)

  - fork one `kubectl logs --follow ...` command per container

  - annotate the logs (the output of each `kubectl logs ...` process) with their origin

  - preserve ordering by using `kubectl logs --timestamps ...` and merge the output

--

- We *could* do it, but thankfully, others did it for us already!

.debug[[k8s/logs-cli.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/logs-cli.md)]
---

## Stern

[Stern](https://github.com/wercker/stern) is an open source project
by [Wercker](http://www.wercker.com/).

From the README:

*Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging.*

*The query is a regular expression so the pod name can easily be filtered and you don't need to specify the exact id (for instance omitting the deployment id). If a pod is deleted it gets removed from tail and if a new pod is added it automatically gets tailed.*

Exactly what we need!

.debug[[k8s/logs-cli.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/logs-cli.md)]
---

## Installing Stern

- For simplicity, let's just grab a binary release

.exercise[

- Download a binary release from GitHub:
  ```bash
  sudo curl -L -o /usr/local/bin/stern \
       https://github.com/wercker/stern/releases/download/1.6.0/stern_linux_amd64
  sudo chmod +x /usr/local/bin/stern
  ```

]

These installation instructions will work on our clusters, since they are Linux amd64 VMs.

However, you will have to adapt them if you want to install Stern on your local machine.

.debug[[k8s/logs-cli.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/logs-cli.md)]
---

## Using Stern

- There are two ways to specify the pods for which we want to see the logs:

  - `-l` followed by a selector expression (like with many `kubectl` commands)

  - with a "pod query", i.e. a regex used to match pod names

- These two ways can be combined if necessary

.exercise[

- View the logs for all the rng containers:
  ```bash
  stern rng
  ```

]

.debug[[k8s/logs-cli.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/logs-cli.md)]
---

## Stern convenient options

- The `--tail N` flag shows the last `N` lines for each container

  (Instead of showing the logs since the creation of the container)

- The `-t` / `--timestamps` flag shows timestamps

- The `--all-namespaces` flag is self-explanatory

.exercise[

- View what's up with the `weave` system containers:
  ```bash
  stern --tail 1 --timestamps --all-namespaces weave
  ```
]

.debug[[k8s/logs-cli.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/logs-cli.md)]
---

## Using Stern with a selector

- When specifying a selector, we can omit the value for a label

- This will match all objects having that label (regardless of the value)

- Everything created with `kubectl run` has a label `run`

- We can use that property to view the logs of all the pods created with `kubectl run`

.exercise[

- View the logs for all the things started with `kubectl run`:
  ```bash
  stern -l run
  ```

]

.debug[[k8s/logs-cli.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/logs-cli.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/plastic-containers.JPG)]

---

name: toc-managing-stacks-with-helm
class: title

Managing stacks with Helm

.nav[
[Previous section](#toc-accessing-logs-from-the-cli)
|
[Back to table of contents](#toc-chapter-8)
|
[Next section](#toc-namespaces)
]

.debug[(automatically generated title slide)]

---
# Managing stacks with Helm

- We created our first resources with `kubectl run`, `kubectl expose` ...

- We have also created resources by loading YAML files with `kubectl apply -f`

- For larger stacks, managing thousands of lines of YAML is unreasonable

- These YAML bundles need to be customized with variable parameters

  (E.g.: number of replicas, image version to use ...)

- It would be nice to have an organized, versioned collection of bundles

- It would be nice to be able to upgrade/rollback these bundles carefully

- [Helm](https://helm.sh/) is an open source project offering all these things!

.debug[[k8s/helm.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/helm.md)]
---

## Helm concepts

- `helm` is a CLI tool

- `tiller` is its companion server-side component

- A "chart" is an archive containing templatized YAML bundles

- Charts are versioned

- Charts can be stored on private or public repositories

.debug[[k8s/helm.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/helm.md)]
---

## Installing Helm

- We need to install the `helm` CLI; then use it to deploy `tiller`

.exercise[

- Install the `helm` CLI:
  ```bash
  curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash
  ```

- Deploy `tiller`:
  ```bash
  helm init
  ```

- Add the `helm` completion:
  ```bash
  . <(helm completion $(basename $SHELL))
  ```

]

.debug[[k8s/helm.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/helm.md)]
---

## Fix account permissions

- Helm permission model requires us to tweak permissions

- In a more realistic deployment, you might create per-user or per-team
  service accounts, roles, and role bindings

.exercise[

- Grant `cluster-admin` role to `kube-system:default` service account:
  ```bash
  kubectl create clusterrolebinding add-on-cluster-admin \
      --clusterrole=cluster-admin --serviceaccount=kube-system:default
  ```

]

(Defining the exact roles and permissions on your cluster requires
a deeper knowledge of Kubernetes' RBAC model. The command above is
fine for personal and development clusters.)

.debug[[k8s/helm.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/helm.md)]
---

## View available charts

- A public repo is pre-configured when installing Helm

- We can view available charts with `helm search` (and an optional keyword)

.exercise[

- View all available charts:
  ```bash
  helm search
  ```

- View charts related to `prometheus`:
  ```bash
  helm search prometheus
  ```

]

.debug[[k8s/helm.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/helm.md)]
---

## Install a chart

- Most charts use `LoadBalancer` service types by default

- Most charts require persistent volumes to store data

- We need to relax these requirements a bit

.exercise[

- Install the Prometheus metrics collector on our cluster:
  ```bash
  helm install stable/prometheus \
         --set server.service.type=NodePort \
         --set server.persistentVolume.enabled=false
  ```

]

Where do these `--set` options come from?

.debug[[k8s/helm.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/helm.md)]
---

## Inspecting a chart

- `helm inspect` shows details about a chart (including available options)

.exercise[

- See the metadata and all available options for `stable/prometheus`:
  ```bash
  helm inspect stable/prometheus
  ```

]

The chart's metadata includes an URL to the project's home page.

(Sometimes it conveniently points to the documentation for the chart.)

.debug[[k8s/helm.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/helm.md)]
---

## Creating a chart

- We are going to show a way to create a *very simplified* chart

- In a real chart, *lots of things* would be templatized

  (Resource names, service types, number of replicas...)

.exercise[

- Create a sample chart:
  ```bash
  helm create dockercoins
  ```

- Move away the sample templates and create an empty template directory:
  ```bash
  mv dockercoins/templates dockercoins/default-templates
  mkdir dockercoins/templates
  ```

]

.debug[[k8s/helm.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/helm.md)]
---

## Exporting the YAML for our application

- The following section assumes that DockerCoins is currently running

.exercise[

- Create one YAML file for each resource that we need:
  .small[
  ```bash

	while read kind name; do
	  kubectl get -o yaml --export $kind $name > dockercoins/templates/$name-$kind.yaml
	done <<EOF
	deployment worker
	deployment hasher
	daemonset rng
	deployment webui
	deployment redis
	service hasher
	service rng
	service webui
	service redis
	EOF
  ```
  ]

]

.debug[[k8s/helm.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/helm.md)]
---

## Testing our helm chart

.exercise[

- Let's install our helm chart! (`dockercoins` is the path to the chart)
  ```bash
  helm install dockercoins
  ```
]

--

- Since the application is already deployed, this will fail:<br>
`Error: release loitering-otter failed: services "hasher" already exists`

- To avoid naming conflicts, we will deploy the application in another *namespace*

.debug[[k8s/helm.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/helm.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-1.jpg)]

---

name: toc-namespaces
class: title

Namespaces

.nav[
[Previous section](#toc-managing-stacks-with-helm)
|
[Back to table of contents](#toc-chapter-8)
|
[Next section](#toc-next-steps)
]

.debug[(automatically generated title slide)]

---
# Namespaces

- We cannot have two resources with the same name

  (Or can we...?)

--

- We cannot have two resources *of the same type* with the same name

  (But it's OK to have a `rng` service, a `rng` deployment, and a `rng` daemon set!)

--

- We cannot have two resources of the same type with the same name *in the same namespace*

  (But it's OK to have e.g. two `rng` services in different namespaces!)

--

- In other words: **the tuple *(type, name, namespace)* needs to be unique**

  (In the resource YAML, the type is called `Kind`)

.debug[[k8s/namespaces.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/namespaces.md)]
---

## Pre-existing namespaces

- If we deploy a cluster with `kubeadm`, we have three namespaces:

  - `default` (for our applications)

  - `kube-system` (for the control plane)

  - `kube-public` (contains one secret used for cluster discovery)

- If we deploy differently, we may have different namespaces

.debug[[k8s/namespaces.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/namespaces.md)]
---

## Creating namespaces

- Creating a namespace is done with the `kubectl create namespace` command:
  ```bash
  kubectl create namespace blue
  ```

- We can also get fancy and use a very minimal YAML snippet, e.g.:
  ```bash
	kubectl apply -f- <<EOF
	apiVersion: v1
	kind: Namespace
	metadata:
	  name: blue
	EOF
  ```

- The two methods above are identical

- If we are using a tool like Helm, it will create namespaces automatically

.debug[[k8s/namespaces.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/namespaces.md)]
---

## Using namespaces

- We can pass a `-n` or `--namespace` flag to most `kubectl` commands:
  ```bash
  kubectl -n blue get svc
  ```

- We can also use *contexts*

- A context is a *(user, cluster, namespace)* tuple

- We can manipulate contexts with the `kubectl config` command

.debug[[k8s/namespaces.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/namespaces.md)]
---

## Creating a context

- We are going to create a context for the `blue` namespace

.exercise[

- View existing contexts to see the cluster name and the current user:
  ```bash
  kubectl config get-contexts
  ```

- Create a new context:
  ```bash
  kubectl config set-context blue --namespace=blue \
      --cluster=kubernetes --user=kubernetes-admin
  ```

]

We have created a context; but this is just some configuration values.

The namespace doesn't exist yet.

.debug[[k8s/namespaces.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/namespaces.md)]
---

## Using a context

- Let's switch to our new context and deploy the DockerCoins chart

.exercise[

- Use the `blue` context:
  ```bash
  kubectl config use-context blue
  ```

- Deploy DockerCoins:
  ```bash
  helm install dockercoins
  ```

]

In the last command line, `dockercoins` is just the local path where
we created our Helm chart before.

.debug[[k8s/namespaces.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/namespaces.md)]
---

## Viewing the deployed app

- Let's see if our Helm chart worked correctly!

.exercise[

- Retrieve the port number allocated to the `webui` service:
  ```bash
  kubectl get svc webui
  ```

- Point our browser to http://X.X.X.X:3xxxx

]

Note: it might take a minute or two for the app to be up and running.

.debug[[k8s/namespaces.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/namespaces.md)]
---

## Namespaces and isolation

- Namespaces *do not* provide isolation

- A pod in the `green` namespace can communicate with a pod in the `blue` namespace

- A pod in the `default` namespace can communicate with a pod in the `kube-system` namespace

- CoreDNS uses a different subdomain for each namespace

- Example: from any pod in the cluster, you can connect to the Kubernetes API with:

  `https://kubernetes.default.svc.cluster.local:443/`

.debug[[k8s/namespaces.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/namespaces.md)]
---

## Isolating pods

- Actual isolation is implemented with *network policies*

- Network policies are resources (like deployments, services, namespaces...)

- Network policies specify which flows are allowed:

  - between pods

  - from pods to the outside world

  - and vice-versa

.debug[[k8s/namespaces.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/namespaces.md)]
---

## Network policies overview

- We can create as many network policies as we want

- Each network policy has:

  - a *pod selector*: "which pods are targeted by the policy?"

  - lists of ingress and/or egress rules: "which peers and ports are allowed or blocked?"

- If a pod is not targeted by any policy, traffic is allowed by default

- If a pod is targeted by at least one policy, traffic must be allowed explicitly

.debug[[k8s/namespaces.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/namespaces.md)]
---

## More about network policies

- This remains a high level overview of network policies

- For more details, check:

  - the [Kubernetes documentation about network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)

  - this [talk about network policies at KubeCon 2017 US](https://www.youtube.com/watch?v=3gGpMmYeEO8) by [@ahmetb](https://twitter.com/ahmetb)

.debug[[k8s/namespaces.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/namespaces.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-2.jpg)]

---

name: toc-next-steps
class: title

Next steps

.nav[
[Previous section](#toc-namespaces)
|
[Back to table of contents](#toc-chapter-8)
|
[Next section](#toc-links-and-resources)
]

.debug[(automatically generated title slide)]

---
# Next steps

*Alright, how do I get started and containerize my apps?*

--

Suggested containerization checklist:

.checklist[
- write a Dockerfile for one service in one app
- write Dockerfiles for the other (buildable) services
- write a Compose file for that whole app
- make sure that devs are empowered to run the app in containers
- set up automated builds of container images from the code repo
- set up a CI pipeline using these container images
- set up a CD pipeline (for staging/QA) using these images
]

And *then* it is time to look at orchestration!

.debug[[k8s/whatsnext.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/whatsnext.md)]
---

## Namespaces

- Namespaces let you run multiple identical stacks side by side

- Two namespaces (e.g. `blue` and `green`) can each have their own `redis` service

- Each of the two `redis` services has its own `ClusterIP`

- CoreDNS creates two entries, mapping to these two `ClusterIP` addresses:

  `redis.blue.svc.cluster.local` and `redis.green.svc.cluster.local`

- Pods in the `blue` namespace get a *search suffix* of `blue.svc.cluster.local`

- As a result, resolving `redis` from a pod in the `blue` namespace yields the "local" `redis`

.warning[This does not provide *isolation*! That would be the job of network policies.]

.debug[[k8s/whatsnext.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/whatsnext.md)]
---

## Stateful services (databases etc.)

- As a first step, it is wiser to keep stateful services *outside* of the cluster

- Exposing them to pods can be done with multiple solutions:

  - `ExternalName` services
    <br/>
    (`redis.blue.svc.cluster.local` will be a `CNAME` record)

  - `ClusterIP` services with explicit `Endpoints`
    <br/>
    (instead of letting Kubernetes generate the endpoints from a selector)

  - Ambassador services
    <br/>
    (application-level proxies that can provide credentials injection and more)

.debug[[k8s/whatsnext.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/whatsnext.md)]
---

## Stateful services (second take)

- If you really want to host stateful services on Kubernetes, you can look into:

  - volumes (to carry persistent data)

  - storage plugins

  - persistent volume claims (to ask for specific volume characteristics)

  - stateful sets (pods that are *not* ephemeral)

.debug[[k8s/whatsnext.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/whatsnext.md)]
---

## HTTP traffic handling

- *Services* are layer 4 constructs

- HTTP is a layer 7 protocol

- It is handled by *ingresses* (a different resource kind)

- *Ingresses* allow:

  - virtual host routing
  - session stickiness
  - URI mapping
  - and much more!

- Check out e.g. [Tr√¶fik](https://docs.traefik.io/user-guide/kubernetes/)

.debug[[k8s/whatsnext.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/whatsnext.md)]
---

## Logging

- Logging is delegated to the container engine

- Logs are exposed through the API

- Logs are also accessible through local files (`/var/log/containers`)

- Log shipping to a central platform is usually done through these files

  (e.g. with an agent bind-mounting the log directory)

.debug[[k8s/whatsnext.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/whatsnext.md)]
---

## Metrics

- The kubelet embeds [cAdvisor](https://github.com/google/cadvisor), which exposes container metrics

  (cAdvisor might be separated in the future for more flexibility)

- It is a good idea to start with [Prometheus](https://prometheus.io/)

  (even if you end up using something else)

- Starting from Kubernetes 1.8, we can use the [Metrics API](https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/)

- [Heapster](https://github.com/kubernetes/heapster) was a popular add-on

  (but is being [deprecated](https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md) starting with Kubernetes 1.11)



.debug[[k8s/whatsnext.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/whatsnext.md)]
---

## Managing the configuration of our applications

- Two constructs are particularly useful: secrets and config maps

- They allow to expose arbitrary information to our containers

- **Avoid** storing configuration in container images

  (There are some exceptions to that rule, but it's generally a Bad Idea)

- **Never** store sensitive information in container images

  (It's the container equivalent of the password on a post-it note on your screen)

.debug[[k8s/whatsnext.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/whatsnext.md)]
---

## Managing stack deployments

- The best deployment tool will vary, depending on:

  - the size and complexity of your stack(s)
  - how often you change it (i.e. add/remove components)
  - the size and skills of your team

- A few examples:

  - shell scripts invoking `kubectl`
  - YAML resources descriptions committed to a repo
  - [Helm](https://github.com/kubernetes/helm) (~package manager)
  - [Spinnaker](https://www.spinnaker.io/) (Netflix' CD platform)
  - [Brigade](https://brigade.sh/) (event-driven scripting; no YAML)

.debug[[k8s/whatsnext.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/whatsnext.md)]
---

## Cluster federation

--

![Star Trek Federation](images/startrek-federation.jpg)

--

Sorry Star Trek fans, this is not the federation you're looking for!

--

(If I add "Your cluster is in another federation" I might get a 3rd fandom wincing!)

.debug[[k8s/whatsnext.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/whatsnext.md)]
---

## Cluster federation

- Kubernetes master operation relies on etcd

- etcd uses the [Raft](https://raft.github.io/) protocol

- Raft recommends low latency between nodes

- What if our cluster spreads to multiple regions?

--

- Break it down in local clusters

- Regroup them in a *cluster federation*

- Synchronize resources across clusters

- Discover resources across clusters

.debug[[k8s/whatsnext.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/whatsnext.md)]
---

## Developer experience

*I've put this last, but it's pretty important!*

- How do you on-board a new developer?

- What do they need to install to get a dev stack?

- How does a code change make it from dev to prod?

- How does someone add a component to a stack?

.debug[[k8s/whatsnext.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/whatsnext.md)]
---
# Links and resources

- [Kubernetes Community](https://kubernetes.io/community/) - Slack, Google Groups, meetups

- [Kubernetes on StackOverflow](https://stackoverflow.com/questions/tagged/kubernetes)

- [Play With Kubernetes Hands-On Labs](https://medium.com/@marcosnils/introducing-pwk-play-with-k8s-159fcfeb787b)

- [Azure Kubernetes Service](https://docs.microsoft.com/azure/aks/)

- [Cloud Developer Advocates](https://developer.microsoft.com/advocates/)

- [Local meetups](https://www.meetup.com/)

- [devopsdays](https://www.devopsdays.org/)

.footnote[These slides (and future updates) are on ‚Üí http://container.training/]

.debug[[k8s/links-bridget.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/k8s/links-bridget.md)]
---
class: title, self-paced

Thank you!

.debug[[shared/thankyou.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/thankyou.md)]
---

class: title, in-person

That's all, folks! <br/> Questions?

![end](images/end.jpg)

.debug[[shared/thankyou.md](https://github.com/jpetazzo/container.training.git/tree/master/slides/shared/thankyou.md)]</textarea>
    <script src="remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        highlightSpans: true,
        excludedClasses: ["self-paced"]
      });
    </script>
    
    <!-- 
    These two scripts will be available only when loading the
    content using the pub/sub server. Otherwise, they'll just
    404 and that's OK.
    -->
    <script src="/socket.io/socket.io.js">
    </script>
    <script src="/remote.js">
    </script>

  </body>
</html>
